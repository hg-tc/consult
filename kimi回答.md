deepresearch是怎么做的长文本生成，我感觉一般模型回答都有点短?


deepresearch 与 其他模型 长文本 对比
下面把「DeepResearch 类长文生成」拆成三步讲：
为什么普通大模型“总写不长”
DeepResearch 怎么把“一次 4 k～8 k token”拼成“几万 token 的完整报告”
如果你自己想做，可以怎么复现这套思路
1. 模型“写不长”的根因
训练数据里就缺长输出
大多数 SFT/RLHF 数据单条长度 <1 k token，模型没见过 1 万字连贯结尾，自然写不到。
推理阶段受限
厂商会同时设「上下文窗口」和「max_new_tokens」两道硬栅栏，例如 DeepSeek-R1 64 k 上下文，但单次最多只让输出 8 k token。
曝光偏差
越往后生成，模型越“飘”，容易自我重复或跑题，于是系统干脆提前截断保安全。
2. DeepResearch 的“分段-拼装”流水线
DeepResearch 不是“一口气写完”，而是把“写长文”转成“做多步检索 + 多段生成 + 后编辑”的 Agent 流程，典型架构如下：
规划层（Planner）
先让模型生成一份「报告提纲」，一般 3～6 级目录、20～50 个段落标题，作为后续生成的“锚点”。
检索层（Retriever）
对每一个段落标题并行做：
Web Search / 本地知识库召回 10～30 篇候选网页/论文；
用「迟分+滑动窗口」重排，挑出与标题最相关的连续文段，避免切片把上下文拦腰截断。
生成层（Writer）
每段独立调用 LLM，Prompt 模板里给足：
本段标题
对应检索结果（已按相关度排序）
全文写作风格示例（可选）
要求输出 500～800 token 的“小短文”，并显式要求“不要写总结、不要写过渡，只聚焦本段内容”。
缓存与去重
把刚写好的段落立即写进「已生成 buffer」；如果后续段落引用到同一数据，直接引用 buffer 里的结论，减少重复搜索。
合并 + 后编辑
所有段落完成后，再做一次「全局 Pass」：
用模型检查逻辑跳跃、数字冲突、引用格式；
把同一出处多次出现的参考文献合并；
可选：让模型生成 Executive Summary、图表、引用列表。
长度控制技巧
每段 500–800 token × 30 段 ≈ 15 k–24 k token；
后编辑阶段再主动要求“扩展到不少于 ××× 字”，利用「继续写」指令把短段补齐；
最终通过「分段请求 + 拼接」绕过 8 k 单次输出上限，得到 2–5 万 token 的完整报告。
3. 如果你想自己搭一套
方案 A：纯 Prompt 工程（零成本）
让模型先列提纲；
对每段单独发请求，Prompt 里写“请严格围绕『xxx 小节标题』写 600 字，不要总结、不要过渡”；
全部收到后，再发一句“以上 20 段已写完，请合并成一篇 2 万字报告，加摘要、参考文献”。
缺点：手敲累、容易超出上下文。
方案 B：LangGraph/LlamaIndex 半自动（推荐）
用 LangGraph 把“规划→搜索→写段→合并”做成 DAG，节点之间用 State 传递已生成文本；
搜索节点接 Jina/Brave/SerpAPI；
Writer 节点调用支持 8 k 输出的模型（DeepSeek、GLM-4、Claude 3.5）；
最后加一道「全局 refine」节点，让模型一次性读入全文做一致性检查。
这样 1～2 小时就能跑通一个“中文 DeepResearch”原型。
方案 C：直接微调
准备 5 k～20 k 字长输出数据（可用 arXiv 论文+对应 PPT 脚本，或维基长条目的“目录→全文”对）；
用 LongWriter-6k 或 LongLORA 方法做 LoRA 微调，把 7 B 模型单次输出推到 16 k token 以上。
