# AI咨询平台 - 项目运行原理详解

## 📖 目录

1. [项目概览](#项目概览)
2. [系统架构](#系统架构)
3. [各组件详解](#各组件详解)
4. [请求处理流程](#请求处理流程)
5. [关键技术说明](#关键技术说明)
6. [快速入门指南](#快速入门指南)

---

## 🎯 项目概览

这是一个**基于RAG（检索增强生成）技术的智能咨询平台**。用户可以将自己的文档上传到系统，然后通过自然语言问题，基于这些文档获得智能回答。

### 核心功能

- 📄 **文档上传与管理** - 支持PDF、Word、Excel等多种格式
- 🧠 **智能问答** - 基于上传的文档进行智能问答
- 🔍 **向量检索** - 使用向量数据库快速检索相关内容
- 💬 **对话式交互** - 支持多轮对话，记住上下文
- 📊 **工作区管理** - 为不同的项目创建独立的工作区
- 🌐 **Web界面** - 友好的Web界面进行交互

---

## 🏗️ 系统架构

### 三层架构图

```
用户浏览器 (访问 http://localhost:13000)
        ↓
    Nginx 反向代理 (端口 13000)
        ↓
   前端服务 (Next.js, 端口 3000)
        ↓ HTTP请求
   后端API (Fastlane, 端口 18000)
        ↓
   RAG服务 + 向量数据库
```

### 端口说明

- **13000端口 (Nginx)** - 对外提供服务的主入口
- **3000端口 (前端开发服务器)** - Next.js开发服务器
- **18000端口 (后端API)** - FastAPI后端服务

---

## 🔧 各组件详解

### 1. 前端 (Frontend)

**位置**: `/root/consult/frontend`

**技术栈**:
- Next.js 15 - React框架，用于构建Web界面
- TypeScript - 提供类型安全
- Tailwind CSS - 样式框架
- SWR - 数据获取和缓存

**核心文件**:

```
frontend/
├── app/                    # Next.js 应用路由
│   ├── page.tsx           # 主页面
│   └── agent/             # Agent功能页面
├── lib/
│   └── api-client.ts      # API客户端，封装所有后端请求
├── hooks/                 # React自定义Hooks
│   ├── use-agent-chat.ts  # Agent聊天功能
│   └── use-workspaces.ts  # 工作区管理
└── components/            # UI组件
```

**主要功能**:
1. 提供用户界面，显示文档列表、对话界面等
2. 接收用户输入（上传文件、提问）
3. 通过API客户端发送请求到后端
4. 接收后端返回的结果并显示给用户

### 2. 后端 (Backend)

**位置**: `/root/consult/backend`

**技术栈**:
- FastAPI - 高性能Python Web框架
- LangChain - RAG框架
- FAISS - 向量数据库
- BGE模型 - 中文文本向量化

**核心文件**:

```
backend/
├── app_simple.py               # 后端主入口文件
├── app/
│   ├── main.py                # 应用配置
│   ├── api/                   # API路由
│   │   └── v1/
│   │       └── endpoints/
│   │           └── global_api.py  # 全局文档API
│   ├── services/              # 业务逻辑服务
│   │   ├── langchain_rag_service.py      # RAG核心服务
│   │   ├── intent_classifier.py          # 意图分类
│   │   ├── content_aggregator.py         # 内容聚合
│   │   ├── document_generator_service.py # 文档生成
│   │   └── workspace_file_manager.py     # 文件管理
│   ├── workflows/             # 工作流编排
│   │   └── workflow_orchestrator.py
│   └── websocket/             # WebSocket支持
└── langchain_vector_db/       # 向量数据库存储
    ├── global_vector_db/      # 全局数据库
    └── workspace_1/           # 工作区1的数据库
```

**主要功能**:
1. 接收前端请求
2. 处理文档（解析、向量化、存储）
3. 执行RAG检索（找到相关文档片段）
4. 生成答案（基于检索内容）
5. 管理对话历史和上下文

### 3. Nginx 反向代理

**位置**: `/etc/nginx/sites-available/agent-platform`

**作用**:
- 统一对外提供服务的端口（13000）
- 将请求转发给前端或后端服务
- 简化访问地址，提供更专业的URL

### 4. API接口一览

系统提供以下主要API接口：

#### 全局文档API (前缀: `/api/global`)
- `POST /api/global/documents/upload` - 上传文档到全局数据库
- `GET /api/global/documents` - 获取全局文档列表
- `DELETE /api/global/documents/{id}` - 删除全局文档
- `GET /api/global/workspaces` - 获取全局工作区列表

#### 工作区管理API (前缀: `/api/workspaces`)
- `GET /api/workspaces` - 获取所有工作区列表
- `POST /api/workspaces` - 创建新工作区
- `PATCH /api/workspaces/{workspace_id}` - 更新工作区
- `DELETE /api/workspaces/{workspace_id}` - 删除工作区

#### 工作区文档API (前缀: `/api/workspaces/{workspace_id}/documents`)
- `POST /api/workspaces/{workspace_id}/documents/upload` - 上传文档到工作区
- `GET /api/workspaces/{workspace_id}/documents` - 获取工作区文档列表
- `DELETE /api/workspaces/{workspace_id}/documents/{doc_id}` - 删除工作区文档
- `GET /api/workspaces/{workspace_id}/documents/{doc_id}/download` - 下载工作区文档

#### Agent对话API (前缀: `/api/agent`)
- `POST /api/agent/chat` - 发送消息给Agent（支持多轮对话）
- `GET /api/agent/chat/{workspace_id}` - 获取聊天历史
- `GET /api/agent/workspace/{workspace_id}/status` - 获取工作区状态

#### 兼容性API (前缀: `/api`)
- `POST /api/documents/upload` - 上传文档（兼容旧版本）
- `POST /api/database/upload` - 上传到数据库
- `GET /api/database/content` - 获取数据库内容

---

## 🔄 请求处理流程

### 示例1: 上传文档

```
1. 用户在浏览器中选择文件并点击上传
   ↓
2. 前端 (frontend/lib/api-client.ts)
   - 创建FormData，包含文件
   - 发送POST请求到: http://localhost:13000/api/database/upload
   ↓
3. Nginx接收请求，转发给后端
   ↓
4. 后端 (backend/app_simple.py, line 192)
   - 接收文件 (upload_database函数)
   - 检查文件类型和大小
   - 保存文件到 uploads/global/ 目录
   - 创建处理任务 (task_queue)
   - 返回响应给前端（包含task_id）
   ↓
5. 后端异步处理文档
   - 使用 LangChainRAGService 解析文档
   - 将文档内容分块
   - 使用BGE模型向量化每个块
   - 存储到FAISS向量数据库
   ↓
6. 前端显示上传成功，并在数据库页面显示文档
```

### 示例1B: 上传文档到工作区

```
1. 用户在浏览器中选择工作区（如：工作区1）
   ↓
2. 在工作区页面选择文件并点击上传
   ↓
3. 前端 (frontend/lib/api-client.ts)
   - 创建FormData，包含文件
   - 发送POST请求到: http://localhost:13000/api/workspaces/{workspace_id}/documents/upload
   ↓
4. 后端 (backend/app_simple.py, line 1910)
   - 接收文件 (upload_workspace_document_api函数)
   - 检查文件类型和大小
   - 保存文件到 uploads/{workspace_id}/ 目录
   - 创建处理任务 (task_queue)
   - 返回响应给前端
   ↓
5. 后端异步处理文档
   - 使用 LangChainRAGService 解析文档
   - 将文档内容分块
   - 使用BGE模型向量化每个块
   - 存储到工作区特定的向量数据库: langchain_vector_db/workspace_{workspace_id}/
   ↓
6. 前端显示上传成功，文档出现在该工作区的文档列表中
```

**全局数据库 vs 工作区数据库的区别**：

| 特性 | 全局数据库 | 工作区数据库 |
|------|-----------|-------------|
| API路径 | `/api/global/documents/*` | `/api/workspaces/{id}/documents/*` |
| 存储位置 | `uploads/global/` | `uploads/{workspace_id}/` |
| 向量数据库 | `global_vector_db/` | `workspace_{id}/` |
| 可见性 | 所有工作区可访问 | 仅当前工作区可用 |
| 用途 | 公共知识库 | 项目专属文档 |

### 示例2: 提出一个问题

```
1. 用户在对话框输入问题："这份文档的主要内容是什么？"
   ↓
2. 前端 (frontend/hooks/use-agent-chat.ts)
   - 收集对话历史
   - 发送POST请求到: http://localhost:13000/api/agent/chat
   - 请求体包含: question, workspace_id, history
   ↓
3. 后端 (backend/app_simple.py, line 342)
   - 接收请求 (ask_question函数)
   ↓
4. 意图识别 (line 358-373)
   - 使用IntentClassifier判断用户意图
   - 是普通问答还是生成文档？
   ↓
5. RAG检索 (line 634-718)
   - 将问题向量化
   - 在向量数据库中搜索最相关的文档片段
   - 检索工作区和全局数据库
   - 返回Top 5最相关的片段
   ↓
6. 答案生成 (line 720-836)
   - 将检索到的文档片段作为上下文
   - 使用LLM（GPT）基于上下文生成答案
   - 返回答案和参考文献
   ↓
7. 前端接收答案并显示
   - 显示AI的回答
   - 显示参考文档来源
```

### 详细流程说明

#### 步骤1: 文档向量化过程

当您上传一个PDF文档时，系统会执行以下步骤：

1. **文档解析** (`langchain_rag_service.py`)
   - 读取PDF文件
   - 提取文本内容
   - 识别表格、图片

2. **文本分块**
   - 将长文档分割成小块（chunks）
   - 每块约300-500字
   - 保留相邻块的上下文

3. **向量化**
   - 使用BGE模型将文本块转换为1024维的向量
   - 例如："AI是人工智能" → [0.123, -0.456, 0.789, ...] (1024个数字)
   - 语义相似的文本会产生相似的向量

4. **存储**
   - 将向量存储到FAISS数据库
   - 同时保存原始文本和元数据（文件名、页码等）

#### 步骤2: 问题检索过程

当您提出问题时：

1. **问题向量化**
   - 将问题转换为向量
   - "文档主要内容" → 向量V_q

2. **相似度计算**
   - 计算V_q与数据库中所有向量的距离
   - 使用余弦相似度（数值越接近1，越相似）

3. **Top-K检索**
   - 返回最相似的5个文档片段
   - 包含：原始文本、文件名、页码、相似度分数

4. **重排序 (可选)**
   - 使用更复杂模型重新排序
   - 提高检索精度

#### 步骤3: 答案生成过程

1. **构建上下文**
   - 将Top-5文档片段拼接
   - 添加对话历史

2. **LLM生成**
   - 创建提示词（prompt）
   - 包含：检索内容、问题、对话历史
   - 发送给LLM生成答案

3. **返回结果**
   - 答案文本
   - 参考文献列表
   - 置信度分数

---

## 🛠️ 关键技术说明

### 1. RAG (检索增强生成)

**什么是RAG？**

传统LLM只能基于训练时的知识回答，RAG通过以下方式增强：

```
用户问题
    ↓
向量检索 → 找到相关文档片段
    ↓
文档片段 + 问题 → LLM生成答案
    ↓
准确的、基于文档的答案
```

**优点**:
- ✅ 可以回答最新信息（文档内容）
- ✅ 减少幻觉（答案有文档支撑）
- ✅ 可追溯（知道答案来自哪里）

### 2. 向量数据库 (FAISS)

**什么是向量数据库？**

存储高维向量的数据库，支持快速相似度搜索。

**工作原理**:
```
文档1: "机器学习算法" → 向量1 [0.1, 0.2, 0.3, ...]
文档2: "深度学习模型" → 向量2 [0.15, 0.25, 0.35, ...]
文档3: "股市行情分析" → 向量3 [0.8, 0.9, 0.7, ...]

问题: "AI技术介绍" → 向量Q [0.12, 0.22, 0.32, ...]

计算相似度:
- 向量Q与向量1: 0.98 (很相似!)
- 向量Q与向量2: 0.95 (相似)
- 向量Q与向量3: 0.12 (不相似)

返回: 文档1和文档2
```

**为什么快？**
- FAISS使用了索引技术
- 与传统暴力搜索O(n)相比，FAISS可以达到近O(log n)

### 3. 工作区 (Workspace)

**什么是工作区？**

为不同项目创建独立的文档空间，每个工作区有独立的向量数据库。

**目录结构**:
```
langchain_vector_db/
├── workspace_1/     # 工作区1的数据库
├── workspace_2/     # 工作区2的数据库
└── global_vector_db/ # 全局数据库（所有工作区共享）
```

**为什么需要？**
- 📁 组织文档：一个工作区 = 一个项目
- 🔒 数据隔离：不同工作区的文档互不干扰
- 🎯 精确检索：只检索相关工作区的文档

### 4. 意图识别 (Intent Classification)

**什么是意图识别？**

判断用户的真实意图，是普通问答还是生成文档。

**示例**:
```
用户: "这份报告的核心观点是什么？"
意图: 问答意图 (answer_question)

用户: "帮我生成一份项目总结报告"
意图: 文档生成意图 (generate_document)
```

**实现方式**:
- 使用LLM分类
- 输入：用户问题 + 对话历史
- 输出：意图类型、置信度、提取的参数

---

## 🚀 快速入门指南

### 如何启动系统

#### 方法1: 一键启动（推荐）

```bash
cd /root/consult
./start.sh
```

这个脚本会：
1. ✅ 启动后端服务（端口18000）
2. ✅ 启动前端服务（端口3000）
3. ✅ 启动Nginx（端口13000）
4. ✅ 等待所有服务就绪

#### 方法2: 分别启动

**启动后端**:
```bash
cd /root/consult
./start_backend.sh
```

**启动前端**:
```bash
cd /root/consult
./start_frontend.sh
```

#### 方法3: 前台启动（开发调试）

**后端前台启动**:
```bash
cd /root/consult/backend
source venv/bin/activate
python app_simple.py
```

**前端前台启动**:
```bash
cd /root/consult/frontend
npm run dev
```

### 如何停止系统

```bash
cd /root/consult
./stop.sh
```

### 如何访问

1. **前端界面**: http://localhost:13000
2. **后端API文档**: http://localhost:18000/docs
3. **健康检查**: http://localhost:18000/api/health

### 查看日志

```bash
# 系统日志
tail -f /root/consult/system.log

# 前端日志
tail -f /root/consult/frontend.log

# 后端日志
tail -f /root/consult/backend/debug.log
```

---

## 📝 使用示例

### 完整使用流程

#### 方式A: 上传到全局数据库（所有工作区共享）

1. 打开浏览器，访问 http://localhost:13000
2. 点击"数据库管理"或"全局数据库"
3. 选择要上传的文档（如：公司规章制度.pdf）
4. 点击"上传"
5. 等待文档处理完成（可以看到进度条）
6. 上传的文档会被添加到 `global_vector_db/`，所有工作区都可以检索

#### 方式B: 上传到工作区（项目专属）

1. 打开浏览器，访问 http://localhost:13000
2. 选择一个工作区（或创建新工作区）
3. 在工作区页面点击"上传文档"
4. 选择要上传的文档（如：项目需求文档.pdf）
5. 点击"上传"
6. 上传的文档会被添加到 `workspace_{id}/`，只有当前工作区可以检索

#### 步骤2: 开始对话

1. 在页面右侧选择工作区（或使用默认工作区）
2. 在对话框输入问题："这份报告的核心内容是什么？"
3. 点击发送
4. AI会基于上传的文档生成答案
5. 您可以继续提问更多问题

#### 步骤3: 查看参考文献

1. AI回答后，会显示参考文档
2. 点击参考文档可以看到原文片段
3. 验证答案的准确性

### 常见问题类型

#### Q1: 总结类问题
```
"这份报告讲了什么？"
"文档的主要内容是什么？"
"能给我一个总结吗？"
```

#### Q2: 查找类问题
```
"报告中提到的时间节点是什么？"
"文档中有哪些关键数据？"
"找到所有相关的金额信息"
```

#### Q3: 比较类问题
```
"这个月和上个月的差异在哪里？"
"不同方案的优缺点是什么？"
```

#### Q4: 生成类需求
```
"帮我生成一份项目总结"
"创建一个Excel表格，包含所有数据"
"生成一份Word报告"
```

---

## 🔍 系统工作原理解析

### 为什么系统能"理解"文档？

系统并不是真正理解文档，而是使用了相似度匹配：

```
1. 问题 → 向量化 → 向量A
2. 文档片段 → 向量化 → 向量B
3. 计算向量A和向量B的相似度
4. 找到最相似的文档片段
5. 将这些片段作为上下文，让LLM生成答案
```

### 为什么有对话历史？

系统会记住之前的对话，这样您可以：

```
您: "这份报告讲了什么？"
AI: "这份报告主要介绍了AI技术的发展趋势..."
您: "具体的趋势是什么？"
AI: "根据刚才的报告，主要包括三个趋势..." (AI知道您指的是之前的报告)
```

### 工作区和全局数据库的区别？

- **工作区数据库**: 只属于当前工作区，其他工作区看不到
- **全局数据库**: 所有工作区都能访问，适合存放公共知识

**使用场景**:
```
工作区1: 项目A的文档 → 只能回答项目A的问题
全局数据库: 公司制度文档 → 所有工作区都可以引用
```

**实际操作示例**:

1. **向全局数据库上传文档**:
```bash
curl -X POST http://localhost:18000/api/global/documents/upload \
  -F "file=@项目规范.pdf"
```
文档会被存储在 `global_vector_db/` 中，所有工作区都可以检索到

2. **向工作区上传文档**:
```bash
curl -X POST http://localhost:18000/api/workspaces/1/documents/upload \
  -F "file=@项目A需求.pdf"
```
文档会被存储在 `workspace_1/` 中，只有工作区1可以检索到

3. **检索时的优先级**:
当您在工作区1提问时，系统会：
- 优先检索工作区1的文档
- 同时检索全局数据库的文档
- 混合返回最相关的结果

---

## 🐛 常见问题排查

### 问题1: 服务启动失败

**现象**: 运行`./start.sh`后报错

**可能原因**:
1. 端口被占用
2. 依赖未安装

**解决方法**:
```bash
# 检查端口占用
netstat -tlnp | grep 18000
netstat -tlnp | grep 3000

# 停止占用端口的进程
./stop.sh

# 重新安装依赖
./setup.sh
```

### 问题2: 文档上传失败

**现象**: 点击上传后没有反应

**排查步骤**:
```bash
# 查看后端日志
tail -f /root/consult/backend/debug.log

# 检查文件大小限制（默认50MB）
# 检查文件格式是否支持
```

### 问题3: 问答没有答案

**现象**: AI回答"抱歉，没有找到相关文档"

**可能原因**:
1. 文档未上传成功
2. 文档未完成向量化
3. 问题与文档内容不匹配

**解决方法**:
```bash
# 检查文档状态
curl http://localhost:18000/api/database/content

# 等待文档处理完成
# 重新上传文档
```

---

## 📊 性能优化说明

### 1. 向量缓存

系统会缓存常见的检索结果，提高响应速度。

### 2. 批量处理

上传多个文档时，系统会并行处理，加速向量化。

### 3. 增量索引

只对新文档进行向量化，已存在的文档不会重复处理。

### 4. 内存管理

- 使用内存缓存加速检索
- 定期清理无用数据
- 限制并发处理数量

---

## 🎓 学习建议

### 想深入理解系统？

1. **先理解RAG概念**
   - 阅读RAG相关文档
   - 了解向量数据库原理

2. **查看代码细节**
   - `backend/app_simple.py` - 后端主逻辑
   - `frontend/lib/api-client.ts` - API调用
   - `backend/app/services/langchain_rag_service.py` - RAG实现

3. **测试不同场景**
   - 上传不同类型的文档
   - 尝试不同的提问方式
   - 观察系统响应

4. **阅读日志**
   - 查看处理流程
   - 理解错误信息
   - 分析性能数据

---

## 🎉 总结

这个AI咨询平台通过以下技术实现了智能问答：

1. **文档处理** - 解析、分块、向量化
2. **向量检索** - 快速找到相关文档
3. **答案生成** - 基于检索内容生成答案
4. **对话管理** - 记住上下文，支持多轮对话

希望这个文档帮助您理解系统的运行原理！如有疑问，欢迎查看代码或日志。
