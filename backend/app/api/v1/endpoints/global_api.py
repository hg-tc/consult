"""
å…¨å±€æ–‡æ¡£åº“APIç«¯ç‚¹
æ”¯æŒå…¬å…±æ–‡æ¡£åº“å’Œå·¥ä½œåŒºåˆ†ç¦»çš„æ¶æ„
"""

from fastapi import APIRouter, Depends, HTTPException, UploadFile, File, BackgroundTasks
from typing import List, Dict, Any, Optional
import os
import uuid
import logging
from pathlib import Path
from datetime import datetime

# ç®€åŒ–å¯¼å…¥ï¼Œé¿å…å¤æ‚çš„ä¾èµ–
# from app.models.global_database import GlobalDatabaseService
# from app.services.global_rag_service import GlobalRAGService
# from app.core.database import get_db
from app.services.langchain_rag_service import LangChainRAGService
from app.services.performance_optimizer import get_performance_optimizer
from app.services.file_index_manager import file_index_manager
# å…¨å±€RAGæœåŠ¡å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
_global_rag_service = None

def get_global_rag_service():
    """è·å–å…¨å±€RAGæœåŠ¡å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _global_rag_service
    if _global_rag_service is None:
        # ä½¿ç”¨global_dbç›®å½•ï¼Œis_global=Trueæ ‡è®°è¿™ä¸æ˜¯ä¸€ä¸ªå·¥ä½œåŒº
        _global_rag_service = LangChainRAGService(vector_db_path="global_db", is_global=True)
    return _global_rag_service

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/global", tags=["global"])

# ç®€åŒ–çš„å…¨å±€æœåŠ¡å®ä¾‹
performance_optimizer = get_performance_optimizer()  # æ€§èƒ½ä¼˜åŒ–å™¨

# æŒä¹…åŒ–å­˜å‚¨è·¯å¾„ï¼ˆä»é…ç½®ä¸­è·å–ï¼‰
from app.core.config import settings

GLOBAL_DATA_DIR = Path(settings.GLOBAL_DATA_PATH)
GLOBAL_DOCUMENTS_FILE = GLOBAL_DATA_DIR / "documents.json"
GLOBAL_WORKSPACES_FILE = GLOBAL_DATA_DIR / "workspaces.json"

# ç¡®ä¿ç›®å½•å­˜åœ¨
GLOBAL_DATA_DIR.mkdir(parents=True, exist_ok=True)

def load_global_documents():
    """ä»æ–‡ä»¶åŠ è½½å…¨å±€æ–‡æ¡£"""
    if GLOBAL_DOCUMENTS_FILE.exists():
        try:
            import json
            with open(GLOBAL_DOCUMENTS_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"åŠ è½½å…¨å±€æ–‡æ¡£å¤±è´¥: {e}")
    return []

def save_global_documents(documents):
    """ä¿å­˜å…¨å±€æ–‡æ¡£åˆ°æ–‡ä»¶"""
    try:
        import json
        with open(GLOBAL_DOCUMENTS_FILE, 'w', encoding='utf-8') as f:
            json.dump(documents, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"ä¿å­˜å…¨å±€æ–‡æ¡£å¤±è´¥: {e}")

def load_global_workspaces():
    """ä»æ–‡ä»¶åŠ è½½å…¨å±€å·¥ä½œåŒº"""
    if GLOBAL_WORKSPACES_FILE.exists():
        try:
            import json
            with open(GLOBAL_WORKSPACES_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"åŠ è½½å…¨å±€å·¥ä½œåŒºå¤±è´¥: {e}")
    return []

def save_global_workspaces(workspaces):
    """ä¿å­˜å…¨å±€å·¥ä½œåŒºåˆ°æ–‡ä»¶"""
    try:
        import json
        with open(GLOBAL_WORKSPACES_FILE, 'w', encoding='utf-8') as f:
            json.dump(workspaces, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"ä¿å­˜å…¨å±€å·¥ä½œåŒºå¤±è´¥: {e}")

def get_global_services():
    """è·å–ç®€åŒ–çš„å…¨å±€æœåŠ¡å®ä¾‹"""
    documents = load_global_documents()
    workspaces = load_global_workspaces()
    return documents, workspaces


@router.post("/documents/upload")
async def upload_global_document(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...)
):
    """ä¸Šä¼ æ–‡æ¡£åˆ°å…¨å±€æ–‡æ¡£åº“"""
    try:
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹
        if not file.filename:
            raise HTTPException(status_code=400, detail="æ–‡ä»¶åä¸èƒ½ä¸ºç©º")
        
        file_ext = os.path.splitext(file.filename)[1].lower()
        allowed_extensions = [
            '.pdf', '.docx', '.doc', '.txt', '.md', '.zip', '.rar',
            '.xlsx', '.xls', '.pptx', '.ppt',
            '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff'
        ]
        
        if file_ext not in allowed_extensions:
            raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}")
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        content = await file.read()
        file_size = len(content)
        
        if file_size > 50 * 1024 * 1024:  # 50MBé™åˆ¶
            raise HTTPException(status_code=413, detail="æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶")
        
        # ç¬¬ä¸€æ­¥ï¼šå¿«é€Ÿä¿å­˜æ–‡ä»¶
        file_id = str(uuid.uuid4())
        filename = f"{file_id}{file_ext}"
        upload_dir = Path("uploads/global")
        upload_dir.mkdir(parents=True, exist_ok=True)
        file_path = upload_dir / filename
        
        with open(file_path, "wb") as f:
            f.write(content)
        
        # åˆ›å»ºæ–‡æ¡£è®°å½•ï¼ˆåˆå§‹çŠ¶æ€ä¸ºuploadedï¼‰
        document_data = {
            'id': file_id,
            'filename': filename,
            'original_filename': file.filename,
            'file_size': file_size,
            'file_path': str(file_path),
            'mime_type': file.content_type or "application/octet-stream",
            'status': 'uploaded',
            'created_at': datetime.now().isoformat(),
            'processing_started': None,
            'processing_completed': None,
            'chunk_count': 0,
            'quality_score': 0.0
        }
        
        # å¦‚æœæ˜¯ZIPæˆ–RARæ–‡ä»¶ï¼Œä½¿ç”¨ä¸“é—¨çš„å½’æ¡£å¤„ç†
        if file_ext in ['.zip', '.rar']:
            # åˆ›å»ºå½’æ¡£å¤„ç†ä»»åŠ¡
            from app.services.task_queue import get_task_queue, TaskStage
            
            task_queue = get_task_queue()
            task_id = task_queue.create_task(
                task_type="global_archive_processing",
                metadata={
                    'original_filename': file.filename,
                    'file_path': str(file_path),
                    'file_size': file_size,
                    'document_id': file_id,
                    'file_type': file_ext
                },
                workspace_id="global"
            )
            
            # å½’æ¡£æ–‡ä»¶ä¸ä¿å­˜åˆ°JSONï¼Œåªç”±å†…éƒ¨æ–‡ä»¶è®°å½•ä¿å­˜
            # æ›´æ–°è¿›åº¦
            task_queue.update_task_progress(
                task_id=task_id,
                stage=TaskStage.UPLOADING,
                progress=100,
                message="å½’æ¡£æ–‡ä»¶ä¸Šä¼ å®Œæˆ"
            )
            logger.info(f"è¿˜æ²¡å¼€å§‹process_zip_asyncå‡½æ•°")
            # ä½¿ç”¨app_simple.pyä¸­çš„process_zip_asyncå‡½æ•°
            from app_simple import process_zip_async
            import asyncio
            asyncio.create_task(process_zip_async(task_id, str(file_path), "global"))
            logger.info(f"å·²ç»å®Œæˆäº†process_zip_asyncå‡½æ•°")
            return {
                "id": file_id,
                "task_id": task_id,
                "filename": filename,
                "original_filename": file.filename,
                "file_size": file_size,
                "status": "uploaded",
                "file_type": file_ext,
                "message": f"{'ZIP' if file_ext == '.zip' else 'RAR'}æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œæ­£åœ¨è§£å‹å¹¶å¤„ç†...",
                "processing_status": "queued"
            }
        
        # ä¿å­˜æ–‡æ¡£è®°å½•åˆ°JSONæ–‡ä»¶
        documents = load_global_documents()
        documents.append(document_data)
        save_global_documents(documents)
        
        # ç¬¬äºŒæ­¥ï¼šåˆ›å»ºTaskQueueä»»åŠ¡å¹¶åå°å¼‚æ­¥å¤„ç†æ–‡æ¡£ï¼ˆä¸é˜»å¡å“åº”ï¼‰
        from app.services.task_queue import get_task_queue, TaskStage
        
        task_queue = get_task_queue()
        task_id = task_queue.create_task(
            task_type="global_document_processing",
            metadata={
                'original_filename': file.filename,
                'file_path': str(file_path),
                'file_size': file_size,
                'document_id': file_id
            },
            workspace_id="global"
        )
        
        # åå°å¤„ç†ï¼ˆä½¿ç”¨TaskQueueï¼‰
        import asyncio
        asyncio.create_task(
            process_global_document_with_progress(
                str(file_path), 
                document_data, 
                task_id
            )
        )
        
        # ç«‹å³è¿”å›æˆåŠŸå“åº”
        return {
            "id": file_id,
            "task_id": task_id,  # æ–°å¢
            "filename": filename,
            "original_filename": file.filename,
            "file_size": file_size,
            "status": "uploaded",
            "message": "æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œæ­£åœ¨åå°å¤„ç†ä¸­...",
            "processing_status": "queued"
        }
        
    except Exception as e:
        logger.error(f"å…¨å±€æ–‡æ¡£ä¸Šä¼ å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ä¸Šä¼ å¤±è´¥: {str(e)}")


def process_global_document_async(file_path: str, document_data: Dict[str, Any]):
    """å¼‚æ­¥åå°å¤„ç†å…¨å±€æ–‡æ¡£ï¼ˆåˆ†æ­¥å¤„ç†ï¼‰"""
    import asyncio
    asyncio.run(process_global_document_step_by_step(file_path, document_data))

async def process_global_document_with_progress(
    file_path: str, 
    document_data: Dict[str, Any],
    task_id: str
):
    """å¸¦è¿›åº¦æ›´æ–°çš„æ–‡æ¡£å¤„ç†"""
    from app.services.task_queue import get_task_queue, TaskStage
    import asyncio
    
    task_queue = get_task_queue()
    
    try:
        doc_id = document_data['id']
        logger.info(f"ğŸš€ å¼€å§‹å¸¦è¿›åº¦å¤„ç†çš„å…¨å±€æ–‡æ¡£: {file_path}, ID: {doc_id}, Task: {task_id}")
        
        # å¼€å§‹ä»»åŠ¡
        task_queue.start_task(task_id)
        
        # é˜¶æ®µ1: è§£ææ–‡æ¡£ (0-30%)
        task_queue.update_task_progress(
            task_id, 
            TaskStage.PARSING, 
            10, 
            "æ­£åœ¨è§£ææ–‡æ¡£..."
        )
        
        # æ·»åŠ å»¶è¿Ÿä»¥ä¾¿è§‚å¯Ÿè¿›åº¦
        await asyncio.sleep(0.5)
        
        # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
        await update_document_status(doc_id, 'processing', processing_started=datetime.now().isoformat())
        
        # è°ƒç”¨RAGæœåŠ¡
        global_rag = get_global_rag_service()
        
        logger.info(f"ğŸ“„ å¼€å§‹è§£ææ–‡æ¡£: {document_data['original_filename']}")
        
        # é˜¶æ®µ2: åˆ†å— (30-50%)
        task_queue.update_task_progress(
            task_id, 
            TaskStage.CHUNKING, 
            40, 
            "æ­£åœ¨åˆ†å‰²æ–‡æ¡£..."
        )
        
        await asyncio.sleep(0.5)
        
        # é˜¶æ®µ3: å‘é‡åŒ– (50-80%)
        task_queue.update_task_progress(
            task_id, 
            TaskStage.VECTORIZING, 
            60, 
            "æ­£åœ¨ç”Ÿæˆå‘é‡..."
        )
        
        # ä½¿ç”¨ LlamaIndex å¯¼å…¥ï¼ˆå¢åŠ è¶…æ—¶ã€å¿ƒè·³ä¸ç»†ç²’åº¦æ—¥å¿—ï¼‰
        logger.info(f"import ä¹‹å‰")
        from app.services.llamaindex_retriever import LlamaIndexRetriever
        logger.info(f"import ä¹‹å")
        import asyncio as _asyncio
        retriever = LlamaIndexRetriever.get_instance("global")
        logger.info(f"ğŸ”§ LlamaIndex add_document å¼€å§‹: path={file_path}, size={document_data['file_size']}, mime={document_data['mime_type']}")
        add_task = _asyncio.create_task(
            retriever.add_document(
                file_path=file_path,
                metadata={
                    'document_id': doc_id,
                    'original_filename': document_data['original_filename'],
                    'file_type': document_data['mime_type'],
                    'file_size': document_data['file_size'],
                    'upload_time': document_data['created_at']
                }
            )
        )
        # å¿ƒè·³è½®è¯¢ï¼šæ¯30ç§’æ›´æ–°ä¸€æ¬¡ï¼Œæœ€å¤§ç­‰å¾…600ç§’
        heartbeat_progress = 65
        total_wait = 0
        added_count = 0
        try:
            while True:
                try:
                    added_count = await _asyncio.wait_for(add_task, timeout=30)
                    break
                except _asyncio.TimeoutError:
                    total_wait += 30
                    # å¿ƒè·³æ›´æ–°
                    task_queue.update_task_progress(
                        task_id,
                        TaskStage.VECTORIZING,
                        min(heartbeat_progress, 85),
                        f"å‘é‡åŒ–è¿›è¡Œä¸­... å·²ç­‰å¾… {total_wait}s"
                    )
                    logger.info(f"â³ add_document å¿ƒè·³: ç­‰å¾… {total_wait}s, progress={heartbeat_progress}%")
                    heartbeat_progress += 5
                    if total_wait >= 600:
                        raise _asyncio.TimeoutError()
        except _asyncio.TimeoutError:
            logger.error(f"â° LlamaIndex add_document è¶…æ—¶(600s): {file_path}")
            await update_document_status(doc_id, 'failed', processing_completed=datetime.now().isoformat())
            task_queue.fail_task(task_id, "LlamaIndex add_document è¶…æ—¶(600s)")
            return
        except Exception as e:
            logger.error(f"âŒ LlamaIndex add_document å¤±è´¥: {e}")
            await update_document_status(doc_id, 'failed', processing_completed=datetime.now().isoformat())
            task_queue.fail_task(task_id, f"LlamaIndex å¤±è´¥: {str(e)}")
            return
        finally:
            logger.info(f"ğŸ”§ LlamaIndex add_document ç»“æŸï¼Œadded_count={added_count}")

        # æŒä¹…åŒ–ä¹ŸåŠ å…¥è¶…æ—¶ä¸å¿ƒè·³
        logger.info("ğŸ’¾ å¼€å§‹æŒä¹…åŒ–ç´¢å¼•åˆ°å­˜å‚¨ç›®å½•")
        persist_task = _asyncio.create_task(_asyncio.to_thread(retriever.index.storage_context.persist, persist_dir=str(retriever.storage_dir)))
        persist_wait = 0
        try:
            while True:
                try:
                    await _asyncio.wait_for(persist_task, timeout=30)
                    break
                except _asyncio.TimeoutError:
                    persist_wait += 30
                    task_queue.update_task_progress(
                        task_id,
                        TaskStage.INDEXING,
                        90,
                        f"æŒä¹…åŒ–ç´¢å¼•ä¸­... å·²ç­‰å¾… {persist_wait}s"
                    )
                    logger.info(f"â³ persist å¿ƒè·³: ç­‰å¾… {persist_wait}s")
                    if persist_wait >= 600:
                        raise _asyncio.TimeoutError()
        except _asyncio.TimeoutError:
            logger.error("â° æŒä¹…åŒ–ç´¢å¼•è¶…æ—¶(600s)")
            await update_document_status(doc_id, 'failed', processing_completed=datetime.now().isoformat())
            task_queue.fail_task(task_id, "æŒä¹…åŒ–ç´¢å¼•è¶…æ—¶(600s)")
            return
        success = bool(added_count)
        
        if success:
            logger.info(f"âœ… æ–‡æ¡£è§£æå’Œå‘é‡åŒ–æˆåŠŸ: {document_data['original_filename']}ï¼Œæ–°å¢ {added_count} ä¸ªèŠ‚ç‚¹")
            
            # ç»Ÿè®¡æ–°å¢èŠ‚ç‚¹æ•°
            chunk_count = int(added_count) if added_count else 0
            chunk_ids = []
            
            # é˜¶æ®µ4: ç´¢å¼•æ„å»º (80-100%)
            task_queue.update_task_progress(
                task_id, 
                TaskStage.INDEXING, 
                90, 
                "æ­£åœ¨å»ºç«‹ç´¢å¼•..."
            )
            
            await asyncio.sleep(0.5)
            
            # æ›´æ–°ç´¢å¼•ï¼šæ·»åŠ æ–‡ä»¶ä¿¡æ¯
            file_index_manager.add_file(
                file_id=doc_id,
                filename=document_data['filename'],
                original_filename=document_data['original_filename'],
                chunk_ids=chunk_ids,
                file_size=document_data['file_size'],
                file_type=document_data['mime_type'],
                file_path=str(file_path),
                upload_time=document_data['created_at']
            )
            
            # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†å®Œæˆ
            await update_document_status(
                doc_id, 
                'completed', 
                processing_completed=datetime.now().isoformat(),
                chunk_count=chunk_count,
                quality_score=0.8  # å¯ä»¥æ ¹æ®å®é™…å¤„ç†ç»“æœè°ƒæ•´
            )
            
            # å®Œæˆä»»åŠ¡
            task_queue.complete_task(task_id, {
                'chunk_count': chunk_count,
                'document_id': doc_id
            })
            
            logger.info(f"ğŸ‰ æ–‡æ¡£å¤„ç†å®Œæˆ: {document_data['original_filename']}, ç”Ÿæˆ {chunk_count} ä¸ªå‘é‡å—")
            
        else:
            logger.warning(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {document_data['original_filename']}")
            await update_document_status(doc_id, 'failed', processing_completed=datetime.now().isoformat())
            task_queue.fail_task(task_id, "æ–‡æ¡£å¤„ç†å¤±è´¥")
            
    except Exception as e:
        logger.error(f"âŒ å¸¦è¿›åº¦æ–‡æ¡£å¤„ç†å¼‚å¸¸: {file_path}, é”™è¯¯: {str(e)}")
        await update_document_status(document_data['id'], 'failed', processing_completed=datetime.now().isoformat())
        task_queue.fail_task(task_id, str(e))

async def process_global_document_step_by_step(file_path: str, document_data: Dict[str, Any]):
    """åˆ†æ­¥å¤„ç†å…¨å±€æ–‡æ¡£ï¼šè§£æ -> LlamaIndex å¯¼å…¥ -> æŒä¹…åŒ–"""
    try:
        doc_id = document_data['id']
        logger.info(f"ğŸš€ å¼€å§‹åˆ†æ­¥å¤„ç†å…¨å±€æ–‡æ¡£: {file_path}, ID: {doc_id}")
        
        # ç¬¬ä¸€æ­¥ï¼šæ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
        await update_document_status(doc_id, 'processing', processing_started=datetime.now().isoformat())
        
        # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨ LlamaIndex å¤„ç†æ–‡æ¡£
        try:
            from app.services.llamaindex_retriever import LlamaIndexRetriever
            logger.info(f"ğŸ“„ å¼€å§‹è§£ææ–‡æ¡£: {document_data['original_filename']}")

            retriever = LlamaIndexRetriever.get_instance("global")
            import asyncio as _asyncio
            logger.info(f"ğŸ”§[step] LlamaIndex add_document å¼€å§‹: {file_path}")
            add_task = _asyncio.create_task(
                retriever.add_document(
                    file_path=file_path,
                    metadata={
                        'document_id': doc_id,
                        'original_filename': document_data['original_filename'],
                        'file_type': document_data['mime_type'],
                        'file_size': document_data['file_size'],
                        'upload_time': document_data['created_at']
                    }
                )
            )
            hb = 65
            waited = 0
            try:
                while True:
                    try:
                        added_count = await _asyncio.wait_for(add_task, timeout=30)
                        break
                    except _asyncio.TimeoutError:
                        waited += 30
                        logger.info(f"â³[step] add_document å¿ƒè·³: ç­‰å¾… {waited}s, progress={hb}%")
                        hb = min(hb + 5, 85)
                        if waited >= 600:
                            raise _asyncio.TimeoutError()
            except _asyncio.TimeoutError:
                logger.error(f"â°[step] LlamaIndex add_document è¶…æ—¶(600s): {file_path}")
                await update_document_status(doc_id, 'failed', processing_completed=datetime.now().isoformat())
                return
            except Exception as e:
                logger.error(f"âŒ[step] LlamaIndex add_document å¤±è´¥: {e}")
                await update_document_status(doc_id, 'failed', processing_completed=datetime.now().isoformat())
                return
            finally:
                logger.info("ğŸ”§[step] LlamaIndex add_document ç»“æŸ")
            # æŒä¹…åŒ–
            logger.info("ğŸ’¾[step] å¼€å§‹æŒä¹…åŒ–ç´¢å¼•åˆ°å­˜å‚¨ç›®å½•")
            persist_task = _asyncio.create_task(_asyncio.to_thread(retriever.index.storage_context.persist, persist_dir=str(retriever.storage_dir)))
            waited_persist = 0
            try:
                while True:
                    try:
                        await _asyncio.wait_for(persist_task, timeout=30)
                        break
                    except _asyncio.TimeoutError:
                        waited_persist += 30
                        logger.info(f"â³[step] persist å¿ƒè·³: ç­‰å¾… {waited_persist}s")
                        if waited_persist >= 600:
                            raise _asyncio.TimeoutError()
            except _asyncio.TimeoutError:
                logger.error("â°[step] æŒä¹…åŒ–ç´¢å¼•è¶…æ—¶(600s)")
                await update_document_status(doc_id, 'failed', processing_completed=datetime.now().isoformat())
                return

            if added_count:
                logger.info(f"âœ… æ–‡æ¡£è§£æå’Œå…¥åº“æˆåŠŸ: {document_data['original_filename']}ï¼Œæ–°èŠ‚ç‚¹: {added_count}")
                # ç®€åŒ–ï¼šæ— æ³•ä» LlamaIndex ç›´æ¥æŒ‰ doc_id ç»Ÿè®¡ chunk æ•°ï¼›è®°å½•èŠ‚ç‚¹æ•°å³å¯
                chunk_count = int(added_count)
                chunk_ids = []

                # æ›´æ–°ç´¢å¼•ï¼šæ·»åŠ æ–‡ä»¶ä¿¡æ¯
                file_index_manager.add_file(
                    file_id=doc_id,
                    filename=document_data['filename'],
                    original_filename=document_data['original_filename'],
                    chunk_ids=chunk_ids,
                    file_size=document_data['file_size'],
                    file_type=document_data['mime_type'],
                    file_path=str(file_path),
                    upload_time=document_data['created_at']
                )
                
                # ç¬¬ä¸‰æ­¥ï¼šæ›´æ–°çŠ¶æ€ä¸ºå¤„ç†å®Œæˆ
                await update_document_status(
                    doc_id, 
                    'completed', 
                    processing_completed=datetime.now().isoformat(),
                    chunk_count=chunk_count,
                    quality_score=0.8  # å¯ä»¥æ ¹æ®å®é™…å¤„ç†ç»“æœè°ƒæ•´
                )
                
                logger.info(f"ğŸ‰ æ–‡æ¡£å¤„ç†å®Œæˆ: {document_data['original_filename']}, ç”Ÿæˆ {chunk_count} ä¸ªå‘é‡å—")
                
                # ç¬¬å››æ­¥ï¼šå¯é€‰ - åˆ é™¤åŸå§‹æ–‡ä»¶ä»¥èŠ‚çœç©ºé—´
                # await cleanup_original_file(file_path, doc_id)
                
            else:
                logger.warning(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {document_data['original_filename']}")
                await update_document_status(doc_id, 'failed', processing_completed=datetime.now().isoformat())
                
        except Exception as rag_error:
            logger.error(f"âŒ RAGæœåŠ¡å¤„ç†å¤±è´¥: {str(rag_error)}")
            await update_document_status(doc_id, 'failed', processing_completed=datetime.now().isoformat())
        
    except Exception as e:
        logger.error(f"âŒ å…¨å±€æ–‡æ¡£å¤„ç†å¼‚å¸¸: {file_path}, é”™è¯¯: {str(e)}")
        await update_document_status(document_data['id'], 'failed', processing_completed=datetime.now().isoformat())

async def update_document_status(doc_id: str, status: str, **kwargs):
    """æ›´æ–°æ–‡æ¡£çŠ¶æ€"""
    try:
        documents = load_global_documents()
        for doc in documents:
            if doc['id'] == doc_id:
                doc['status'] = status
                for key, value in kwargs.items():
                    doc[key] = value
                break
        save_global_documents(documents)
        logger.info(f"ğŸ“ æ–‡æ¡£çŠ¶æ€å·²æ›´æ–°: {doc_id} -> {status}")
    except Exception as e:
        logger.error(f"æ›´æ–°æ–‡æ¡£çŠ¶æ€å¤±è´¥: {str(e)}")

async def cleanup_original_file(file_path: str, doc_id: str):
    """æ¸…ç†åŸå§‹æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰"""
    try:
        if os.path.exists(file_path):
            os.remove(file_path)
            logger.info(f"ğŸ—‘ï¸ å·²åˆ é™¤åŸå§‹æ–‡ä»¶: {file_path}")
            
            # æ›´æ–°æ–‡æ¡£è®°å½•
            documents = load_global_documents()
            for doc in documents:
                if doc['id'] == doc_id:
                    doc['original_file_deleted'] = True
                    doc['file_path'] = None  # æ¸…ç©ºæ–‡ä»¶è·¯å¾„
                    break
            save_global_documents(documents)
    except Exception as e:
        logger.error(f"æ¸…ç†åŸå§‹æ–‡ä»¶å¤±è´¥: {str(e)}")


@router.get("/documents/status/{doc_id}")
async def get_document_status(doc_id: str):
    """è·å–æ–‡æ¡£å¤„ç†çŠ¶æ€"""
    try:
        documents = load_global_documents()
        for doc in documents:
            if doc['id'] == doc_id:
                return {
                    "id": doc_id,
                    "status": doc.get('status', 'unknown'),
                    "original_filename": doc.get('original_filename', ''),
                    "processing_started": doc.get('processing_started'),
                    "processing_completed": doc.get('processing_completed'),
                    "chunk_count": doc.get('chunk_count', 0),
                    "quality_score": doc.get('quality_score', 0.0),
                    "file_size": doc.get('file_size', 0),
                    "created_at": doc.get('created_at')
                }
        
        raise HTTPException(status_code=404, detail="æ–‡æ¡£æœªæ‰¾åˆ°")
    except Exception as e:
        logger.error(f"è·å–æ–‡æ¡£çŠ¶æ€å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–çŠ¶æ€å¤±è´¥: {str(e)}")

@router.get("/documents")
async def list_global_documents():
    """åˆ—å‡ºæ‰€æœ‰å…¨å±€æ–‡æ¡£ï¼ˆåªä»JSONæ–‡ä»¶ï¼Œå¿«é€Ÿè¿”å›ï¼‰"""
    try:
        # ä» LlamaIndex å‘é‡åº“åŠ è½½ï¼ˆæ–°æ ¼å¼ï¼‰
        vector_documents = []
        try:
            from app.core.config import settings
            llamaindex_storage_dir = Path(settings.LLAMAINDEX_STORAGE_PATH) / "global"
            docstore_file = llamaindex_storage_dir / "docstore.json"
            
            if docstore_file.exists():
                import json
                with open(docstore_file, 'r', encoding='utf-8') as f:
                    docstore_data = json.load(f)
                
                # è§£æ LlamaIndex æ ¼å¼çš„ docstore
                file_groups = {}
                nodes = docstore_data.get('docstore/data', {})
                
                for node_id, node_data in nodes.items():
                    data = node_data.get('__data__', {})
                    metadata = data.get('metadata', {})
                    
                    original_filename = metadata.get('original_filename') or metadata.get('file_name', f"æ–‡æ¡£_{node_id[:8]}")
                    
                    if original_filename not in file_groups:
                        file_groups[original_filename] = {
                            "id": node_id,
                            "filename": original_filename,
                            "original_filename": original_filename,
                            "file_size": metadata.get('file_size', 0),
                            "file_type": metadata.get('file_type', metadata.get('mime_type', '')),
                            "status": "completed",
                            "created_at": metadata.get('upload_time', metadata.get('creation_date', '')),
                            "chunk_count": 1
                        }
                    else:
                        file_groups[original_filename]["chunk_count"] += 1
                
                vector_documents = list(file_groups.values())
                logger.info(f"âœ… ä» LlamaIndex å‘é‡åº“åŠ è½½äº† {len(vector_documents)} ä¸ªæ–‡æ¡£")
        except Exception as e:
            logger.warning(f"âš ï¸ ä» LlamaIndex åŠ è½½å¤±è´¥: {e}ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•")
        
        # ä»JSONæ–‡ä»¶åŠ è½½
        json_documents = load_global_documents()
        
        # åˆå¹¶ä¸¤ä¸ªæ•°æ®æºï¼Œå»é‡ï¼ˆä¼˜å…ˆä½¿ç”¨å‘é‡æ•°æ®åº“çš„æ•°æ®ï¼‰
        result_map = {}
        
        # å…ˆæ·»åŠ å‘é‡æ•°æ®åº“ä¸­çš„æ–‡æ¡£
        for doc in vector_documents:
            key = doc.get('original_filename', '')
            if key not in result_map:
                result_map[key] = doc
        
        # å†æ·»åŠ JSONæ–‡ä»¶ä¸­ä¸å­˜åœ¨äºå‘é‡æ•°æ®åº“çš„æ–‡æ¡£
        for doc in json_documents:
            # è·³è¿‡å½’æ¡£æ–‡ä»¶è®°å½•ï¼ˆå…¼å®¹å†å²æ•°æ®ï¼Œæ–°ä¸Šä¼ çš„å½’æ¡£æ–‡ä»¶ä¸ä¼šå†™å…¥JSONï¼‰
            if doc.get('is_archive', False):
                continue
                
            key = doc.get('original_filename', '')
            if key not in result_map:
                result_map[key] = {
                    "id": doc.get('id', ''),
                    "filename": doc.get('filename', ''),
                    "original_filename": doc.get('original_filename', ''),
                    "file_size": doc.get('file_size', 0),
                    "file_type": doc.get('file_type', ''),
                    "status": doc.get('status', 'completed'),
                    "created_at": doc.get('created_at', ''),
                    "chunk_count": doc.get('chunk_count', 0),
                    # æ·»åŠ chunk_idså­—æ®µï¼Œä½¿ç”¨idä½œä¸ºç¬¬ä¸€ä¸ªchunk ID
                    "chunk_ids": [doc.get('id', '')]
                }
        
        result_documents = list(result_map.values())
        
        logger.info(f"å…¨å±€æ–‡æ¡£åˆ—è¡¨: å‘é‡æ•°æ®åº“ {len(vector_documents)} ä¸ªï¼ŒJSONæ–‡ä»¶ {len(json_documents)} ä¸ªï¼Œåˆå¹¶å {len(result_documents)} ä¸ª")
        
        return {
            "documents": result_documents,
            "total_count": len(result_documents),
            "message": f"å…¨å±€æ–‡æ¡£åˆ—è¡¨ï¼ˆå‘é‡æ•°æ®åº“: {len(vector_documents)}, JSON: {len(json_documents)}ï¼‰"
        }
    except Exception as e:
        logger.error(f"è·å–å…¨å±€æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"è·å–å¤±è´¥: {str(e)}")


@router.post("/search")
async def search_global_documents(
    request: Dict[str, Any]
):
    """æœç´¢å…¨å±€æ–‡æ¡£"""
    try:
        # å¼€å§‹æ€§èƒ½è®¡æ—¶
        start_time = performance_optimizer.start_search_timer()
        
        query = request.get('query', '')
        workspace_id = request.get('workspace_id')
        top_k = request.get('top_k', 5)
        
        # æ£€æŸ¥ç¼“å­˜
        cached_result = performance_optimizer.get_cached_search_result(query, workspace_id or "global")
        if cached_result:
            logger.info(f"ä½¿ç”¨ç¼“å­˜æœç´¢ç»“æœ: {query}")
            return cached_result
        
        documents = load_global_documents()
        
        # ä½¿ç”¨RAGæœåŠ¡è¿›è¡ŒçœŸæ­£çš„æ–‡æ¡£æœç´¢
        results = []
        try:
            # åˆ›å»ºå…¨å±€RAGæœåŠ¡å®ä¾‹
            global_rag = get_global_rag_service()
            
            # ä½¿ç”¨RAGæœåŠ¡è¿›è¡ŒçœŸæ­£çš„æ–‡æ¡£æœç´¢
            # ç›´æ¥ä½¿ç”¨RAGæœåŠ¡çš„æœç´¢åŠŸèƒ½ï¼Œè€Œä¸æ˜¯é—®ç­”åŠŸèƒ½
            logger.info(f"å¼€å§‹RAGæœç´¢: {query}")
            search_results = await global_rag.ask_question(
                workspace_id="global",
                question=query,
                top_k=top_k
            )
            logger.info(f"RAGæœç´¢å®Œæˆï¼Œå¼•ç”¨æ•°é‡: {len(search_results.get('references', []))}")
            
            if search_results.get('references'):
                logger.info(f"å¤„ç† {len(search_results['references'])} ä¸ªå¼•ç”¨")
                for ref in search_results['references']:
                    # å®‰å…¨è·å–å†…å®¹ï¼Œå¤„ç†ä¸åŒçš„å¼•ç”¨ç»“æ„
                    content = ref.get('content', ref.get('page_content', ref.get('content_preview', '')))
                    logger.info(f"å¼•ç”¨å†…å®¹é•¿åº¦: {len(content)}")
                    if content:
                        results.append({
                            "content": content[:200] + "..." if len(content) > 200 else content,
                            "metadata": {
                                "document_id": ref.get('document_id', ''),
                                "chunk_id": ref.get('chunk_id', ''),
                                "similarity": ref.get('similarity', 0.0),
                                "original_filename": ref.get('metadata', {}).get('original_filename', 'è®¡ç½‘.pdf')
                            },
                            "similarity": ref.get('similarity', 0.0)
                        })
                        logger.info(f"æ·»åŠ ç»“æœï¼Œå½“å‰ç»“æœæ•°é‡: {len(results)}")
            else:
                logger.warning("RAGæœç´¢æ²¡æœ‰è¿”å›å¼•ç”¨")
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å†…å®¹ï¼Œå›é€€åˆ°æ–‡ä»¶ååŒ¹é…
            if not results:
                for doc in documents:
                    if query.lower() in doc['original_filename'].lower():
                        results.append({
                            'content': f"æ–‡ä»¶å: {doc['original_filename']}",
                            'metadata': doc,
                            'similarity': 0.8
                        })
                        
        except Exception as rag_error:
            logger.warning(f"RAGæœç´¢å¤±è´¥ï¼Œå›é€€åˆ°ç®€å•æœç´¢: {str(rag_error)}")
            # å›é€€åˆ°ç®€å•æœç´¢
            for doc in documents:
                if query.lower() in doc['original_filename'].lower():
                    results.append({
                        'content': f"æ–‡ä»¶å: {doc['original_filename']}",
                        'metadata': doc,
                        'similarity': 0.8
                    })
        
        search_result = {
            "query": query,
            "results": results[:top_k],
            "total_count": len(results),
            "workspace_id": workspace_id,
            "cached": False
        }
        
        # ç¼“å­˜æœç´¢ç»“æœ
        performance_optimizer.cache_search_result(query, workspace_id or "global", search_result)
        
        # ç»“æŸæ€§èƒ½è®¡æ—¶
        search_time = performance_optimizer.end_search_timer(start_time)
        logger.info(f"æœç´¢å®Œæˆ: {query}, è€—æ—¶: {search_time:.3f}s, ç»“æœæ•°: {len(results)}")
        
        return search_result
        
    except Exception as e:
        logger.error(f"å…¨å±€æ–‡æ¡£æœç´¢å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æœç´¢å¤±è´¥: {str(e)}")


@router.post("/chat")
async def global_chat(
    request: Dict[str, Any]
):
    """å…¨å±€é—®ç­”"""
    try:
        question = request.get('question', '')
        workspace_id = request.get('workspace_id')
        top_k = request.get('top_k', 5)
        
        documents = load_global_documents()
        
        # ä½¿ç”¨RAGæœåŠ¡è¿›è¡ŒçœŸæ­£çš„é—®ç­”
        try:
            # åˆ›å»ºå…¨å±€RAGæœåŠ¡å®ä¾‹
            global_rag = get_global_rag_service()
            
            # ä½¿ç”¨RAGæœåŠ¡è¿›è¡Œé—®ç­”
            rag_result = await global_rag.ask_question(
                workspace_id="global",
                question=question,
                top_k=top_k
            )
            
            if rag_result.get('answer') and rag_result.get('references'):
                return {
                    "answer": rag_result['answer'],
                    "references": rag_result['references'],
                    "confidence": rag_result.get('confidence', 0.7),
                    "metadata": {
                        "mode": "global_rag",
                        "document_count": len(documents),
                        "workspace_id": workspace_id
                    }
                }
            else:
                # å›é€€åˆ°ç®€å•é—®ç­”
                answer = f"æ ¹æ®å…¨å±€æ–‡æ¡£åº“ä¸­çš„ {len(documents)} ä¸ªæ–‡æ¡£ï¼Œæˆ‘æ‰¾åˆ°äº†ç›¸å…³ä¿¡æ¯ã€‚"
                if documents:
                    answer += f" å…¶ä¸­åŒ…æ‹¬: {', '.join([doc['original_filename'] for doc in documents[:3]])}"
                
                return {
                    "answer": answer,
                    "references": [],
                    "confidence": 0.7,
                    "metadata": {
                        "mode": "global_simple",
                        "document_count": len(documents),
                        "workspace_id": workspace_id
                    }
                }
                
        except Exception as rag_error:
            logger.warning(f"RAGé—®ç­”å¤±è´¥ï¼Œå›é€€åˆ°ç®€å•é—®ç­”: {str(rag_error)}")
            # å›é€€åˆ°ç®€å•é—®ç­”
            answer = f"æ ¹æ®å…¨å±€æ–‡æ¡£åº“ä¸­çš„ {len(documents)} ä¸ªæ–‡æ¡£ï¼Œæˆ‘æ‰¾åˆ°äº†ç›¸å…³ä¿¡æ¯ã€‚"
            if documents:
                answer += f" å…¶ä¸­åŒ…æ‹¬: {', '.join([doc['original_filename'] for doc in documents[:3]])}"
            
            return {
                "answer": answer,
                "references": [],
                "confidence": 0.7,
                "metadata": {
                    "mode": "global_simple",
                    "document_count": len(documents),
                    "workspace_id": workspace_id
                }
            }
        
    except Exception as e:
        logger.error(f"å…¨å±€é—®ç­”å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"é—®ç­”å¤±è´¥: {str(e)}")


@router.get("/stats")
async def get_global_stats():
    """è·å–å…¨å±€ç»Ÿè®¡ä¿¡æ¯"""
    try:
        documents = load_global_documents()
        workspaces = load_global_workspaces()
        
        stats = {
            'global_document_count': len(documents),
            'global_workspace_count': len(workspaces),
            'vector_store_available': False,  # ç®€åŒ–ç‰ˆæš‚æ—¶ä¸å¯ç”¨
            'embedding_model': 'simplified'
        }
        
        return stats
        
    except Exception as e:
        logger.error(f"è·å–å…¨å±€ç»Ÿè®¡å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–ç»Ÿè®¡å¤±è´¥: {str(e)}")


# å·¥ä½œåŒºç®¡ç†API
@router.post("/workspaces")
async def create_workspace(
    name: str,
    description: Optional[str] = None,
    settings: Optional[Dict[str, Any]] = None
):
    """åˆ›å»ºå·¥ä½œåŒº"""
    try:
        workspace_id = str(uuid.uuid4())
        
        # æ·»åŠ åˆ°æŒä¹…åŒ–å­˜å‚¨
        workspaces = load_global_workspaces()
        workspace_data = {
            "id": workspace_id,
            "name": name,
            "description": description,
            "settings": settings or {},
            "status": "active",
            "created_at": "2025-10-23T22:00:00Z"
        }
        workspaces.append(workspace_data)
        save_global_workspaces(workspaces)
        
        return workspace_data
        
    except Exception as e:
        logger.error(f"åˆ›å»ºå·¥ä½œåŒºå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºå¤±è´¥: {str(e)}")


@router.get("/workspaces")
async def list_workspaces():
    """åˆ—å‡ºæ‰€æœ‰å·¥ä½œåŒº"""
    try:
        workspaces = load_global_workspaces()
        
        # æ·»åŠ é»˜è®¤å·¥ä½œåŒº
        if not workspaces:
            default_workspace = {
                "id": "1",
                "name": "é»˜è®¤å·¥ä½œåŒº",
                "description": "åŒ…å«æ‰€æœ‰å…¨å±€æ–‡æ¡£çš„é»˜è®¤å·¥ä½œåŒº",
                "status": "active",
                "document_count": 0,
                "created_at": "2025-10-23T22:00:00Z"
            }
            workspaces.append(default_workspace)
        
        return {
            "workspaces": workspaces,
            "total_count": len(workspaces)
        }
        
    except Exception as e:
        logger.error(f"è·å–å·¥ä½œåŒºåˆ—è¡¨å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–å¤±è´¥: {str(e)}")


@router.post("/workspaces/{workspace_id}/documents/{document_id}/access")
async def grant_document_access(
    workspace_id: str,
    document_id: str,
    access_level: str = "read"
):
    """æˆäºˆå·¥ä½œåŒºæ–‡æ¡£è®¿é—®æƒé™"""
    try:
        # ç®€åŒ–å¤„ç†ï¼Œç›´æ¥è¿”å›æˆåŠŸ
        return {
            "workspace_id": workspace_id,
            "document_id": document_id,
            "access_level": access_level,
            "status": "granted"
        }
        
    except Exception as e:
        logger.error(f"æˆäºˆæ–‡æ¡£è®¿é—®æƒé™å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æˆæƒå¤±è´¥: {str(e)}")


@router.delete("/workspaces/{workspace_id}/documents/{document_id}/access")
async def revoke_document_access(
    workspace_id: str,
    document_id: str
):
    """æ’¤é”€å·¥ä½œåŒºæ–‡æ¡£è®¿é—®æƒé™"""
    try:
        # ç®€åŒ–å¤„ç†ï¼Œç›´æ¥è¿”å›æˆåŠŸ
        return {
            "workspace_id": workspace_id,
            "document_id": document_id,
            "status": "revoked"
        }
        
    except Exception as e:
        logger.error(f"æ’¤é”€æ–‡æ¡£è®¿é—®æƒé™å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ’¤é”€å¤±è´¥: {str(e)}")


# æ€§èƒ½ä¼˜åŒ–API
@router.get("/performance/stats")
async def get_performance_stats():
    """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
    try:
        stats = performance_optimizer.get_performance_stats()
        return stats
        
    except Exception as e:
        logger.error(f"è·å–æ€§èƒ½ç»Ÿè®¡å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–ç»Ÿè®¡å¤±è´¥: {str(e)}")


@router.post("/performance/optimize")
async def optimize_performance():
    """æ‰§è¡Œæ€§èƒ½ä¼˜åŒ–"""
    try:
        # ä¼˜åŒ–å‘é‡ç´¢å¼•
        from app.core.config import settings
        vector_db_path = settings.LANGCHAIN_VECTOR_DB_PATH
        optimization_result = performance_optimizer.optimize_vector_index(vector_db_path)
        
        # æ¸…ç†è¿‡æœŸç¼“å­˜
        cleanup_result = performance_optimizer.clear_cache()
        
        return {
            "success": True,
            "message": "æ€§èƒ½ä¼˜åŒ–å®Œæˆ",
            "vector_optimization": optimization_result,
            "cache_cleanup": cleanup_result
        }
        
    except Exception as e:
        logger.error(f"æ€§èƒ½ä¼˜åŒ–å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ä¼˜åŒ–å¤±è´¥: {str(e)}")


@router.delete("/performance/cache")
async def clear_performance_cache(cache_type: str = None):
    """æ¸…ç†æ€§èƒ½ç¼“å­˜"""
    try:
        result = performance_optimizer.clear_cache(cache_type)
        return result
        
    except Exception as e:
        logger.error(f"æ¸…ç†ç¼“å­˜å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ¸…ç†å¤±è´¥: {str(e)}")


@router.get("/documents/{doc_id}/download")
async def download_global_document(doc_id: str):
    """ä¸‹è½½å…¨å±€æ–‡æ¡£"""
    try:
        from fastapi.responses import FileResponse
        
        # æŸ¥æ‰¾æ–‡æ¡£æ–‡ä»¶è·¯å¾„
        rag_service = get_global_rag_service()
        vector_store = rag_service._load_vector_store("global")
        
        if vector_store and hasattr(vector_store, 'docstore'):
            docstore = vector_store.docstore
            if hasattr(docstore, '_dict') and doc_id in docstore._dict:
                doc = docstore._dict[doc_id]
                metadata = doc.metadata if hasattr(doc, 'metadata') else {}
                file_path = metadata.get('file_path')
                
                if file_path and os.path.exists(file_path):
                    filename = metadata.get('original_filename', f"document_{doc_id[:8]}")
                    
                    return FileResponse(
                        path=file_path,
                        filename=filename,
                        media_type='application/octet-stream'
                    )
        
        raise HTTPException(status_code=404, detail="æ–‡æ¡£æœªæ‰¾åˆ°")
        
    except Exception as e:
        logger.error(f"ä¸‹è½½å…¨å±€æ–‡æ¡£å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ä¸‹è½½å¤±è´¥: {str(e)}")


@router.delete("/documents/{doc_id}")
async def delete_global_document(doc_id: str):
    """åˆ é™¤å…¨å±€æ–‡æ¡£"""
    try:
        logger.info(f"æ”¶åˆ°åˆ é™¤å…¨å±€æ–‡æ¡£è¯·æ±‚: {doc_id}")
        
        # ä» LlamaIndex å‘é‡åº“ä¸­æŸ¥æ‰¾åŒ¹é…çš„æ–‡æ¡£
        original_filename = None
        all_node_ids = []
        file_path = None
        
        from app.core.config import settings
        llamaindex_storage_dir = Path(settings.LLAMAINDEX_STORAGE_PATH) / "global"
        docstore_file = llamaindex_storage_dir / "docstore.json"
        
        if docstore_file.exists():
            import json
            with open(docstore_file, 'r', encoding='utf-8') as f:
                docstore_data = json.load(f)
            
            nodes = docstore_data.get('docstore/data', {})
            
            # æŸ¥æ‰¾åŒ¹é…çš„æ–‡æ¡£
            for node_id, node_data in nodes.items():
                data = node_data.get('__data__', {})
                metadata = data.get('metadata', {})
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºåŒ¹é…çš„èŠ‚ç‚¹
                if node_id == doc_id or metadata.get('document_id') == doc_id:
                    original_filename = metadata.get('original_filename') or metadata.get('file_name', 'æœªçŸ¥æ–‡æ¡£')
                    file_path = metadata.get('file_path', '')
                    
                    # æ‰¾åˆ°æ‰€æœ‰å±äºè¿™ä¸ªæ–‡æ¡£çš„èŠ‚ç‚¹
                    for nid, ndata in nodes.items():
                        nd = ndata.get('__data__', {})
                        nmeta = nd.get('metadata', {})
                        nfilename = nmeta.get('original_filename') or nmeta.get('file_name', '')
                        
                        if nfilename == original_filename:
                            all_node_ids.append(nid)
                    break
        
        logger.info(f"æ‰¾åˆ°æ–‡ä»¶: {original_filename}, nodes: {len(all_node_ids)}")
        
        if not original_filename:
            logger.warning(f"æ–‡æ¡£ {doc_id} åœ¨å‘é‡æ•°æ®åº“ä¸­æœªæ‰¾åˆ°")
            raise HTTPException(status_code=404, detail="æ–‡æ¡£æœªæ‰¾åˆ°")
        
        # ä» LlamaIndex å‘é‡å­˜å‚¨ä¸­åˆ é™¤æ‰€æœ‰ç›¸å…³èŠ‚ç‚¹
        deleted_count = 0
        try:
            # åŠ è½½ LlamaIndex ç´¢å¼•
            from app.services.llamaindex_retriever import LlamaIndexRetriever
            retriever = LlamaIndexRetriever.get_instance("global")
            
            # è·å–å‘é‡å­˜å‚¨
            vector_store = retriever.index._vector_store if hasattr(retriever.index, '_vector_store') else None
            
            # åˆ é™¤æ‰€æœ‰ç›¸å…³èŠ‚ç‚¹
            for node_id in all_node_ids:
                # ä» docstore åˆ é™¤
                if hasattr(retriever.index, '_docstore') and retriever.index._docstore:
                    retriever.index._docstore.delete_document(node_id, raise_error=False)
                    deleted_count += 1
                    logger.debug(f"åˆ é™¤èŠ‚ç‚¹: {node_id}")
                
                # ä»å‘é‡å­˜å‚¨åˆ é™¤
                if vector_store and hasattr(vector_store, '_data'):
                    if node_id in vector_store._data.embedding_dict:
                        del vector_store._data.embedding_dict[node_id]
                        logger.debug(f"ä»å‘é‡å­˜å‚¨åˆ é™¤èŠ‚ç‚¹: {node_id}")
                    if node_id in vector_store._data.text_id_to_ref_doc_id:
                        del vector_store._data.text_id_to_ref_doc_id[node_id]
                    if node_id in vector_store._data.metadata_dict:
                        del vector_store._data.metadata_dict[node_id]
                
                # ä» index_store åˆ é™¤
                if hasattr(retriever.index, '_index_struct') and hasattr(retriever.index._index_struct, 'nodes_dict'):
                    if node_id in retriever.index._index_struct.nodes_dict:
                        del retriever.index._index_struct.nodes_dict[node_id]
            
            # æŒä¹…åŒ–æ›´æ”¹
            retriever.index.storage_context.persist(persist_dir=str(retriever.storage_dir))
            logger.info(f"æˆåŠŸåˆ é™¤ {deleted_count} ä¸ªèŠ‚ç‚¹å¹¶æŒä¹…åŒ–")
        except Exception as e:
            logger.error(f"åˆ é™¤èŠ‚ç‚¹å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
        
        # åˆ é™¤ç‰©ç†æ–‡ä»¶
        if file_path and os.path.exists(file_path):
            try:
                os.remove(file_path)
                logger.info(f"æˆåŠŸåˆ é™¤ç‰©ç†æ–‡ä»¶: {file_path}")
            except Exception as e:
                logger.warning(f"åˆ é™¤ç‰©ç†æ–‡ä»¶å¤±è´¥: {str(e)}")
        
        # ä» file_index_manager ä¸­åˆ é™¤ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        try:
            # æŸ¥æ‰¾æ–‡æ¡£çš„åŸå§‹ ID
            documents = load_global_documents()
            for doc in documents:
                if doc.get('original_filename') == original_filename:
                    file_index_manager.remove_file(doc.get('id'))
                    break
        except Exception as e:
            logger.warning(f"ä»ç´¢å¼•åˆ é™¤å¤±è´¥: {str(e)}")
        
        # ä» JSON æ–‡ä»¶ä¸­åˆ é™¤è®°å½•
        documents = load_global_documents()
        documents_before = len(documents)
        
        # åˆ é™¤æ‰€æœ‰åŒåæ–‡ä»¶è®°å½•
        documents = [doc for doc in documents if doc.get('original_filename') != original_filename]
        
        documents_after = len(documents)
        removed_from_json = documents_before - documents_after
        
        logger.info(f"ä»JSONä¸­åˆ é™¤äº† {removed_from_json} æ¡è®°å½•")
        
        # ä¿å­˜æ›´æ–°åçš„æ–‡æ¡£åˆ—è¡¨
        save_global_documents(documents)
        
        logger.info(f"æ–‡æ¡£åˆ é™¤å®Œæˆ: {original_filename}")
        
        return {
            "status": "deleted",
            "id": doc_id,
            "filename": original_filename,
            "message": f"æ–‡æ¡£ {original_filename} å·²æˆåŠŸåˆ é™¤"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ é™¤å…¨å±€æ–‡æ¡£å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ é™¤å¤±è´¥: {str(e)}")


@router.post("/rebuild-index")
async def rebuild_file_index():
    """é‡å»ºæ–‡ä»¶ç´¢å¼•ï¼ˆç”¨äºæ•°æ®æ¢å¤ï¼‰"""
    try:
        logger.info("å¼€å§‹é‡å»ºæ–‡ä»¶ç´¢å¼•...")
        
        rag_service = get_global_rag_service()
        vector_store = rag_service._load_vector_store("global")
        
        # é‡å»ºç´¢å¼•
        file_index_manager.rebuild_index_from_vector_store(vector_store)
        
        # è·å–é‡å»ºåçš„ç»Ÿè®¡ä¿¡æ¯
        file_count = file_index_manager.get_file_count()
        files = file_index_manager.list_files()
        
        total_chunks = sum(file_info.get("chunk_count", 0) for file_info in files)
        
        logger.info(f"ç´¢å¼•é‡å»ºå®Œæˆ: {file_count} ä¸ªæ–‡ä»¶, {total_chunks} ä¸ªå—")
        
        return {
            "status": "success",
            "message": "æ–‡ä»¶ç´¢å¼•é‡å»ºå®Œæˆ",
            "file_count": file_count,
            "total_chunks": total_chunks,
            "files": files
        }
        
    except Exception as e:
        logger.error(f"é‡å»ºæ–‡ä»¶ç´¢å¼•å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"é‡å»ºç´¢å¼•å¤±è´¥: {str(e)}")


@router.get("/index-status")
async def get_index_status():
    """è·å–ç´¢å¼•çŠ¶æ€"""
    try:
        file_count = file_index_manager.get_file_count()
        files = file_index_manager.list_files()
        
        total_chunks = sum(file_info.get("chunk_count", 0) for file_info in files)
        total_size = sum(file_info.get("file_size", 0) for file_info in files)
        
        return {
            "file_count": file_count,
            "total_chunks": total_chunks,
            "total_size": total_size,
            "index_file": str(file_index_manager.index_file),
            "files": files
        }
        
    except Exception as e:
        logger.error(f"è·å–ç´¢å¼•çŠ¶æ€å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–çŠ¶æ€å¤±è´¥: {str(e)}")
