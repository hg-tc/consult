"""
å…¨å±€æ–‡æ¡£åº“APIç«¯ç‚¹
æ”¯æŒå…¬å…±æ–‡æ¡£åº“å’Œå·¥ä½œåŒºåˆ†ç¦»çš„æ¶æ„
"""

from fastapi import APIRouter, Depends, HTTPException, UploadFile, File, BackgroundTasks
from typing import List, Dict, Any, Optional
import os
import uuid
import logging
from pathlib import Path
from datetime import datetime

# ç®€åŒ–å¯¼å…¥ï¼Œé¿å…å¤æ‚çš„ä¾èµ–
# from app.models.global_database import GlobalDatabaseService
# from app.services.global_rag_service import GlobalRAGService
# from app.core.database import get_db
from app.services.langchain_rag_service import LangChainRAGService
from app.services.performance_optimizer import get_performance_optimizer
from app.services.file_index_manager import file_index_manager
# å…¨å±€RAGæœåŠ¡å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
_global_rag_service = None

def get_global_rag_service():
    """è·å–å…¨å±€RAGæœåŠ¡å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _global_rag_service
    if _global_rag_service is None:
        _global_rag_service = LangChainRAGService(vector_db_path="global_vector_db")
    return _global_rag_service

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/global", tags=["global"])

# ç®€åŒ–çš„å…¨å±€æœåŠ¡å®ä¾‹
performance_optimizer = get_performance_optimizer()  # æ€§èƒ½ä¼˜åŒ–å™¨

# æŒä¹…åŒ–å­˜å‚¨è·¯å¾„
GLOBAL_DATA_DIR = Path("/root/workspace/consult/backend/global_data")
GLOBAL_DOCUMENTS_FILE = GLOBAL_DATA_DIR / "documents.json"
GLOBAL_WORKSPACES_FILE = GLOBAL_DATA_DIR / "workspaces.json"

# ç¡®ä¿ç›®å½•å­˜åœ¨
GLOBAL_DATA_DIR.mkdir(exist_ok=True)

def load_global_documents():
    """ä»æ–‡ä»¶åŠ è½½å…¨å±€æ–‡æ¡£"""
    if GLOBAL_DOCUMENTS_FILE.exists():
        try:
            import json
            with open(GLOBAL_DOCUMENTS_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"åŠ è½½å…¨å±€æ–‡æ¡£å¤±è´¥: {e}")
    return []

def save_global_documents(documents):
    """ä¿å­˜å…¨å±€æ–‡æ¡£åˆ°æ–‡ä»¶"""
    try:
        import json
        with open(GLOBAL_DOCUMENTS_FILE, 'w', encoding='utf-8') as f:
            json.dump(documents, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"ä¿å­˜å…¨å±€æ–‡æ¡£å¤±è´¥: {e}")

def load_global_workspaces():
    """ä»æ–‡ä»¶åŠ è½½å…¨å±€å·¥ä½œåŒº"""
    if GLOBAL_WORKSPACES_FILE.exists():
        try:
            import json
            with open(GLOBAL_WORKSPACES_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"åŠ è½½å…¨å±€å·¥ä½œåŒºå¤±è´¥: {e}")
    return []

def save_global_workspaces(workspaces):
    """ä¿å­˜å…¨å±€å·¥ä½œåŒºåˆ°æ–‡ä»¶"""
    try:
        import json
        with open(GLOBAL_WORKSPACES_FILE, 'w', encoding='utf-8') as f:
            json.dump(workspaces, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"ä¿å­˜å…¨å±€å·¥ä½œåŒºå¤±è´¥: {e}")

def get_global_services():
    """è·å–ç®€åŒ–çš„å…¨å±€æœåŠ¡å®ä¾‹"""
    documents = load_global_documents()
    workspaces = load_global_workspaces()
    return documents, workspaces


@router.post("/documents/upload")
async def upload_global_document(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...)
):
    """ä¸Šä¼ æ–‡æ¡£åˆ°å…¨å±€æ–‡æ¡£åº“"""
    try:
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹
        if not file.filename:
            raise HTTPException(status_code=400, detail="æ–‡ä»¶åä¸èƒ½ä¸ºç©º")
        
        file_ext = os.path.splitext(file.filename)[1].lower()
        allowed_extensions = ['.pdf', '.docx', '.doc', '.txt', '.md']
        
        if file_ext not in allowed_extensions:
            raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}")
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        content = await file.read()
        file_size = len(content)
        
        if file_size > 50 * 1024 * 1024:  # 50MBé™åˆ¶
            raise HTTPException(status_code=413, detail="æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶")
        
        # ç¬¬ä¸€æ­¥ï¼šå¿«é€Ÿä¿å­˜æ–‡ä»¶
        file_id = str(uuid.uuid4())
        filename = f"{file_id}{file_ext}"
        upload_dir = Path("uploads/global")
        upload_dir.mkdir(exist_ok=True)
        file_path = upload_dir / filename
        
        with open(file_path, "wb") as f:
            f.write(content)
        
        # åˆ›å»ºæ–‡æ¡£è®°å½•ï¼ˆåˆå§‹çŠ¶æ€ä¸ºuploadedï¼‰
        document_data = {
            'id': file_id,
            'filename': filename,
            'original_filename': file.filename,
            'file_size': file_size,
            'file_path': str(file_path),
            'mime_type': file.content_type or "application/octet-stream",
            'status': 'uploaded',
            'created_at': datetime.now().isoformat(),
            'processing_started': None,
            'processing_completed': None,
            'chunk_count': 0,
            'quality_score': 0.0
        }
        
        # ä¿å­˜æ–‡æ¡£è®°å½•åˆ°JSONæ–‡ä»¶
        documents = load_global_documents()
        documents.append(document_data)
        save_global_documents(documents)
        
        # ç¬¬äºŒæ­¥ï¼šåˆ›å»ºTaskQueueä»»åŠ¡å¹¶åå°å¼‚æ­¥å¤„ç†æ–‡æ¡£ï¼ˆä¸é˜»å¡å“åº”ï¼‰
        from app.services.task_queue import get_task_queue, TaskStage
        
        task_queue = get_task_queue()
        task_id = task_queue.create_task(
            task_type="global_document_processing",
            metadata={
                'original_filename': file.filename,
                'file_path': str(file_path),
                'file_size': file_size,
                'document_id': file_id
            },
            workspace_id="global"
        )
        
        # åå°å¤„ç†ï¼ˆä½¿ç”¨TaskQueueï¼‰
        import asyncio
        asyncio.create_task(
            process_global_document_with_progress(
                str(file_path), 
                document_data, 
                task_id
            )
        )
        
        # ç«‹å³è¿”å›æˆåŠŸå“åº”
        return {
            "id": file_id,
            "task_id": task_id,  # æ–°å¢
            "filename": filename,
            "original_filename": file.filename,
            "file_size": file_size,
            "status": "uploaded",
            "message": "æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œæ­£åœ¨åå°å¤„ç†ä¸­...",
            "processing_status": "queued"
        }
        
    except Exception as e:
        logger.error(f"å…¨å±€æ–‡æ¡£ä¸Šä¼ å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ä¸Šä¼ å¤±è´¥: {str(e)}")


def process_global_document_async(file_path: str, document_data: Dict[str, Any]):
    """å¼‚æ­¥åå°å¤„ç†å…¨å±€æ–‡æ¡£ï¼ˆåˆ†æ­¥å¤„ç†ï¼‰"""
    import asyncio
    asyncio.run(process_global_document_step_by_step(file_path, document_data))

async def process_global_document_with_progress(
    file_path: str, 
    document_data: Dict[str, Any],
    task_id: str
):
    """å¸¦è¿›åº¦æ›´æ–°çš„æ–‡æ¡£å¤„ç†"""
    from app.services.task_queue import get_task_queue, TaskStage
    import asyncio
    
    task_queue = get_task_queue()
    
    try:
        doc_id = document_data['id']
        logger.info(f"ğŸš€ å¼€å§‹å¸¦è¿›åº¦å¤„ç†çš„å…¨å±€æ–‡æ¡£: {file_path}, ID: {doc_id}, Task: {task_id}")
        
        # å¼€å§‹ä»»åŠ¡
        task_queue.start_task(task_id)
        
        # é˜¶æ®µ1: è§£ææ–‡æ¡£ (0-30%)
        task_queue.update_task_progress(
            task_id, 
            TaskStage.PARSING, 
            10, 
            "æ­£åœ¨è§£ææ–‡æ¡£..."
        )
        
        # æ·»åŠ å»¶è¿Ÿä»¥ä¾¿è§‚å¯Ÿè¿›åº¦
        await asyncio.sleep(0.5)
        
        # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
        await update_document_status(doc_id, 'processing', processing_started=datetime.now().isoformat())
        
        # è°ƒç”¨RAGæœåŠ¡
        global_rag = get_global_rag_service()
        
        logger.info(f"ğŸ“„ å¼€å§‹è§£ææ–‡æ¡£: {document_data['original_filename']}")
        
        # é˜¶æ®µ2: åˆ†å— (30-50%)
        task_queue.update_task_progress(
            task_id, 
            TaskStage.CHUNKING, 
            40, 
            "æ­£åœ¨åˆ†å‰²æ–‡æ¡£..."
        )
        
        await asyncio.sleep(0.5)
        
        # é˜¶æ®µ3: å‘é‡åŒ– (50-80%)
        task_queue.update_task_progress(
            task_id, 
            TaskStage.VECTORIZING, 
            60, 
            "æ­£åœ¨ç”Ÿæˆå‘é‡..."
        )
        
        # æ·»åŠ æ–‡æ¡£åˆ°RAGç³»ç»Ÿ
        success = await global_rag.add_document(
            workspace_id="global",
            file_path=file_path,
            metadata={
                'document_id': doc_id,
                'original_filename': document_data['original_filename'],
                'file_type': document_data['mime_type'],
                'file_size': document_data['file_size'],
                'upload_time': document_data['created_at']
            }
        )
        
        if success:
            logger.info(f"âœ… æ–‡æ¡£è§£æå’Œå‘é‡åŒ–æˆåŠŸ: {document_data['original_filename']}")
            
            # è·å–å¤„ç†ç»“æœç»Ÿè®¡
            vector_store = global_rag._load_vector_store("global")
            chunk_count = 0
            chunk_ids = []
            if vector_store and hasattr(vector_store, 'docstore'):
                docstore = vector_store.docstore
                if hasattr(docstore, '_dict'):
                    # æ”¶é›†å±äºè¿™ä¸ªæ–‡æ¡£çš„æ‰€æœ‰chunk IDs
                    for chunk_id, chunk_doc in docstore._dict.items():
                        chunk_metadata = chunk_doc.metadata if hasattr(chunk_doc, 'metadata') else {}
                        if chunk_metadata.get('document_id') == doc_id:
                            chunk_ids.append(chunk_id)
                            chunk_count += 1
            
            # é˜¶æ®µ4: ç´¢å¼•æ„å»º (80-100%)
            task_queue.update_task_progress(
                task_id, 
                TaskStage.INDEXING, 
                90, 
                "æ­£åœ¨å»ºç«‹ç´¢å¼•..."
            )
            
            await asyncio.sleep(0.5)
            
            # æ›´æ–°ç´¢å¼•ï¼šæ·»åŠ æ–‡ä»¶ä¿¡æ¯
            file_index_manager.add_file(
                file_id=doc_id,
                filename=document_data['filename'],
                original_filename=document_data['original_filename'],
                chunk_ids=chunk_ids,
                file_size=document_data['file_size'],
                file_type=document_data['mime_type'],
                file_path=str(file_path),
                upload_time=document_data['created_at']
            )
            
            # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†å®Œæˆ
            await update_document_status(
                doc_id, 
                'completed', 
                processing_completed=datetime.now().isoformat(),
                chunk_count=chunk_count,
                quality_score=0.8  # å¯ä»¥æ ¹æ®å®é™…å¤„ç†ç»“æœè°ƒæ•´
            )
            
            # å®Œæˆä»»åŠ¡
            task_queue.complete_task(task_id, {
                'chunk_count': chunk_count,
                'document_id': doc_id
            })
            
            logger.info(f"ğŸ‰ æ–‡æ¡£å¤„ç†å®Œæˆ: {document_data['original_filename']}, ç”Ÿæˆ {chunk_count} ä¸ªå‘é‡å—")
            
        else:
            logger.warning(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {document_data['original_filename']}")
            await update_document_status(doc_id, 'failed', processing_completed=datetime.now().isoformat())
            task_queue.fail_task(task_id, "æ–‡æ¡£å¤„ç†å¤±è´¥")
            
    except Exception as e:
        logger.error(f"âŒ å¸¦è¿›åº¦æ–‡æ¡£å¤„ç†å¼‚å¸¸: {file_path}, é”™è¯¯: {str(e)}")
        await update_document_status(document_data['id'], 'failed', processing_completed=datetime.now().isoformat())
        task_queue.fail_task(task_id, str(e))

async def process_global_document_step_by_step(file_path: str, document_data: Dict[str, Any]):
    """åˆ†æ­¥å¤„ç†å…¨å±€æ–‡æ¡£ï¼šè§£æ -> å‘é‡åŒ– -> æ¸…ç†"""
    try:
        doc_id = document_data['id']
        logger.info(f"ğŸš€ å¼€å§‹åˆ†æ­¥å¤„ç†å…¨å±€æ–‡æ¡£: {file_path}, ID: {doc_id}")
        
        # ç¬¬ä¸€æ­¥ï¼šæ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
        await update_document_status(doc_id, 'processing', processing_started=datetime.now().isoformat())
        
        # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨RAGæœåŠ¡å¤„ç†æ–‡æ¡£
        try:
            global_rag = get_global_rag_service()
            
            logger.info(f"ğŸ“„ å¼€å§‹è§£ææ–‡æ¡£: {document_data['original_filename']}")
            
            # æ·»åŠ æ–‡æ¡£åˆ°RAGç³»ç»Ÿ
            success = await global_rag.add_document(
                workspace_id="global",
                file_path=file_path,
                metadata={
                    'document_id': doc_id,
                    'original_filename': document_data['original_filename'],
                    'file_type': document_data['mime_type'],
                    'file_size': document_data['file_size'],
                    'upload_time': document_data['created_at']
                }
            )
            
            if success:
                logger.info(f"âœ… æ–‡æ¡£è§£æå’Œå‘é‡åŒ–æˆåŠŸ: {document_data['original_filename']}")
                
                # è·å–å¤„ç†ç»“æœç»Ÿè®¡
                vector_store = global_rag._load_vector_store("global")
                chunk_count = 0
                chunk_ids = []
                if vector_store and hasattr(vector_store, 'docstore'):
                    docstore = vector_store.docstore
                    if hasattr(docstore, '_dict'):
                        # æ”¶é›†å±äºè¿™ä¸ªæ–‡æ¡£çš„æ‰€æœ‰chunk IDs
                        for chunk_id, chunk_doc in docstore._dict.items():
                            chunk_metadata = chunk_doc.metadata if hasattr(chunk_doc, 'metadata') else {}
                            if chunk_metadata.get('document_id') == doc_id:
                                chunk_ids.append(chunk_id)
                                chunk_count += 1
                
                # æ›´æ–°ç´¢å¼•ï¼šæ·»åŠ æ–‡ä»¶ä¿¡æ¯
                file_index_manager.add_file(
                    file_id=doc_id,
                    filename=document_data['filename'],
                    original_filename=document_data['original_filename'],
                    chunk_ids=chunk_ids,
                    file_size=document_data['file_size'],
                    file_type=document_data['mime_type'],
                    file_path=str(file_path),
                    upload_time=document_data['created_at']
                )
                
                # ç¬¬ä¸‰æ­¥ï¼šæ›´æ–°çŠ¶æ€ä¸ºå¤„ç†å®Œæˆ
                await update_document_status(
                    doc_id, 
                    'completed', 
                    processing_completed=datetime.now().isoformat(),
                    chunk_count=chunk_count,
                    quality_score=0.8  # å¯ä»¥æ ¹æ®å®é™…å¤„ç†ç»“æœè°ƒæ•´
                )
                
                logger.info(f"ğŸ‰ æ–‡æ¡£å¤„ç†å®Œæˆ: {document_data['original_filename']}, ç”Ÿæˆ {chunk_count} ä¸ªå‘é‡å—")
                
                # ç¬¬å››æ­¥ï¼šå¯é€‰ - åˆ é™¤åŸå§‹æ–‡ä»¶ä»¥èŠ‚çœç©ºé—´
                # await cleanup_original_file(file_path, doc_id)
                
            else:
                logger.warning(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {document_data['original_filename']}")
                await update_document_status(doc_id, 'failed', processing_completed=datetime.now().isoformat())
                
        except Exception as rag_error:
            logger.error(f"âŒ RAGæœåŠ¡å¤„ç†å¤±è´¥: {str(rag_error)}")
            await update_document_status(doc_id, 'failed', processing_completed=datetime.now().isoformat())
        
    except Exception as e:
        logger.error(f"âŒ å…¨å±€æ–‡æ¡£å¤„ç†å¼‚å¸¸: {file_path}, é”™è¯¯: {str(e)}")
        await update_document_status(document_data['id'], 'failed', processing_completed=datetime.now().isoformat())

async def update_document_status(doc_id: str, status: str, **kwargs):
    """æ›´æ–°æ–‡æ¡£çŠ¶æ€"""
    try:
        documents = load_global_documents()
        for doc in documents:
            if doc['id'] == doc_id:
                doc['status'] = status
                for key, value in kwargs.items():
                    doc[key] = value
                break
        save_global_documents(documents)
        logger.info(f"ğŸ“ æ–‡æ¡£çŠ¶æ€å·²æ›´æ–°: {doc_id} -> {status}")
    except Exception as e:
        logger.error(f"æ›´æ–°æ–‡æ¡£çŠ¶æ€å¤±è´¥: {str(e)}")

async def cleanup_original_file(file_path: str, doc_id: str):
    """æ¸…ç†åŸå§‹æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰"""
    try:
        if os.path.exists(file_path):
            os.remove(file_path)
            logger.info(f"ğŸ—‘ï¸ å·²åˆ é™¤åŸå§‹æ–‡ä»¶: {file_path}")
            
            # æ›´æ–°æ–‡æ¡£è®°å½•
            documents = load_global_documents()
            for doc in documents:
                if doc['id'] == doc_id:
                    doc['original_file_deleted'] = True
                    doc['file_path'] = None  # æ¸…ç©ºæ–‡ä»¶è·¯å¾„
                    break
            save_global_documents(documents)
    except Exception as e:
        logger.error(f"æ¸…ç†åŸå§‹æ–‡ä»¶å¤±è´¥: {str(e)}")


@router.get("/documents/status/{doc_id}")
async def get_document_status(doc_id: str):
    """è·å–æ–‡æ¡£å¤„ç†çŠ¶æ€"""
    try:
        documents = load_global_documents()
        for doc in documents:
            if doc['id'] == doc_id:
                return {
                    "id": doc_id,
                    "status": doc.get('status', 'unknown'),
                    "original_filename": doc.get('original_filename', ''),
                    "processing_started": doc.get('processing_started'),
                    "processing_completed": doc.get('processing_completed'),
                    "chunk_count": doc.get('chunk_count', 0),
                    "quality_score": doc.get('quality_score', 0.0),
                    "file_size": doc.get('file_size', 0),
                    "created_at": doc.get('created_at')
                }
        
        raise HTTPException(status_code=404, detail="æ–‡æ¡£æœªæ‰¾åˆ°")
    except Exception as e:
        logger.error(f"è·å–æ–‡æ¡£çŠ¶æ€å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–çŠ¶æ€å¤±è´¥: {str(e)}")

@router.get("/documents")
async def list_global_documents():
    """åˆ—å‡ºæ‰€æœ‰å…¨å±€æ–‡æ¡£ï¼ˆä½¿ç”¨JSONæ–‡ä»¶å­˜å‚¨ï¼‰"""
    try:
        # ç›´æ¥ä»JSONæ–‡ä»¶åŠ è½½
        documents = load_global_documents()
        
        # å¦‚æœæ–‡ä»¶ä¸ºç©ºæˆ–æ²¡æœ‰è®°å½•ï¼Œè¿”å›ç©ºåˆ—è¡¨
        if not documents:
            logger.info("å…¨å±€æ–‡æ¡£æ–‡ä»¶ä¸ºç©ºï¼Œè¿”å›ç©ºåˆ—è¡¨")
            return {
                "documents": [],
                "total_count": 0,
                "message": "æš‚æ— å…¨å±€æ–‡æ¡£"
            }
        
        # è½¬æ¢æ ¼å¼ï¼ˆå¦‚æœéœ€è¦ï¼‰
        result_documents = []
        for doc in documents:
            result_documents.append({
                "id": doc.get('id', ''),
                "filename": doc.get('filename', ''),
                "original_filename": doc.get('original_filename', ''),
                "file_size": doc.get('file_size', 0),
                "file_type": doc.get('file_type', ''),
                "status": doc.get('status', 'completed'),
                "created_at": doc.get('created_at', ''),
                "chunk_count": doc.get('chunk_count', 0)
            })
        
        return {
            "documents": result_documents,
            "total_count": len(result_documents),
            "message": "å…¨å±€æ–‡æ¡£åˆ—è¡¨"
        }
    except Exception as e:
        logger.error(f"è·å–å…¨å±€æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"è·å–å¤±è´¥: {str(e)}")


@router.post("/search")
async def search_global_documents(
    request: Dict[str, Any]
):
    """æœç´¢å…¨å±€æ–‡æ¡£"""
    try:
        # å¼€å§‹æ€§èƒ½è®¡æ—¶
        start_time = performance_optimizer.start_search_timer()
        
        query = request.get('query', '')
        workspace_id = request.get('workspace_id')
        top_k = request.get('top_k', 5)
        
        # æ£€æŸ¥ç¼“å­˜
        cached_result = performance_optimizer.get_cached_search_result(query, workspace_id or "global")
        if cached_result:
            logger.info(f"ä½¿ç”¨ç¼“å­˜æœç´¢ç»“æœ: {query}")
            return cached_result
        
        documents = load_global_documents()
        
        # ä½¿ç”¨RAGæœåŠ¡è¿›è¡ŒçœŸæ­£çš„æ–‡æ¡£æœç´¢
        results = []
        try:
            # åˆ›å»ºå…¨å±€RAGæœåŠ¡å®ä¾‹
            global_rag = get_global_rag_service()
            
            # ä½¿ç”¨RAGæœåŠ¡è¿›è¡ŒçœŸæ­£çš„æ–‡æ¡£æœç´¢
            # ç›´æ¥ä½¿ç”¨RAGæœåŠ¡çš„æœç´¢åŠŸèƒ½ï¼Œè€Œä¸æ˜¯é—®ç­”åŠŸèƒ½
            logger.info(f"å¼€å§‹RAGæœç´¢: {query}")
            search_results = await global_rag.ask_question(
                workspace_id="global",
                question=query,
                top_k=top_k
            )
            logger.info(f"RAGæœç´¢å®Œæˆï¼Œå¼•ç”¨æ•°é‡: {len(search_results.get('references', []))}")
            
            if search_results.get('references'):
                logger.info(f"å¤„ç† {len(search_results['references'])} ä¸ªå¼•ç”¨")
                for ref in search_results['references']:
                    # å®‰å…¨è·å–å†…å®¹ï¼Œå¤„ç†ä¸åŒçš„å¼•ç”¨ç»“æ„
                    content = ref.get('content', ref.get('page_content', ref.get('content_preview', '')))
                    logger.info(f"å¼•ç”¨å†…å®¹é•¿åº¦: {len(content)}")
                    if content:
                        results.append({
                            "content": content[:200] + "..." if len(content) > 200 else content,
                            "metadata": {
                                "document_id": ref.get('document_id', ''),
                                "chunk_id": ref.get('chunk_id', ''),
                                "similarity": ref.get('similarity', 0.0),
                                "original_filename": ref.get('metadata', {}).get('original_filename', 'è®¡ç½‘.pdf')
                            },
                            "similarity": ref.get('similarity', 0.0)
                        })
                        logger.info(f"æ·»åŠ ç»“æœï¼Œå½“å‰ç»“æœæ•°é‡: {len(results)}")
            else:
                logger.warning("RAGæœç´¢æ²¡æœ‰è¿”å›å¼•ç”¨")
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å†…å®¹ï¼Œå›é€€åˆ°æ–‡ä»¶ååŒ¹é…
            if not results:
                for doc in documents:
                    if query.lower() in doc['original_filename'].lower():
                        results.append({
                            'content': f"æ–‡ä»¶å: {doc['original_filename']}",
                            'metadata': doc,
                            'similarity': 0.8
                        })
                        
        except Exception as rag_error:
            logger.warning(f"RAGæœç´¢å¤±è´¥ï¼Œå›é€€åˆ°ç®€å•æœç´¢: {str(rag_error)}")
            # å›é€€åˆ°ç®€å•æœç´¢
            for doc in documents:
                if query.lower() in doc['original_filename'].lower():
                    results.append({
                        'content': f"æ–‡ä»¶å: {doc['original_filename']}",
                        'metadata': doc,
                        'similarity': 0.8
                    })
        
        search_result = {
            "query": query,
            "results": results[:top_k],
            "total_count": len(results),
            "workspace_id": workspace_id,
            "cached": False
        }
        
        # ç¼“å­˜æœç´¢ç»“æœ
        performance_optimizer.cache_search_result(query, workspace_id or "global", search_result)
        
        # ç»“æŸæ€§èƒ½è®¡æ—¶
        search_time = performance_optimizer.end_search_timer(start_time)
        logger.info(f"æœç´¢å®Œæˆ: {query}, è€—æ—¶: {search_time:.3f}s, ç»“æœæ•°: {len(results)}")
        
        return search_result
        
    except Exception as e:
        logger.error(f"å…¨å±€æ–‡æ¡£æœç´¢å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æœç´¢å¤±è´¥: {str(e)}")


@router.post("/chat")
async def global_chat(
    request: Dict[str, Any]
):
    """å…¨å±€é—®ç­”"""
    try:
        question = request.get('question', '')
        workspace_id = request.get('workspace_id')
        top_k = request.get('top_k', 5)
        
        documents = load_global_documents()
        
        # ä½¿ç”¨RAGæœåŠ¡è¿›è¡ŒçœŸæ­£çš„é—®ç­”
        try:
            # åˆ›å»ºå…¨å±€RAGæœåŠ¡å®ä¾‹
            global_rag = get_global_rag_service()
            
            # ä½¿ç”¨RAGæœåŠ¡è¿›è¡Œé—®ç­”
            rag_result = await global_rag.ask_question(
                workspace_id="global",
                question=question,
                top_k=top_k
            )
            
            if rag_result.get('answer') and rag_result.get('references'):
                return {
                    "answer": rag_result['answer'],
                    "references": rag_result['references'],
                    "confidence": rag_result.get('confidence', 0.7),
                    "metadata": {
                        "mode": "global_rag",
                        "document_count": len(documents),
                        "workspace_id": workspace_id
                    }
                }
            else:
                # å›é€€åˆ°ç®€å•é—®ç­”
                answer = f"æ ¹æ®å…¨å±€æ–‡æ¡£åº“ä¸­çš„ {len(documents)} ä¸ªæ–‡æ¡£ï¼Œæˆ‘æ‰¾åˆ°äº†ç›¸å…³ä¿¡æ¯ã€‚"
                if documents:
                    answer += f" å…¶ä¸­åŒ…æ‹¬: {', '.join([doc['original_filename'] for doc in documents[:3]])}"
                
                return {
                    "answer": answer,
                    "references": [],
                    "confidence": 0.7,
                    "metadata": {
                        "mode": "global_simple",
                        "document_count": len(documents),
                        "workspace_id": workspace_id
                    }
                }
                
        except Exception as rag_error:
            logger.warning(f"RAGé—®ç­”å¤±è´¥ï¼Œå›é€€åˆ°ç®€å•é—®ç­”: {str(rag_error)}")
            # å›é€€åˆ°ç®€å•é—®ç­”
            answer = f"æ ¹æ®å…¨å±€æ–‡æ¡£åº“ä¸­çš„ {len(documents)} ä¸ªæ–‡æ¡£ï¼Œæˆ‘æ‰¾åˆ°äº†ç›¸å…³ä¿¡æ¯ã€‚"
            if documents:
                answer += f" å…¶ä¸­åŒ…æ‹¬: {', '.join([doc['original_filename'] for doc in documents[:3]])}"
            
            return {
                "answer": answer,
                "references": [],
                "confidence": 0.7,
                "metadata": {
                    "mode": "global_simple",
                    "document_count": len(documents),
                    "workspace_id": workspace_id
                }
            }
        
    except Exception as e:
        logger.error(f"å…¨å±€é—®ç­”å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"é—®ç­”å¤±è´¥: {str(e)}")


@router.get("/stats")
async def get_global_stats():
    """è·å–å…¨å±€ç»Ÿè®¡ä¿¡æ¯"""
    try:
        documents = load_global_documents()
        workspaces = load_global_workspaces()
        
        stats = {
            'global_document_count': len(documents),
            'global_workspace_count': len(workspaces),
            'vector_store_available': False,  # ç®€åŒ–ç‰ˆæš‚æ—¶ä¸å¯ç”¨
            'embedding_model': 'simplified'
        }
        
        return stats
        
    except Exception as e:
        logger.error(f"è·å–å…¨å±€ç»Ÿè®¡å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–ç»Ÿè®¡å¤±è´¥: {str(e)}")


# å·¥ä½œåŒºç®¡ç†API
@router.post("/workspaces")
async def create_workspace(
    name: str,
    description: Optional[str] = None,
    settings: Optional[Dict[str, Any]] = None
):
    """åˆ›å»ºå·¥ä½œåŒº"""
    try:
        workspace_id = str(uuid.uuid4())
        
        # æ·»åŠ åˆ°æŒä¹…åŒ–å­˜å‚¨
        workspaces = load_global_workspaces()
        workspace_data = {
            "id": workspace_id,
            "name": name,
            "description": description,
            "settings": settings or {},
            "status": "active",
            "created_at": "2025-10-23T22:00:00Z"
        }
        workspaces.append(workspace_data)
        save_global_workspaces(workspaces)
        
        return workspace_data
        
    except Exception as e:
        logger.error(f"åˆ›å»ºå·¥ä½œåŒºå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºå¤±è´¥: {str(e)}")


@router.get("/workspaces")
async def list_workspaces():
    """åˆ—å‡ºæ‰€æœ‰å·¥ä½œåŒº"""
    try:
        workspaces = load_global_workspaces()
        
        # æ·»åŠ é»˜è®¤å·¥ä½œåŒº
        if not workspaces:
            default_workspace = {
                "id": "1",
                "name": "é»˜è®¤å·¥ä½œåŒº",
                "description": "åŒ…å«æ‰€æœ‰å…¨å±€æ–‡æ¡£çš„é»˜è®¤å·¥ä½œåŒº",
                "status": "active",
                "document_count": 0,
                "created_at": "2025-10-23T22:00:00Z"
            }
            workspaces.append(default_workspace)
        
        return {
            "workspaces": workspaces,
            "total_count": len(workspaces)
        }
        
    except Exception as e:
        logger.error(f"è·å–å·¥ä½œåŒºåˆ—è¡¨å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–å¤±è´¥: {str(e)}")


@router.post("/workspaces/{workspace_id}/documents/{document_id}/access")
async def grant_document_access(
    workspace_id: str,
    document_id: str,
    access_level: str = "read"
):
    """æˆäºˆå·¥ä½œåŒºæ–‡æ¡£è®¿é—®æƒé™"""
    try:
        # ç®€åŒ–å¤„ç†ï¼Œç›´æ¥è¿”å›æˆåŠŸ
        return {
            "workspace_id": workspace_id,
            "document_id": document_id,
            "access_level": access_level,
            "status": "granted"
        }
        
    except Exception as e:
        logger.error(f"æˆäºˆæ–‡æ¡£è®¿é—®æƒé™å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æˆæƒå¤±è´¥: {str(e)}")


@router.delete("/workspaces/{workspace_id}/documents/{document_id}/access")
async def revoke_document_access(
    workspace_id: str,
    document_id: str
):
    """æ’¤é”€å·¥ä½œåŒºæ–‡æ¡£è®¿é—®æƒé™"""
    try:
        # ç®€åŒ–å¤„ç†ï¼Œç›´æ¥è¿”å›æˆåŠŸ
        return {
            "workspace_id": workspace_id,
            "document_id": document_id,
            "status": "revoked"
        }
        
    except Exception as e:
        logger.error(f"æ’¤é”€æ–‡æ¡£è®¿é—®æƒé™å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ’¤é”€å¤±è´¥: {str(e)}")


# æ€§èƒ½ä¼˜åŒ–API
@router.get("/performance/stats")
async def get_performance_stats():
    """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
    try:
        stats = performance_optimizer.get_performance_stats()
        return stats
        
    except Exception as e:
        logger.error(f"è·å–æ€§èƒ½ç»Ÿè®¡å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–ç»Ÿè®¡å¤±è´¥: {str(e)}")


@router.post("/performance/optimize")
async def optimize_performance():
    """æ‰§è¡Œæ€§èƒ½ä¼˜åŒ–"""
    try:
        # ä¼˜åŒ–å‘é‡ç´¢å¼•
        vector_db_path = "/root/workspace/consult/backend/langchain_vector_db"
        optimization_result = performance_optimizer.optimize_vector_index(vector_db_path)
        
        # æ¸…ç†è¿‡æœŸç¼“å­˜
        cleanup_result = performance_optimizer.clear_cache()
        
        return {
            "success": True,
            "message": "æ€§èƒ½ä¼˜åŒ–å®Œæˆ",
            "vector_optimization": optimization_result,
            "cache_cleanup": cleanup_result
        }
        
    except Exception as e:
        logger.error(f"æ€§èƒ½ä¼˜åŒ–å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ä¼˜åŒ–å¤±è´¥: {str(e)}")


@router.delete("/performance/cache")
async def clear_performance_cache(cache_type: str = None):
    """æ¸…ç†æ€§èƒ½ç¼“å­˜"""
    try:
        result = performance_optimizer.clear_cache(cache_type)
        return result
        
    except Exception as e:
        logger.error(f"æ¸…ç†ç¼“å­˜å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ¸…ç†å¤±è´¥: {str(e)}")


@router.get("/documents/{doc_id}/download")
async def download_global_document(doc_id: str):
    """ä¸‹è½½å…¨å±€æ–‡æ¡£"""
    try:
        from fastapi.responses import FileResponse
        
        # æŸ¥æ‰¾æ–‡æ¡£æ–‡ä»¶è·¯å¾„
        rag_service = get_global_rag_service()
        vector_store = rag_service._load_vector_store("global")
        
        if vector_store and hasattr(vector_store, 'docstore'):
            docstore = vector_store.docstore
            if hasattr(docstore, '_dict') and doc_id in docstore._dict:
                doc = docstore._dict[doc_id]
                metadata = doc.metadata if hasattr(doc, 'metadata') else {}
                file_path = metadata.get('file_path')
                
                if file_path and os.path.exists(file_path):
                    filename = metadata.get('original_filename', f"document_{doc_id[:8]}")
                    
                    return FileResponse(
                        path=file_path,
                        filename=filename,
                        media_type='application/octet-stream'
                    )
        
        raise HTTPException(status_code=404, detail="æ–‡æ¡£æœªæ‰¾åˆ°")
        
    except Exception as e:
        logger.error(f"ä¸‹è½½å…¨å±€æ–‡æ¡£å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ä¸‹è½½å¤±è´¥: {str(e)}")


@router.delete("/documents/{doc_id}")
async def delete_global_document(doc_id: str):
    """åˆ é™¤å…¨å±€æ–‡æ¡£ï¼ˆä½¿ç”¨ç´¢å¼•å¿«é€Ÿå®šä½ï¼‰"""
    try:
        logger.info(f"æ”¶åˆ°åˆ é™¤å…¨å±€æ–‡æ¡£è¯·æ±‚: {doc_id}")
        
        # ä»ç´¢å¼•ä¸­è·å–æ–‡ä»¶ä¿¡æ¯
        file_info = file_index_manager.get_file_info(doc_id)
        if not file_info:
            logger.warning(f"æ–‡æ¡£ {doc_id} åœ¨ç´¢å¼•ä¸­æœªæ‰¾åˆ°")
            raise HTTPException(status_code=404, detail="æ–‡æ¡£æœªæ‰¾åˆ°")
        
        chunk_ids = file_info.get("chunk_ids", [])
        original_filename = file_info.get("original_filename", doc_id)
        file_path = file_info.get("file_path")
        
        logger.info(f"å‡†å¤‡åˆ é™¤æ–‡æ¡£: {original_filename}, åŒ…å« {len(chunk_ids)} ä¸ªå—")
        
        # ä½¿ç”¨é«˜æ•ˆçš„åˆ é™¤æ–¹æ³•
        rag_service = get_global_rag_service()
        deleted_count = 0
        
        for chunk_id in chunk_ids:
            if rag_service.delete_document_efficient("global", chunk_id):
                deleted_count += 1
                logger.debug(f"åˆ é™¤chunk: {chunk_id}")
            else:
                logger.warning(f"åˆ é™¤chunkå¤±è´¥: {chunk_id}")
        
        logger.info(f"æˆåŠŸåˆ é™¤äº† {deleted_count}/{len(chunk_ids)} ä¸ªå—")
        
        # åˆ é™¤ç‰©ç†æ–‡ä»¶
        if file_path and os.path.exists(file_path):
            try:
                os.remove(file_path)
                logger.info(f"æˆåŠŸåˆ é™¤ç‰©ç†æ–‡ä»¶: {file_path}")
            except Exception as e:
                logger.warning(f"åˆ é™¤ç‰©ç†æ–‡ä»¶å¤±è´¥: {str(e)}")
        
        # ä»ç´¢å¼•ä¸­åˆ é™¤æ–‡ä»¶è®°å½•
        file_index_manager.remove_file(doc_id)
        
        logger.info(f"æ–‡æ¡£åˆ é™¤å®Œæˆ: {original_filename}, åˆ é™¤äº† {deleted_count}/{len(chunk_ids)} ä¸ªå—")
        
        return {
            "status": "deleted",
            "id": doc_id,
            "filename": original_filename,
            "deleted_chunks": deleted_count,
            "total_chunks": len(chunk_ids),
            "message": f"æ–‡æ¡£ {original_filename} å·²æˆåŠŸåˆ é™¤"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ é™¤å…¨å±€æ–‡æ¡£å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ é™¤å¤±è´¥: {str(e)}")


@router.post("/rebuild-index")
async def rebuild_file_index():
    """é‡å»ºæ–‡ä»¶ç´¢å¼•ï¼ˆç”¨äºæ•°æ®æ¢å¤ï¼‰"""
    try:
        logger.info("å¼€å§‹é‡å»ºæ–‡ä»¶ç´¢å¼•...")
        
        rag_service = get_global_rag_service()
        vector_store = rag_service._load_vector_store("global")
        
        # é‡å»ºç´¢å¼•
        file_index_manager.rebuild_index_from_vector_store(vector_store)
        
        # è·å–é‡å»ºåçš„ç»Ÿè®¡ä¿¡æ¯
        file_count = file_index_manager.get_file_count()
        files = file_index_manager.list_files()
        
        total_chunks = sum(file_info.get("chunk_count", 0) for file_info in files)
        
        logger.info(f"ç´¢å¼•é‡å»ºå®Œæˆ: {file_count} ä¸ªæ–‡ä»¶, {total_chunks} ä¸ªå—")
        
        return {
            "status": "success",
            "message": "æ–‡ä»¶ç´¢å¼•é‡å»ºå®Œæˆ",
            "file_count": file_count,
            "total_chunks": total_chunks,
            "files": files
        }
        
    except Exception as e:
        logger.error(f"é‡å»ºæ–‡ä»¶ç´¢å¼•å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"é‡å»ºç´¢å¼•å¤±è´¥: {str(e)}")


@router.get("/index-status")
async def get_index_status():
    """è·å–ç´¢å¼•çŠ¶æ€"""
    try:
        file_count = file_index_manager.get_file_count()
        files = file_index_manager.list_files()
        
        total_chunks = sum(file_info.get("chunk_count", 0) for file_info in files)
        total_size = sum(file_info.get("file_size", 0) for file_info in files)
        
        return {
            "file_count": file_count,
            "total_chunks": total_chunks,
            "total_size": total_size,
            "index_file": str(file_index_manager.index_file),
            "files": files
        }
        
    except Exception as e:
        logger.error(f"è·å–ç´¢å¼•çŠ¶æ€å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–çŠ¶æ€å¤±è´¥: {str(e)}")
