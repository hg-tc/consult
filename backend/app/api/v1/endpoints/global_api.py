"""
å…¨å±€æ–‡æ¡£åº“APIç«¯ç‚¹
æ”¯æŒå…¬å…±æ–‡æ¡£åº“å’Œå·¥ä½œåŒºåˆ†ç¦»çš„æ¶æ„
"""

from fastapi import APIRouter, Depends, HTTPException, UploadFile, File, BackgroundTasks
from typing import List, Dict, Any, Optional
import os
import uuid
import logging
from pathlib import Path
from datetime import datetime

# ç®€åŒ–å¯¼å…¥ï¼Œé¿å…å¤æ‚çš„ä¾èµ–
# from app.models.global_database import GlobalDatabaseService
# from app.services.global_rag_service import GlobalRAGService
# from app.core.database import get_db
from app.services.langchain_rag_service import LangChainRAGService
from app.services.performance_optimizer import get_performance_optimizer
from app.services.file_index_manager import file_index_manager
# å…¨å±€RAGæœåŠ¡å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
_global_rag_service = None

def get_global_rag_service():
    """è·å–å…¨å±€RAGæœåŠ¡å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _global_rag_service
    if _global_rag_service is None:
        # ä½¿ç”¨global_dbç›®å½•ï¼Œis_global=Trueæ ‡è®°è¿™ä¸æ˜¯ä¸€ä¸ªå·¥ä½œåŒº
        _global_rag_service = LangChainRAGService(vector_db_path="global_db", is_global=True)
    return _global_rag_service

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/global", tags=["global"])

# ç®€åŒ–çš„å…¨å±€æœåŠ¡å®ä¾‹
performance_optimizer = get_performance_optimizer()  # æ€§èƒ½ä¼˜åŒ–å™¨

# æŒä¹…åŒ–å­˜å‚¨è·¯å¾„
GLOBAL_DATA_DIR = Path("/root/consult/backend/global_data")
GLOBAL_DOCUMENTS_FILE = GLOBAL_DATA_DIR / "documents.json"
GLOBAL_WORKSPACES_FILE = GLOBAL_DATA_DIR / "workspaces.json"

# ç¡®ä¿ç›®å½•å­˜åœ¨
GLOBAL_DATA_DIR.mkdir(exist_ok=True)

def load_global_documents():
    """ä»æ–‡ä»¶åŠ è½½å…¨å±€æ–‡æ¡£"""
    if GLOBAL_DOCUMENTS_FILE.exists():
        try:
            import json
            with open(GLOBAL_DOCUMENTS_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"åŠ è½½å…¨å±€æ–‡æ¡£å¤±è´¥: {e}")
    return []

def save_global_documents(documents):
    """ä¿å­˜å…¨å±€æ–‡æ¡£åˆ°æ–‡ä»¶"""
    try:
        import json
        with open(GLOBAL_DOCUMENTS_FILE, 'w', encoding='utf-8') as f:
            json.dump(documents, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"ä¿å­˜å…¨å±€æ–‡æ¡£å¤±è´¥: {e}")

def load_global_workspaces():
    """ä»æ–‡ä»¶åŠ è½½å…¨å±€å·¥ä½œåŒº"""
    if GLOBAL_WORKSPACES_FILE.exists():
        try:
            import json
            with open(GLOBAL_WORKSPACES_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"åŠ è½½å…¨å±€å·¥ä½œåŒºå¤±è´¥: {e}")
    return []

def save_global_workspaces(workspaces):
    """ä¿å­˜å…¨å±€å·¥ä½œåŒºåˆ°æ–‡ä»¶"""
    try:
        import json
        with open(GLOBAL_WORKSPACES_FILE, 'w', encoding='utf-8') as f:
            json.dump(workspaces, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"ä¿å­˜å…¨å±€å·¥ä½œåŒºå¤±è´¥: {e}")

def get_global_services():
    """è·å–ç®€åŒ–çš„å…¨å±€æœåŠ¡å®ä¾‹"""
    documents = load_global_documents()
    workspaces = load_global_workspaces()
    return documents, workspaces


@router.post("/documents/upload")
async def upload_global_document(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...)
):
    """ä¸Šä¼ æ–‡æ¡£åˆ°å…¨å±€æ–‡æ¡£åº“"""
    try:
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹
        if not file.filename:
            raise HTTPException(status_code=400, detail="æ–‡ä»¶åä¸èƒ½ä¸ºç©º")
        
        file_ext = os.path.splitext(file.filename)[1].lower()
        # ä¸å‰ç«¯ accept å¯¹é½ï¼Œæ”¯æŒ Office ä¸å½’æ¡£
        allowed_extensions = ['.pdf', '.docx', '.doc', '.txt', '.md', '.xlsx', '.xls', '.pptx', '.ppt', '.zip', '.rar']
        
        if file_ext not in allowed_extensions:
            raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}")
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        content = await file.read()
        file_size = len(content)
        
        if file_size > 50 * 1024 * 1024:  # 50MBé™åˆ¶
            raise HTTPException(status_code=413, detail="æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶")
        
        # ç¬¬ä¸€æ­¥ï¼šå¿«é€Ÿä¿å­˜æ–‡ä»¶
        file_id = str(uuid.uuid4())
        filename = f"{file_id}{file_ext}"
        upload_dir = Path("uploads/global")
        upload_dir.mkdir(parents=True, exist_ok=True)
        file_path = upload_dir / filename
        
        with open(file_path, "wb") as f:
            f.write(content)
        
        # åˆ›å»ºæ–‡æ¡£è®°å½•ï¼ˆåˆå§‹çŠ¶æ€ä¸ºuploadedï¼‰
        document_data = {
            'id': file_id,
            'filename': filename,
            'original_filename': file.filename,
            'file_size': file_size,
            'file_path': str(file_path),
            'mime_type': file.content_type or "application/octet-stream",
            'status': 'uploaded',
            'created_at': datetime.now().isoformat(),
            'processing_started': None,
            'processing_completed': None,
            'chunk_count': 0,
            'quality_score': 0.0
        }
        
        # å¦‚æœæ˜¯ZIPæˆ–RARæ–‡ä»¶ï¼Œä½¿ç”¨ä¸“é—¨çš„å½’æ¡£å¤„ç†
        if file_ext in ['.zip', '.rar']:
            # åˆ›å»ºå½’æ¡£å¤„ç†ä»»åŠ¡
            from app.services.task_queue import get_task_queue, TaskStage
            
            task_queue = get_task_queue()
            task_id = task_queue.create_task(
                task_type="global_archive_processing",
                metadata={
                    'original_filename': file.filename,
                    'file_path': str(file_path),
                    'file_size': file_size,
                    'document_id': file_id,
                    'file_type': file_ext
                },
                workspace_id="global"
            )
            
            # å½’æ¡£æ–‡ä»¶ä¸ä¿å­˜åˆ°JSONï¼Œåªç”±å†…éƒ¨æ–‡ä»¶è®°å½•ä¿å­˜
            # æ›´æ–°è¿›åº¦
            task_queue.update_task_progress(
                task_id=task_id,
                stage=TaskStage.UPLOADING,
                progress=100,
                message="å½’æ¡£æ–‡ä»¶ä¸Šä¼ å®Œæˆ"
            )
            
            # ä½¿ç”¨app_simple.pyä¸­çš„process_zip_asyncå‡½æ•°
            from app_simple import process_zip_async
            import asyncio
            asyncio.create_task(process_zip_async(task_id, str(file_path), "global"))
            return {
                "id": file_id,
                "task_id": task_id,
                "filename": filename,
                "original_filename": file.filename,
                "file_size": file_size,
                "status": "uploaded",
                "file_type": file_ext,
                "message": f"{'ZIP' if file_ext == '.zip' else 'RAR'}æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œæ­£åœ¨è§£å‹å¹¶å¤„ç†...",
                "processing_status": "queued"
            }
        
        # ä¿å­˜æ–‡æ¡£è®°å½•åˆ°JSONæ–‡ä»¶
        documents = load_global_documents()
        documents.append(document_data)
        save_global_documents(documents)
        
        # ç¬¬äºŒæ­¥ï¼šåˆ›å»ºTaskQueueä»»åŠ¡å¹¶åå°å¼‚æ­¥å¤„ç†æ–‡æ¡£ï¼ˆä¸é˜»å¡å“åº”ï¼‰
        from app.services.task_queue import get_task_queue, TaskStage
        
        task_queue = get_task_queue()
        task_id = task_queue.create_task(
            task_type="global_document_processing",
            metadata={
                'original_filename': file.filename,
                'file_path': str(file_path),
                'file_size': file_size,
                'document_id': file_id
            },
            workspace_id="global"
        )
        
        # åå°å¤„ç†ï¼ˆä½¿ç”¨TaskQueueï¼‰
        import asyncio
        asyncio.create_task(
            process_global_document_with_progress(
                str(file_path), 
                document_data, 
                task_id
            )
        )
        
        # ç«‹å³è¿”å›æˆåŠŸå“åº”
        return {
            "id": file_id,
            "task_id": task_id,  # æ–°å¢
            "filename": filename,
            "original_filename": file.filename,
            "file_size": file_size,
            "status": "uploaded",
            "message": "æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œæ­£åœ¨åå°å¤„ç†ä¸­...",
            "processing_status": "queued"
        }
        
    except Exception as e:
        logger.error(f"å…¨å±€æ–‡æ¡£ä¸Šä¼ å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ä¸Šä¼ å¤±è´¥: {str(e)}")


def process_global_document_async(file_path: str, document_data: Dict[str, Any]):
    """å¼‚æ­¥åå°å¤„ç†å…¨å±€æ–‡æ¡£ï¼ˆåˆ†æ­¥å¤„ç†ï¼‰"""
    import asyncio
    asyncio.run(process_global_document_step_by_step(file_path, document_data))

async def process_global_document_with_progress(
    file_path: str, 
    document_data: Dict[str, Any],
    task_id: str
):
    """å¸¦è¿›åº¦æ›´æ–°çš„æ–‡æ¡£å¤„ç†"""
    from app.services.task_queue import get_task_queue, TaskStage
    import asyncio
    
    task_queue = get_task_queue()
    
    try:
        doc_id = document_data['id']
        logger.info(f"ğŸš€ å¼€å§‹å¸¦è¿›åº¦å¤„ç†çš„å…¨å±€æ–‡æ¡£: {file_path}, ID: {doc_id}, Task: {task_id}")
        
        # å¼€å§‹ä»»åŠ¡
        task_queue.start_task(task_id)
        
        # é˜¶æ®µ1: è§£ææ–‡æ¡£ (0-30%)
        task_queue.update_task_progress(
            task_id, 
            TaskStage.PARSING, 
            10, 
            "æ­£åœ¨è§£ææ–‡æ¡£..."
        )
        
        # æ·»åŠ å»¶è¿Ÿä»¥ä¾¿è§‚å¯Ÿè¿›åº¦
        await asyncio.sleep(0.5)
        
        # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
        await update_document_status(doc_id, 'processing', processing_started=datetime.now().isoformat())
        
        # è°ƒç”¨RAGæœåŠ¡
        global_rag = get_global_rag_service()
        
        logger.info(f"ğŸ“„ å¼€å§‹è§£ææ–‡æ¡£: {document_data['original_filename']}")
        
        # é˜¶æ®µ2: åˆ†å— (30-50%)
        task_queue.update_task_progress(
            task_id, 
            TaskStage.CHUNKING, 
            40, 
            "æ­£åœ¨åˆ†å‰²æ–‡æ¡£..."
        )
        
        await asyncio.sleep(0.5)
        
        # é˜¶æ®µ3: å‘é‡åŒ– (50-80%)
        task_queue.update_task_progress(
            task_id, 
            TaskStage.VECTORIZING, 
            60, 
            "æ­£åœ¨åŠ è½½æ£€ç´¢æ¨¡å—..."
        )
        
        # ä½¿ç”¨ LlamaIndex å¯¼å…¥ï¼ˆå¢åŠ è¶…æ—¶æ£€æµ‹ï¼‰
        logger.info(f"å‡†å¤‡å¼€å§‹LlamaIndexRetrieverå¯¼å…¥å’Œåˆå§‹åŒ–")
        import asyncio as _asyncio
        import concurrent.futures
        
        # å¯¼å…¥è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼š60ç§’ = 1åˆ†é’Ÿ
        IMPORT_TIMEOUT = 60
        
        def import_and_get_retriever():
            """åœ¨çº¿ç¨‹ä¸­æ‰§è¡Œå¯¼å…¥å’Œåˆå§‹åŒ–ï¼Œé¿å…é˜»å¡ä¸»äº‹ä»¶å¾ªç¯"""
            try:
                from app.services.llamaindex_retriever import get_retriever
                logger.info(f"âœ… LlamaIndexRetrieveræ¨¡å—å¯¼å…¥å®Œæˆ")
                retriever = get_retriever("global")  # ä½¿ç”¨ç¼“å­˜å•ä¾‹ï¼Œé¿å…é‡å¤åŠ è½½æ¨¡å‹å’Œç´¢å¼•
                logger.info(f"âœ… LlamaIndexRetrieveråˆå§‹åŒ–å®Œæˆ")
                return retriever
            except Exception as e:
                logger.error(f"âŒ LlamaIndexRetrieverå¯¼å…¥æˆ–åˆå§‹åŒ–å¤±è´¥: {e}", exc_info=True)
                raise
        
        retriever = None
        try:
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œå¯¼å…¥ï¼Œè®¾ç½®è¶…æ—¶
            loop = _asyncio.get_event_loop()
            task_queue.update_task_progress(
                task_id,
                TaskStage.VECTORIZING,
                62,
                f"æ­£åœ¨å¯¼å…¥æ£€ç´¢æ¨¡å—ï¼ˆæœ€å¤šç­‰å¾…{IMPORT_TIMEOUT}ç§’ï¼‰..."
            )
            retriever = await _asyncio.wait_for(
                loop.run_in_executor(None, import_and_get_retriever),
                timeout=IMPORT_TIMEOUT
            )
            logger.info(f"âœ… LlamaIndexRetrieverå¯¼å…¥å’Œåˆå§‹åŒ–æˆåŠŸå®Œæˆ")
        except _asyncio.TimeoutError:
            logger.error(f"â° LlamaIndexRetrieverå¯¼å…¥è¶…æ—¶({IMPORT_TIMEOUT}ç§’): {file_path}")
            # æ¸…ç†å¤±è´¥çš„ä¸Šä¼ 
            await cleanup_failed_upload(doc_id, file_path)
            await update_document_status(doc_id, 'failed', 
                                        processing_completed=datetime.now().isoformat(),
                                        error_message=f"å¯¼å…¥æ£€ç´¢æ¨¡å—è¶…æ—¶({IMPORT_TIMEOUT}ç§’)ï¼Œè¯·é‡æ–°ä¸Šä¼ ")
            task_queue.fail_task(task_id, f"å¯¼å…¥æ£€ç´¢æ¨¡å—è¶…æ—¶({IMPORT_TIMEOUT}ç§’)ï¼Œæ–‡ä»¶ä¸Šä¼ å¤±è´¥ï¼Œè¯·é‡æ–°ä¸Šä¼ ")
            return
        except Exception as e:
            logger.error(f"âŒ LlamaIndexRetrieverå¯¼å…¥å¤±è´¥: {e}", exc_info=True)
            # æ¸…ç†å¤±è´¥çš„ä¸Šä¼ 
            await cleanup_failed_upload(doc_id, file_path)
            await update_document_status(doc_id, 'failed',
                                        processing_completed=datetime.now().isoformat(),
                                        error_message=f"å¯¼å…¥æ£€ç´¢æ¨¡å—å¤±è´¥: {str(e)}")
            task_queue.fail_task(task_id, f"å¯¼å…¥æ£€ç´¢æ¨¡å—å¤±è´¥: {str(e)}ï¼Œæ–‡ä»¶ä¸Šä¼ å¤±è´¥ï¼Œè¯·é‡æ–°ä¸Šä¼ ")
            return
        logger.info(f"ğŸ”§ LlamaIndex add_document å¼€å§‹: path={file_path}, size={document_data['file_size']}, mime={document_data['mime_type']}")
        add_task = _asyncio.create_task(
            retriever.add_document(
                file_path=file_path,
                metadata={
                    'document_id': doc_id,
                    'original_filename': document_data['original_filename'],
                    'file_type': document_data['mime_type'],
                    'file_size': document_data['file_size'],
                    'upload_time': document_data['created_at']
                }
            )
        )
        # å¿ƒè·³è½®è¯¢ï¼šæ¯30ç§’æ›´æ–°ä¸€æ¬¡ï¼Œæœ€å¤§ç­‰å¾…600ç§’
        heartbeat_progress = 65
        total_wait = 0
        added_count = 0
        try:
            while True:
                try:
                    added_count = await _asyncio.wait_for(add_task, timeout=30)
                    break
                except _asyncio.TimeoutError:
                    total_wait += 30
                    # å¿ƒè·³æ›´æ–°
                    task_queue.update_task_progress(
                        task_id,
                        TaskStage.VECTORIZING,
                        min(heartbeat_progress, 85),
                        f"å‘é‡åŒ–è¿›è¡Œä¸­... å·²ç­‰å¾… {total_wait}s"
                    )
                    logger.info(f"â³ add_document å¿ƒè·³: ç­‰å¾… {total_wait}s, progress={heartbeat_progress}%")
                    heartbeat_progress += 5
                    if total_wait >= 600:
                        raise _asyncio.TimeoutError()
        except _asyncio.TimeoutError:
            logger.error(f"â° LlamaIndex add_document è¶…æ—¶(600s): {file_path}")
            # å°è¯•ä»ç´¢å¼•ä¸­åˆ é™¤å¯èƒ½å·²æ·»åŠ çš„æ•°æ®
            try:
                if retriever:
                    retriever.delete_by_document_id(doc_id)
                    logger.info(f"å·²å°è¯•ä»ç´¢å¼•ä¸­åˆ é™¤è¶…æ—¶çš„æ–‡æ¡£: {doc_id}")
            except Exception as cleanup_err:
                logger.warning(f"æ¸…ç†ç´¢å¼•æ•°æ®å¤±è´¥: {cleanup_err}")
            # æ¸…ç†å¤±è´¥çš„ä¸Šä¼ 
            await cleanup_failed_upload(doc_id, file_path)
            await update_document_status(doc_id, 'failed', 
                                        processing_completed=datetime.now().isoformat(),
                                        error_message="æ–‡æ¡£å¤„ç†è¶…æ—¶(600ç§’)ï¼Œè¯·é‡æ–°ä¸Šä¼ ")
            task_queue.fail_task(task_id, "æ–‡æ¡£å¤„ç†è¶…æ—¶(600ç§’)ï¼Œæ–‡ä»¶ä¸Šä¼ å¤±è´¥ï¼Œè¯·é‡æ–°ä¸Šä¼ ")
            return
        except Exception as e:
            logger.error(f"âŒ LlamaIndex add_document å¤±è´¥: {e}")
            # å°è¯•ä»ç´¢å¼•ä¸­åˆ é™¤å¯èƒ½å·²æ·»åŠ çš„æ•°æ®
            try:
                if retriever:
                    retriever.delete_by_document_id(doc_id)
                    logger.info(f"å·²å°è¯•ä»ç´¢å¼•ä¸­åˆ é™¤å¤±è´¥çš„æ–‡æ¡£: {doc_id}")
            except Exception as cleanup_err:
                logger.warning(f"æ¸…ç†ç´¢å¼•æ•°æ®å¤±è´¥: {cleanup_err}")
            # æ¸…ç†å¤±è´¥çš„ä¸Šä¼ 
            await cleanup_failed_upload(doc_id, file_path)
            await update_document_status(doc_id, 'failed',
                                        processing_completed=datetime.now().isoformat(),
                                        error_message=f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}ï¼Œè¯·é‡æ–°ä¸Šä¼ ")
            task_queue.fail_task(task_id, f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}ï¼Œæ–‡ä»¶ä¸Šä¼ å¤±è´¥ï¼Œè¯·é‡æ–°ä¸Šä¼ ")
            return
        finally:
            logger.info(f"ğŸ”§ LlamaIndex add_document ç»“æŸï¼Œadded_count={added_count}")

        # æŒä¹…åŒ–ä¹ŸåŠ å…¥è¶…æ—¶ä¸å¿ƒè·³
        logger.info("ğŸ’¾ å¼€å§‹æŒä¹…åŒ–ç´¢å¼•åˆ°å­˜å‚¨ç›®å½•")
        persist_task = _asyncio.create_task(_asyncio.to_thread(retriever.index.storage_context.persist, persist_dir=str(retriever.storage_dir)))
        persist_wait = 0
        try:
            while True:
                try:
                    await _asyncio.wait_for(persist_task, timeout=30)
                    break
                except _asyncio.TimeoutError:
                    persist_wait += 30
                    task_queue.update_task_progress(
                        task_id,
                        TaskStage.INDEXING,
                        90,
                        f"æŒä¹…åŒ–ç´¢å¼•ä¸­... å·²ç­‰å¾… {persist_wait}s"
                    )
                    logger.info(f"â³ persist å¿ƒè·³: ç­‰å¾… {persist_wait}s")
                    if persist_wait >= 600:
                        raise _asyncio.TimeoutError()
        except _asyncio.TimeoutError:
            logger.error("â° æŒä¹…åŒ–ç´¢å¼•è¶…æ—¶(600s)")
            # å°è¯•ä»ç´¢å¼•ä¸­åˆ é™¤å·²æ·»åŠ çš„æ•°æ®
            try:
                if retriever:
                    retriever.delete_by_document_id(doc_id)
                    logger.info(f"å·²å°è¯•ä»ç´¢å¼•ä¸­åˆ é™¤æŒä¹…åŒ–è¶…æ—¶çš„æ–‡æ¡£: {doc_id}")
            except Exception as cleanup_err:
                logger.warning(f"æ¸…ç†ç´¢å¼•æ•°æ®å¤±è´¥: {cleanup_err}")
            # æ¸…ç†å¤±è´¥çš„ä¸Šä¼ 
            await cleanup_failed_upload(doc_id, file_path)
            await update_document_status(doc_id, 'failed', 
                                        processing_completed=datetime.now().isoformat(),
                                        error_message="ç´¢å¼•æŒä¹…åŒ–è¶…æ—¶(600ç§’)ï¼Œè¯·é‡æ–°ä¸Šä¼ ")
            task_queue.fail_task(task_id, "ç´¢å¼•æŒä¹…åŒ–è¶…æ—¶(600ç§’)ï¼Œæ–‡ä»¶ä¸Šä¼ å¤±è´¥ï¼Œè¯·é‡æ–°ä¸Šä¼ ")
            return
        success = bool(added_count)
        
        if success:
            logger.info(f"âœ… æ–‡æ¡£è§£æå’Œå‘é‡åŒ–æˆåŠŸ: {document_data['original_filename']}ï¼Œæ–°å¢ {added_count} ä¸ªèŠ‚ç‚¹")
            
            # ç»Ÿè®¡æ–°å¢èŠ‚ç‚¹æ•° & æ”¶é›†node_idåˆ—è¡¨
            chunk_count = int(added_count) if added_count else 0
            try:
                node_ids = retriever.get_node_ids_by_document_id(doc_id)
            except Exception:
                node_ids = []
            chunk_ids = list(node_ids)
            
            # é˜¶æ®µ4: ç´¢å¼•æ„å»º (80-100%)
            task_queue.update_task_progress(
                task_id, 
                TaskStage.INDEXING, 
                90, 
                "æ­£åœ¨å»ºç«‹ç´¢å¼•..."
            )
            
            await asyncio.sleep(0.5)
            
            # æ›´æ–°ç´¢å¼•ï¼šæ·»åŠ æ–‡ä»¶ä¿¡æ¯
            file_index_manager.add_file(
                file_id=doc_id,
                filename=document_data['filename'],
                original_filename=document_data['original_filename'],
                chunk_ids=chunk_ids,
                file_size=document_data['file_size'],
                file_type=document_data['mime_type'],
                file_path=str(file_path),
                upload_time=document_data['created_at']
            )
            
            # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†å®Œæˆ
            await update_document_status(
                doc_id, 
                'completed', 
                processing_completed=datetime.now().isoformat(),
                chunk_count=chunk_count,
                node_ids=chunk_ids,
                quality_score=0.8  # å¯ä»¥æ ¹æ®å®é™…å¤„ç†ç»“æœè°ƒæ•´
            )
            
            # å®Œæˆä»»åŠ¡
            task_queue.complete_task(task_id, {
                'chunk_count': chunk_count,
                'document_id': doc_id
            })
            
            logger.info(f"ğŸ‰ æ–‡æ¡£å¤„ç†å®Œæˆ: {document_data['original_filename']}, ç”Ÿæˆ {chunk_count} ä¸ªå‘é‡å—")
            
        else:
            logger.warning(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {document_data['original_filename']}")
            await update_document_status(doc_id, 'failed', processing_completed=datetime.now().isoformat())
            task_queue.fail_task(task_id, "æ–‡æ¡£å¤„ç†å¤±è´¥")
            
    except Exception as e:
        logger.error(f"âŒ å¸¦è¿›åº¦æ–‡æ¡£å¤„ç†å¼‚å¸¸: {file_path}, é”™è¯¯: {str(e)}", exc_info=True)
        doc_id = document_data.get('id')
        # å°è¯•ä»ç´¢å¼•ä¸­åˆ é™¤å¯èƒ½å·²æ·»åŠ çš„æ•°æ®
        try:
            # å°è¯•è·å–retrieverï¼ˆå¦‚æœä¹‹å‰å·²ç»åˆ›å»ºï¼‰
            try:
                from app.services.llamaindex_retriever import get_retriever
                retriever = get_retriever("global")
                if retriever and doc_id:
                    retriever.delete_by_document_id(doc_id)
                    logger.info(f"å·²å°è¯•ä»ç´¢å¼•ä¸­åˆ é™¤å¼‚å¸¸å¤„ç†çš„æ–‡æ¡£: {doc_id}")
            except Exception:
                pass  # å¦‚æœè·å–retrieverå¤±è´¥ï¼Œå¿½ç•¥
        except Exception as cleanup_err:
            logger.warning(f"æ¸…ç†ç´¢å¼•æ•°æ®å¤±è´¥: {cleanup_err}")
        # æ¸…ç†å¤±è´¥çš„ä¸Šä¼ 
        if doc_id:
            await cleanup_failed_upload(doc_id, file_path)
            await update_document_status(doc_id, 'failed', 
                                        processing_completed=datetime.now().isoformat(),
                                        error_message=f"å¤„ç†å¼‚å¸¸: {str(e)}ï¼Œè¯·é‡æ–°ä¸Šä¼ ")
        task_queue.fail_task(task_id, f"å¤„ç†å¼‚å¸¸: {str(e)}ï¼Œæ–‡ä»¶ä¸Šä¼ å¤±è´¥ï¼Œè¯·é‡æ–°ä¸Šä¼ ")

async def process_global_document_step_by_step(file_path: str, document_data: Dict[str, Any]):
    """åˆ†æ­¥å¤„ç†å…¨å±€æ–‡æ¡£ï¼šè§£æ -> LlamaIndex å¯¼å…¥ -> æŒä¹…åŒ–"""
    try:
        doc_id = document_data['id']
        logger.info(f"ğŸš€ å¼€å§‹åˆ†æ­¥å¤„ç†å…¨å±€æ–‡æ¡£: {file_path}, ID: {doc_id}")
        
        # ç¬¬ä¸€æ­¥ï¼šæ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
        await update_document_status(doc_id, 'processing', processing_started=datetime.now().isoformat())
        
        # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨ LlamaIndex å¤„ç†æ–‡æ¡£
        try:
            from app.services.llamaindex_retriever import get_retriever
            logger.info(f"ğŸ“„ å¼€å§‹è§£ææ–‡æ¡£: {document_data['original_filename']}")

            retriever = get_retriever("global")  # ä½¿ç”¨ç¼“å­˜å•ä¾‹ï¼Œé¿å…é‡å¤åŠ è½½æ¨¡å‹å’Œç´¢å¼•
            import asyncio as _asyncio
            logger.info(f"ğŸ”§[step] LlamaIndex add_document å¼€å§‹: {file_path}")
            add_task = _asyncio.create_task(
                retriever.add_document(
                    file_path=file_path,
                    metadata={
                        'document_id': doc_id,
                        'original_filename': document_data['original_filename'],
                        'file_type': document_data['mime_type'],
                        'file_size': document_data['file_size'],
                        'upload_time': document_data['created_at']
                    }
                )
            )
            hb = 65
            waited = 0
            try:
                while True:
                    try:
                        added_count = await _asyncio.wait_for(add_task, timeout=30)
                        break
                    except _asyncio.TimeoutError:
                        waited += 30
                        logger.info(f"â³[step] add_document å¿ƒè·³: ç­‰å¾… {waited}s, progress={hb}%")
                        hb = min(hb + 5, 85)
                        if waited >= 600:
                            raise _asyncio.TimeoutError()
            except _asyncio.TimeoutError:
                logger.error(f"â°[step] LlamaIndex add_document è¶…æ—¶(600s): {file_path}")
                await update_document_status(doc_id, 'failed', processing_completed=datetime.now().isoformat())
                return
            except Exception as e:
                logger.error(f"âŒ[step] LlamaIndex add_document å¤±è´¥: {e}")
                await update_document_status(doc_id, 'failed', processing_completed=datetime.now().isoformat())
                return
            finally:
                logger.info("ğŸ”§[step] LlamaIndex add_document ç»“æŸ")
            # æŒä¹…åŒ–
            logger.info("ğŸ’¾[step] å¼€å§‹æŒä¹…åŒ–ç´¢å¼•åˆ°å­˜å‚¨ç›®å½•")
            persist_task = _asyncio.create_task(_asyncio.to_thread(retriever.index.storage_context.persist, persist_dir=str(retriever.storage_dir)))
            waited_persist = 0
            try:
                while True:
                    try:
                        await _asyncio.wait_for(persist_task, timeout=30)
                        break
                    except _asyncio.TimeoutError:
                        waited_persist += 30
                        logger.info(f"â³[step] persist å¿ƒè·³: ç­‰å¾… {waited_persist}s")
                        if waited_persist >= 600:
                            raise _asyncio.TimeoutError()
            except _asyncio.TimeoutError:
                logger.error("â°[step] æŒä¹…åŒ–ç´¢å¼•è¶…æ—¶(600s)")
                await update_document_status(doc_id, 'failed', processing_completed=datetime.now().isoformat())
                return

            if added_count:
                logger.info(f"âœ… æ–‡æ¡£è§£æå’Œå…¥åº“æˆåŠŸ: {document_data['original_filename']}ï¼Œæ–°èŠ‚ç‚¹: {added_count}")
                # ç®€åŒ–ï¼šæ— æ³•ä» LlamaIndex ç›´æ¥æŒ‰ doc_id ç»Ÿè®¡ chunk æ•°ï¼›è®°å½•èŠ‚ç‚¹æ•°å³å¯
                chunk_count = int(added_count)
                chunk_ids = []

                # æ›´æ–°ç´¢å¼•ï¼šæ·»åŠ æ–‡ä»¶ä¿¡æ¯
                file_index_manager.add_file(
                    file_id=doc_id,
                    filename=document_data['filename'],
                    original_filename=document_data['original_filename'],
                    chunk_ids=chunk_ids,
                    file_size=document_data['file_size'],
                    file_type=document_data['mime_type'],
                    file_path=str(file_path),
                    upload_time=document_data['created_at']
                )
                
                # ç¬¬ä¸‰æ­¥ï¼šæ›´æ–°çŠ¶æ€ä¸ºå¤„ç†å®Œæˆ
                await update_document_status(
                    doc_id, 
                    'completed', 
                    processing_completed=datetime.now().isoformat(),
                    chunk_count=chunk_count,
                    quality_score=0.8  # å¯ä»¥æ ¹æ®å®é™…å¤„ç†ç»“æœè°ƒæ•´
                )
                
                logger.info(f"ğŸ‰ æ–‡æ¡£å¤„ç†å®Œæˆ: {document_data['original_filename']}, ç”Ÿæˆ {chunk_count} ä¸ªå‘é‡å—")
                
                # ç¬¬å››æ­¥ï¼šå¯é€‰ - åˆ é™¤åŸå§‹æ–‡ä»¶ä»¥èŠ‚çœç©ºé—´
                # await cleanup_original_file(file_path, doc_id)
                
            else:
                logger.warning(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {document_data['original_filename']}")
                await update_document_status(doc_id, 'failed', processing_completed=datetime.now().isoformat())
                
        except Exception as rag_error:
            logger.error(f"âŒ RAGæœåŠ¡å¤„ç†å¤±è´¥: {str(rag_error)}")
            await update_document_status(doc_id, 'failed', processing_completed=datetime.now().isoformat())
        
    except Exception as e:
        logger.error(f"âŒ å…¨å±€æ–‡æ¡£å¤„ç†å¼‚å¸¸: {file_path}, é”™è¯¯: {str(e)}")
        await update_document_status(document_data['id'], 'failed', processing_completed=datetime.now().isoformat())

async def update_document_status(doc_id: str, status: str, **kwargs):
    """æ›´æ–°æ–‡æ¡£çŠ¶æ€"""
    try:
        documents = load_global_documents()
        for doc in documents:
            if doc['id'] == doc_id:
                doc['status'] = status
                for key, value in kwargs.items():
                    doc[key] = value
                break
        save_global_documents(documents)
        logger.info(f"ğŸ“ æ–‡æ¡£çŠ¶æ€å·²æ›´æ–°: {doc_id} -> {status}")
    except Exception as e:
        logger.error(f"æ›´æ–°æ–‡æ¡£çŠ¶æ€å¤±è´¥: {str(e)}")

async def cleanup_failed_upload(doc_id: str, file_path: str):
    """æ¸…ç†å¤±è´¥çš„ä¸Šä¼ ï¼šåˆ é™¤JSONè®°å½•å’Œç‰©ç†æ–‡ä»¶"""
    try:
        logger.info(f"ğŸ—‘ï¸ å¼€å§‹æ¸…ç†å¤±è´¥çš„ä¸Šä¼ : doc_id={doc_id}, file_path={file_path}")
        
        # 1. ä»JSONä¸­åˆ é™¤æ–‡æ¡£è®°å½•
        documents = load_global_documents()
        original_count = len(documents)
        documents = [doc for doc in documents if doc.get('id') != doc_id]
        if len(documents) < original_count:
            save_global_documents(documents)
            logger.info(f"âœ… å·²ä»JSONä¸­åˆ é™¤æ–‡æ¡£è®°å½•: {doc_id}")
        else:
            logger.warning(f"âš ï¸ JSONä¸­æœªæ‰¾åˆ°æ–‡æ¡£è®°å½•: {doc_id}")
        
        # 2. åˆ é™¤ç‰©ç†æ–‡ä»¶
        if file_path and os.path.exists(file_path):
            try:
                os.remove(file_path)
                logger.info(f"âœ… å·²åˆ é™¤ç‰©ç†æ–‡ä»¶: {file_path}")
            except Exception as file_err:
                logger.error(f"åˆ é™¤ç‰©ç†æ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {file_err}")
        else:
            logger.warning(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨æˆ–è·¯å¾„ä¸ºç©º: {file_path}")
        
        # 3. ä»æ–‡ä»¶ç´¢å¼•ä¸­åˆ é™¤ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        try:
            file_index_manager.remove_file(doc_id)
            logger.info(f"âœ… å·²ä»æ–‡ä»¶ç´¢å¼•ä¸­åˆ é™¤: {doc_id}")
        except Exception as index_err:
            logger.warning(f"ä»æ–‡ä»¶ç´¢å¼•åˆ é™¤å¤±è´¥ï¼ˆå¯èƒ½ä¸å­˜åœ¨ï¼‰: {index_err}")
        
        logger.info(f"âœ… æ¸…ç†å¤±è´¥ä¸Šä¼ å®Œæˆ: {doc_id}")
    except Exception as e:
        logger.error(f"æ¸…ç†å¤±è´¥ä¸Šä¼ æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}", exc_info=True)

async def cleanup_original_file(file_path: str, doc_id: str):
    """æ¸…ç†åŸå§‹æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰"""
    try:
        if os.path.exists(file_path):
            os.remove(file_path)
            logger.info(f"ğŸ—‘ï¸ å·²åˆ é™¤åŸå§‹æ–‡ä»¶: {file_path}")
            
            # æ›´æ–°æ–‡æ¡£è®°å½•
            documents = load_global_documents()
            for doc in documents:
                if doc['id'] == doc_id:
                    doc['original_file_deleted'] = True
                    doc['file_path'] = None  # æ¸…ç©ºæ–‡ä»¶è·¯å¾„
                    break
            save_global_documents(documents)
    except Exception as e:
        logger.error(f"æ¸…ç†åŸå§‹æ–‡ä»¶å¤±è´¥: {str(e)}")


@router.get("/documents/status/{doc_id}")
async def get_document_status(doc_id: str):
    """è·å–æ–‡æ¡£å¤„ç†çŠ¶æ€"""
    try:
        documents = load_global_documents()
        for doc in documents:
            if doc['id'] == doc_id:
                return {
                    "id": doc_id,
                    "status": doc.get('status', 'unknown'),
                    "original_filename": doc.get('original_filename', ''),
                    "processing_started": doc.get('processing_started'),
                    "processing_completed": doc.get('processing_completed'),
                    "chunk_count": doc.get('chunk_count', 0),
                    "quality_score": doc.get('quality_score', 0.0),
                    "file_size": doc.get('file_size', 0),
                    "created_at": doc.get('created_at')
                }
        
        raise HTTPException(status_code=404, detail="æ–‡æ¡£æœªæ‰¾åˆ°")
    except Exception as e:
        logger.error(f"è·å–æ–‡æ¡£çŠ¶æ€å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–çŠ¶æ€å¤±è´¥: {str(e)}")

@router.get("/documents")
async def list_global_documents():
    """åˆ—å‡ºæ‰€æœ‰å…¨å±€æ–‡æ¡£ï¼ˆåªä»JSONæ–‡ä»¶ï¼Œå¿«é€Ÿè¿”å›ï¼‰"""
    try:
        # ä» LlamaIndex å‘é‡åº“åŠ è½½ï¼ˆæ–°æ ¼å¼ï¼‰
        vector_documents = []
        try:
            llamaindex_storage_dir = Path("llamaindex_storage/global")
            docstore_file = llamaindex_storage_dir / "docstore.json"
            
            if docstore_file.exists():
                import json
                with open(docstore_file, 'r', encoding='utf-8') as f:
                    docstore_data = json.load(f)
                
                # è§£æ LlamaIndex æ ¼å¼çš„ docstore
                file_groups = {}
                nodes = docstore_data.get('docstore/data', {})
                
                for node_id, node_data in nodes.items():
                    data = node_data.get('__data__', {})
                    metadata = data.get('metadata', {})
                    
                    original_filename = metadata.get('original_filename') or metadata.get('file_name', f"æ–‡æ¡£_{node_id[:8]}")
                    
                    if original_filename not in file_groups:
                        file_groups[original_filename] = {
                            "id": node_id,
                            "filename": original_filename,
                            "original_filename": original_filename,
                            "file_size": metadata.get('file_size', 0),
                            "file_type": metadata.get('file_type', metadata.get('mime_type', '')),
                            "status": "completed",
                            "created_at": metadata.get('upload_time', metadata.get('creation_date', '')),
                            "chunk_count": 1
                        }
                    else:
                        file_groups[original_filename]["chunk_count"] += 1
                
                vector_documents = list(file_groups.values())
                logger.info(f"âœ… ä» LlamaIndex å‘é‡åº“åŠ è½½äº† {len(vector_documents)} ä¸ªæ–‡æ¡£")
        except Exception as e:
            logger.warning(f"âš ï¸ ä» LlamaIndex åŠ è½½å¤±è´¥: {e}ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•")
        
        # ä»JSONæ–‡ä»¶åŠ è½½
        json_documents = load_global_documents()
        
        # åˆå¹¶ä¸¤ä¸ªæ•°æ®æºï¼Œå»é‡ï¼ˆä¼˜å…ˆä½¿ç”¨å‘é‡æ•°æ®åº“çš„æ•°æ®ï¼‰
        result_map = {}
        
        # å…ˆæ·»åŠ å‘é‡æ•°æ®åº“ä¸­çš„æ–‡æ¡£
        for doc in vector_documents:
            key = doc.get('original_filename', '')
            if key not in result_map:
                result_map[key] = doc
        
        # å†æ·»åŠ JSONæ–‡ä»¶ä¸­ä¸å­˜åœ¨äºå‘é‡æ•°æ®åº“çš„æ–‡æ¡£
        for doc in json_documents:
            # è·³è¿‡å½’æ¡£æ–‡ä»¶è®°å½•ï¼ˆå…¼å®¹å†å²æ•°æ®ï¼Œæ–°ä¸Šä¼ çš„å½’æ¡£æ–‡ä»¶ä¸ä¼šå†™å…¥JSONï¼‰
            if doc.get('is_archive', False):
                continue
                
            key = doc.get('original_filename', '')
            if key not in result_map:
                result_map[key] = {
                    "id": doc.get('id', ''),
                    "filename": doc.get('filename', ''),
                    "original_filename": doc.get('original_filename', ''),
                    "file_size": doc.get('file_size', 0),
                    "file_type": doc.get('file_type', ''),
                    "status": doc.get('status', 'completed'),
                    "created_at": doc.get('created_at', ''),
                    "chunk_count": doc.get('chunk_count', 0),
                    # æ·»åŠ chunk_idså­—æ®µï¼Œä½¿ç”¨idä½œä¸ºç¬¬ä¸€ä¸ªchunk ID
                    "chunk_ids": [doc.get('id', '')]
                }
        
        result_documents = list(result_map.values())
        
        logger.info(f"å…¨å±€æ–‡æ¡£åˆ—è¡¨: å‘é‡æ•°æ®åº“ {len(vector_documents)} ä¸ªï¼ŒJSONæ–‡ä»¶ {len(json_documents)} ä¸ªï¼Œåˆå¹¶å {len(result_documents)} ä¸ª")
        
        return {
            "documents": result_documents,
            "total_count": len(result_documents),
            "message": f"å…¨å±€æ–‡æ¡£åˆ—è¡¨ï¼ˆå‘é‡æ•°æ®åº“: {len(vector_documents)}, JSON: {len(json_documents)}ï¼‰"
        }
    except Exception as e:
        logger.error(f"è·å–å…¨å±€æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"è·å–å¤±è´¥: {str(e)}")


@router.post("/search")
async def search_global_documents(
    request: Dict[str, Any]
):
    """æœç´¢å…¨å±€æ–‡æ¡£"""
    try:
        # å¼€å§‹æ€§èƒ½è®¡æ—¶
        start_time = performance_optimizer.start_search_timer()
        
        query = request.get('query', '')
        workspace_id = request.get('workspace_id')
        top_k = request.get('top_k', 5)
        
        # æ£€æŸ¥ç¼“å­˜
        cached_result = performance_optimizer.get_cached_search_result(query, workspace_id or "global")
        if cached_result:
            logger.info(f"ä½¿ç”¨ç¼“å­˜æœç´¢ç»“æœ: {query}")
            return cached_result
        
        documents = load_global_documents()
        
        # ä½¿ç”¨RAGæœåŠ¡è¿›è¡ŒçœŸæ­£çš„æ–‡æ¡£æœç´¢
        results = []
        try:
            # åˆ›å»ºå…¨å±€RAGæœåŠ¡å®ä¾‹
            global_rag = get_global_rag_service()
            
            # ä½¿ç”¨RAGæœåŠ¡è¿›è¡ŒçœŸæ­£çš„æ–‡æ¡£æœç´¢
            # ç›´æ¥ä½¿ç”¨RAGæœåŠ¡çš„æœç´¢åŠŸèƒ½ï¼Œè€Œä¸æ˜¯é—®ç­”åŠŸèƒ½
            logger.info(f"å¼€å§‹RAGæœç´¢: {query}")
            search_results = await global_rag.ask_question(
                workspace_id="global",
                question=query,
                top_k=top_k
            )
            logger.info(f"RAGæœç´¢å®Œæˆï¼Œå¼•ç”¨æ•°é‡: {len(search_results.get('references', []))}")
            
            if search_results.get('references'):
                logger.info(f"å¤„ç† {len(search_results['references'])} ä¸ªå¼•ç”¨")
                for ref in search_results['references']:
                    # å®‰å…¨è·å–å†…å®¹ï¼Œå¤„ç†ä¸åŒçš„å¼•ç”¨ç»“æ„
                    content = ref.get('content', ref.get('page_content', ref.get('content_preview', '')))
                    logger.info(f"å¼•ç”¨å†…å®¹é•¿åº¦: {len(content)}")
                    if content:
                        results.append({
                            "content": content[:200] + "..." if len(content) > 200 else content,
                            "metadata": {
                                "document_id": ref.get('document_id', ''),
                                "chunk_id": ref.get('chunk_id', ''),
                                "similarity": ref.get('similarity', 0.0),
                                "original_filename": ref.get('metadata', {}).get('original_filename', 'è®¡ç½‘.pdf')
                            },
                            "similarity": ref.get('similarity', 0.0)
                        })
                        logger.info(f"æ·»åŠ ç»“æœï¼Œå½“å‰ç»“æœæ•°é‡: {len(results)}")
            else:
                logger.warning("RAGæœç´¢æ²¡æœ‰è¿”å›å¼•ç”¨")
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å†…å®¹ï¼Œå›é€€åˆ°æ–‡ä»¶ååŒ¹é…
            if not results:
                for doc in documents:
                    if query.lower() in doc['original_filename'].lower():
                        results.append({
                            'content': f"æ–‡ä»¶å: {doc['original_filename']}",
                            'metadata': doc,
                            'similarity': 0.8
                        })
                        
        except Exception as rag_error:
            logger.warning(f"RAGæœç´¢å¤±è´¥ï¼Œå›é€€åˆ°ç®€å•æœç´¢: {str(rag_error)}")
            # å›é€€åˆ°ç®€å•æœç´¢
            for doc in documents:
                if query.lower() in doc['original_filename'].lower():
                    results.append({
                        'content': f"æ–‡ä»¶å: {doc['original_filename']}",
                        'metadata': doc,
                        'similarity': 0.8
                    })
        
        search_result = {
            "query": query,
            "results": results[:top_k],
            "total_count": len(results),
            "workspace_id": workspace_id,
            "cached": False
        }
        
        # ç¼“å­˜æœç´¢ç»“æœ
        performance_optimizer.cache_search_result(query, workspace_id or "global", search_result)
        
        # ç»“æŸæ€§èƒ½è®¡æ—¶
        search_time = performance_optimizer.end_search_timer(start_time)
        logger.info(f"æœç´¢å®Œæˆ: {query}, è€—æ—¶: {search_time:.3f}s, ç»“æœæ•°: {len(results)}")
        
        return search_result
        
    except Exception as e:
        logger.error(f"å…¨å±€æ–‡æ¡£æœç´¢å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æœç´¢å¤±è´¥: {str(e)}")


@router.post("/chat")
async def global_chat(
    request: Dict[str, Any]
):
    """å…¨å±€é—®ç­”"""
    try:
        question = request.get('question', '')
        workspace_id = request.get('workspace_id')
        top_k = request.get('top_k', 5)
        
        documents = load_global_documents()
        
        # ä½¿ç”¨RAGæœåŠ¡è¿›è¡ŒçœŸæ­£çš„é—®ç­”
        try:
            # åˆ›å»ºå…¨å±€RAGæœåŠ¡å®ä¾‹
            global_rag = get_global_rag_service()
            
            # ä½¿ç”¨RAGæœåŠ¡è¿›è¡Œé—®ç­”
            rag_result = await global_rag.ask_question(
                workspace_id="global",
                question=question,
                top_k=top_k
            )
            
            if rag_result.get('answer') and rag_result.get('references'):
                return {
                    "answer": rag_result['answer'],
                    "references": rag_result['references'],
                    "confidence": rag_result.get('confidence', 0.7),
                    "metadata": {
                        "mode": "global_rag",
                        "document_count": len(documents),
                        "workspace_id": workspace_id
                    }
                }
            else:
                # å›é€€åˆ°ç®€å•é—®ç­”
                answer = f"æ ¹æ®å…¨å±€æ–‡æ¡£åº“ä¸­çš„ {len(documents)} ä¸ªæ–‡æ¡£ï¼Œæˆ‘æ‰¾åˆ°äº†ç›¸å…³ä¿¡æ¯ã€‚"
                if documents:
                    answer += f" å…¶ä¸­åŒ…æ‹¬: {', '.join([doc['original_filename'] for doc in documents[:3]])}"
                
                return {
                    "answer": answer,
                    "references": [],
                    "confidence": 0.7,
                    "metadata": {
                        "mode": "global_simple",
                        "document_count": len(documents),
                        "workspace_id": workspace_id
                    }
                }
                
        except Exception as rag_error:
            logger.warning(f"RAGé—®ç­”å¤±è´¥ï¼Œå›é€€åˆ°ç®€å•é—®ç­”: {str(rag_error)}")
            # å›é€€åˆ°ç®€å•é—®ç­”
            answer = f"æ ¹æ®å…¨å±€æ–‡æ¡£åº“ä¸­çš„ {len(documents)} ä¸ªæ–‡æ¡£ï¼Œæˆ‘æ‰¾åˆ°äº†ç›¸å…³ä¿¡æ¯ã€‚"
            if documents:
                answer += f" å…¶ä¸­åŒ…æ‹¬: {', '.join([doc['original_filename'] for doc in documents[:3]])}"
            
            return {
                "answer": answer,
                "references": [],
                "confidence": 0.7,
                "metadata": {
                    "mode": "global_simple",
                    "document_count": len(documents),
                    "workspace_id": workspace_id
                }
            }
        
    except Exception as e:
        logger.error(f"å…¨å±€é—®ç­”å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"é—®ç­”å¤±è´¥: {str(e)}")


@router.get("/stats")
async def get_global_stats():
    """è·å–å…¨å±€ç»Ÿè®¡ä¿¡æ¯"""
    try:
        documents = load_global_documents()
        workspaces = load_global_workspaces()
        
        stats = {
            'global_document_count': len(documents),
            'global_workspace_count': len(workspaces),
            'vector_store_available': False,  # ç®€åŒ–ç‰ˆæš‚æ—¶ä¸å¯ç”¨
            'embedding_model': 'simplified'
        }
        
        return stats
        
    except Exception as e:
        logger.error(f"è·å–å…¨å±€ç»Ÿè®¡å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–ç»Ÿè®¡å¤±è´¥: {str(e)}")


# å·¥ä½œåŒºç®¡ç†API
@router.post("/workspaces")
async def create_workspace(
    name: str,
    description: Optional[str] = None,
    settings: Optional[Dict[str, Any]] = None
):
    """åˆ›å»ºå·¥ä½œåŒº"""
    try:
        workspace_id = str(uuid.uuid4())
        
        # æ·»åŠ åˆ°æŒä¹…åŒ–å­˜å‚¨
        workspaces = load_global_workspaces()
        workspace_data = {
            "id": workspace_id,
            "name": name,
            "description": description,
            "settings": settings or {},
            "status": "active",
            "created_at": "2025-10-23T22:00:00Z"
        }
        workspaces.append(workspace_data)
        save_global_workspaces(workspaces)
        
        return workspace_data
        
    except Exception as e:
        logger.error(f"åˆ›å»ºå·¥ä½œåŒºå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºå¤±è´¥: {str(e)}")


@router.get("/workspaces")
async def list_workspaces():
    """åˆ—å‡ºæ‰€æœ‰å·¥ä½œåŒº"""
    try:
        workspaces = load_global_workspaces()
        
        # æ·»åŠ é»˜è®¤å·¥ä½œåŒº
        if not workspaces:
            default_workspace = {
                "id": "1",
                "name": "é»˜è®¤å·¥ä½œåŒº",
                "description": "åŒ…å«æ‰€æœ‰å…¨å±€æ–‡æ¡£çš„é»˜è®¤å·¥ä½œåŒº",
                "status": "active",
                "document_count": 0,
                "created_at": "2025-10-23T22:00:00Z"
            }
            workspaces.append(default_workspace)
        
        return {
            "workspaces": workspaces,
            "total_count": len(workspaces)
        }
        
    except Exception as e:
        logger.error(f"è·å–å·¥ä½œåŒºåˆ—è¡¨å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–å¤±è´¥: {str(e)}")


@router.post("/workspaces/{workspace_id}/documents/{document_id}/access")
async def grant_document_access(
    workspace_id: str,
    document_id: str,
    access_level: str = "read"
):
    """æˆäºˆå·¥ä½œåŒºæ–‡æ¡£è®¿é—®æƒé™"""
    try:
        # ç®€åŒ–å¤„ç†ï¼Œç›´æ¥è¿”å›æˆåŠŸ
        return {
            "workspace_id": workspace_id,
            "document_id": document_id,
            "access_level": access_level,
            "status": "granted"
        }
        
    except Exception as e:
        logger.error(f"æˆäºˆæ–‡æ¡£è®¿é—®æƒé™å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æˆæƒå¤±è´¥: {str(e)}")


@router.delete("/workspaces/{workspace_id}/documents/{document_id}/access")
async def revoke_document_access(
    workspace_id: str,
    document_id: str
):
    """æ’¤é”€å·¥ä½œåŒºæ–‡æ¡£è®¿é—®æƒé™"""
    try:
        # ç®€åŒ–å¤„ç†ï¼Œç›´æ¥è¿”å›æˆåŠŸ
        return {
            "workspace_id": workspace_id,
            "document_id": document_id,
            "status": "revoked"
        }
        
    except Exception as e:
        logger.error(f"æ’¤é”€æ–‡æ¡£è®¿é—®æƒé™å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ’¤é”€å¤±è´¥: {str(e)}")


# æ€§èƒ½ä¼˜åŒ–API
@router.get("/performance/stats")
async def get_performance_stats():
    """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
    try:
        stats = performance_optimizer.get_performance_stats()
        return stats
        
    except Exception as e:
        logger.error(f"è·å–æ€§èƒ½ç»Ÿè®¡å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–ç»Ÿè®¡å¤±è´¥: {str(e)}")


@router.post("/performance/optimize")
async def optimize_performance():
    """æ‰§è¡Œæ€§èƒ½ä¼˜åŒ–"""
    try:
        # ä¼˜åŒ–å‘é‡ç´¢å¼•
        vector_db_path = "/root/consult/backend/global_db"
        optimization_result = performance_optimizer.optimize_vector_index(vector_db_path)
        
        # æ¸…ç†è¿‡æœŸç¼“å­˜
        cleanup_result = performance_optimizer.clear_cache()
        
        return {
            "success": True,
            "message": "æ€§èƒ½ä¼˜åŒ–å®Œæˆ",
            "vector_optimization": optimization_result,
            "cache_cleanup": cleanup_result
        }
        
    except Exception as e:
        logger.error(f"æ€§èƒ½ä¼˜åŒ–å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ä¼˜åŒ–å¤±è´¥: {str(e)}")


@router.delete("/performance/cache")
async def clear_performance_cache(cache_type: str = None):
    """æ¸…ç†æ€§èƒ½ç¼“å­˜"""
    try:
        result = performance_optimizer.clear_cache(cache_type)
        return result
        
    except Exception as e:
        logger.error(f"æ¸…ç†ç¼“å­˜å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ¸…ç†å¤±è´¥: {str(e)}")


@router.get("/documents/{doc_id}/download")
async def download_global_document(doc_id: str):
    """ä¸‹è½½å…¨å±€æ–‡æ¡£"""
    try:
        from fastapi.responses import FileResponse
        
        # æŸ¥æ‰¾æ–‡æ¡£æ–‡ä»¶è·¯å¾„
        rag_service = get_global_rag_service()
        vector_store = rag_service._load_vector_store("global")
        
        if vector_store and hasattr(vector_store, 'docstore'):
            docstore = vector_store.docstore
            if hasattr(docstore, '_dict') and doc_id in docstore._dict:
                doc = docstore._dict[doc_id]
                metadata = doc.metadata if hasattr(doc, 'metadata') else {}
                file_path = metadata.get('file_path')
                
                if file_path and os.path.exists(file_path):
                    filename = metadata.get('original_filename', f"document_{doc_id[:8]}")
                    
                    return FileResponse(
                        path=file_path,
                        filename=filename,
                        media_type='application/octet-stream'
                    )
        
        raise HTTPException(status_code=404, detail="æ–‡æ¡£æœªæ‰¾åˆ°")
        
    except Exception as e:
        logger.error(f"ä¸‹è½½å…¨å±€æ–‡æ¡£å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ä¸‹è½½å¤±è´¥: {str(e)}")


@router.delete("/documents/{doc_id}")
async def delete_global_document(doc_id: str):
    """åˆ é™¤å…¨å±€æ–‡æ¡£"""
    import asyncio as _asyncio
    
    try:
        logger.info(f"æ”¶åˆ°åˆ é™¤å…¨å±€æ–‡æ¡£è¯·æ±‚: {doc_id}")
        
        # 1) å¼‚æ­¥è·å–æ£€ç´¢å™¨ï¼ˆå¸¦è¶…æ—¶ï¼‰
        RETRIEVER_INIT_TIMEOUT = 120  # æ£€ç´¢å™¨åˆå§‹åŒ–è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        DELETE_OPERATION_TIMEOUT = 300  # åˆ é™¤æ“ä½œè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        
        def import_and_get_retriever():
            """åœ¨çº¿ç¨‹ä¸­æ‰§è¡Œå¯¼å…¥å’Œåˆå§‹åŒ–ï¼Œé¿å…é˜»å¡ä¸»äº‹ä»¶å¾ªç¯"""
            try:
                from app.services.llamaindex_retriever import get_retriever
                logger.info(f"âœ… LlamaIndexRetrieveræ¨¡å—å¯¼å…¥å®Œæˆ")
                retriever = get_retriever("global")
                logger.info(f"âœ… LlamaIndexRetrieveråˆå§‹åŒ–å®Œæˆ")
                return retriever
            except Exception as e:
                logger.error(f"âŒ LlamaIndexRetrieverå¯¼å…¥æˆ–åˆå§‹åŒ–å¤±è´¥: {e}", exc_info=True)
                raise
        
        retriever = None
        try:
            loop = _asyncio.get_event_loop()
            logger.info(f"â³ å¼€å§‹è·å–æ£€ç´¢å™¨å®ä¾‹ï¼ˆæœ€å¤šç­‰å¾…{RETRIEVER_INIT_TIMEOUT}ç§’ï¼‰...")
            retriever = await _asyncio.wait_for(
                loop.run_in_executor(None, import_and_get_retriever),
                timeout=RETRIEVER_INIT_TIMEOUT
            )
            logger.info(f"âœ… æ£€ç´¢å™¨å®ä¾‹è·å–æˆåŠŸ")
        except _asyncio.TimeoutError:
            logger.error(f"â° è·å–æ£€ç´¢å™¨è¶…æ—¶({RETRIEVER_INIT_TIMEOUT}ç§’)")
            raise HTTPException(
                status_code=504,
                detail=f"è·å–æ£€ç´¢å™¨è¶…æ—¶({RETRIEVER_INIT_TIMEOUT}ç§’)ï¼Œè¯·ç¨åé‡è¯•"
            )
        except Exception as e:
            logger.error(f"âŒ è·å–æ£€ç´¢å™¨å¤±è´¥: {e}", exc_info=True)
            raise HTTPException(
                status_code=500,
                detail=f"è·å–æ£€ç´¢å™¨å¤±è´¥: {str(e)}"
            )
        
        # 2) å¼‚æ­¥æ‰§è¡Œåˆ é™¤æ“ä½œï¼ˆå¸¦è¶…æ—¶ï¼‰
        def delete_operation():
            """åœ¨çº¿ç¨‹ä¸­æ‰§è¡Œåˆ é™¤æ“ä½œ"""
            try:
                logger.info(f"å¼€å§‹æ‰§è¡Œåˆ é™¤æ“ä½œ: document_id={doc_id}")
                res = retriever.delete_by_document_id(doc_id)
                logger.info(f"åˆ é™¤æ“ä½œå®Œæˆ: deleted={res.get('deleted', 0)}")
                return res
            except Exception as e:
                logger.error(f"åˆ é™¤æ“ä½œå¤±è´¥: {e}", exc_info=True)
                raise
        
        res = None
        try:
            logger.info(f"â³ å¼€å§‹åˆ é™¤æ“ä½œï¼ˆæœ€å¤šç­‰å¾…{DELETE_OPERATION_TIMEOUT}ç§’ï¼‰...")
            res = await _asyncio.wait_for(
                loop.run_in_executor(None, delete_operation),
                timeout=DELETE_OPERATION_TIMEOUT
            )
            logger.info(f"âœ… åˆ é™¤æ“ä½œæˆåŠŸå®Œæˆ")
        except _asyncio.TimeoutError:
            logger.error(f"â° åˆ é™¤æ“ä½œè¶…æ—¶({DELETE_OPERATION_TIMEOUT}ç§’)")
            raise HTTPException(
                status_code=504,
                detail=f"åˆ é™¤æ“ä½œè¶…æ—¶({DELETE_OPERATION_TIMEOUT}ç§’)ï¼Œç´¢å¼•å¯èƒ½è¾ƒå¤§ï¼Œè¯·ç¨åé‡è¯•æˆ–è”ç³»ç®¡ç†å‘˜"
            )
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤æ“ä½œå¤±è´¥: {e}", exc_info=True)
            raise HTTPException(
                status_code=500,
                detail=f"åˆ é™¤æ“ä½œå¤±è´¥: {str(e)}"
            )
        
        deleted = res.get("deleted", 0) if res else 0
        
        # 2) æ‰¾å‡º original_filename ä»¥åŒæ­¥ä¸šåŠ¡ JSON ä¸ç‰©ç†æ–‡ä»¶åˆ é™¤
        original_filename = None
        file_path = None
        try:
            ds = getattr(retriever.index.storage_context, 'docstore', None)
            nodes_map = getattr(ds, 'docs', None) or getattr(ds, '_docstore', None) or getattr(ds, '_dict', None) or {}
            # è‹¥æŒ‰ doc_id æœªå‘½ä¸­ï¼Œéœ€è¦ä»æ—§çš„å­˜å‚¨è¯»å–å…ƒæ•°æ®ï¼ˆå›é€€é€»è¾‘ï¼šç›´æ¥æ‰«æå­˜å‚¨æ–‡ä»¶è¾ƒé‡ï¼Œè¿™é‡Œä¿æŒè½»é‡è¡Œä¸ºï¼‰
        except Exception:
            nodes_map = {}
        
        # 2) å¦‚æœç´¢å¼•ä¸­æœªæ‰¾åˆ°ï¼Œå°è¯•æŒ‰æ–‡ä»¶ååˆ é™¤ï¼ˆå¯èƒ½ document_id è®¾ç½®ä¸æ­£ç¡®ï¼‰
        if deleted == 0:
            # å…ˆä»JSONä¸­è·å–æ–‡ä»¶å
            documents = load_global_documents()
            for doc in documents:
                if doc.get('id') == doc_id:
                    original_filename = doc.get('original_filename') or original_filename
                    file_path = doc.get('file_path') or file_path
                    break
            
            # å°è¯•æŒ‰æ–‡ä»¶ååˆ é™¤ï¼ˆå¼‚æ­¥æ‰§è¡Œï¼‰
            if original_filename:
                logger.warning(f"æŒ‰ document_id={doc_id} æœªåœ¨ç´¢å¼•ä¸­æ‰¾åˆ°ï¼Œå°è¯•æŒ‰æ–‡ä»¶ååˆ é™¤: {original_filename}")
                def delete_by_filename_operation():
                    """åœ¨çº¿ç¨‹ä¸­æ‰§è¡ŒæŒ‰æ–‡ä»¶ååˆ é™¤æ“ä½œ"""
                    try:
                        logger.info(f"å¼€å§‹æŒ‰æ–‡ä»¶ååˆ é™¤æ“ä½œ: filename={original_filename}")
                        res2 = retriever.delete_by_original_filename(original_filename)
                        logger.info(f"æŒ‰æ–‡ä»¶ååˆ é™¤æ“ä½œå®Œæˆ: deleted={res2.get('deleted', 0)}")
                        return res2
                    except Exception as e:
                        logger.error(f"æŒ‰æ–‡ä»¶ååˆ é™¤æ“ä½œå¤±è´¥: {e}", exc_info=True)
                        raise
                
                try:
                    res2 = await _asyncio.wait_for(
                        loop.run_in_executor(None, delete_by_filename_operation),
                        timeout=DELETE_OPERATION_TIMEOUT
                    )
                    deleted = max(deleted, res2.get("deleted", 0))
                    if deleted > 0:
                        logger.info(f"é€šè¿‡æ–‡ä»¶ååˆ é™¤æˆåŠŸï¼Œåˆ é™¤äº† {deleted} ä¸ªèŠ‚ç‚¹")
                except _asyncio.TimeoutError:
                    logger.warning(f"æŒ‰æ–‡ä»¶ååˆ é™¤æ“ä½œè¶…æ—¶({DELETE_OPERATION_TIMEOUT}ç§’)")
                except Exception as e:
                    logger.warning(f"æŒ‰æ–‡ä»¶ååˆ é™¤æ“ä½œå¤±è´¥: {e}")
            
            # å¦‚æœä»ç„¶æœªæ‰¾åˆ°ï¼Œå¯èƒ½æ˜¯ç´¢å¼•åˆ›å»ºæœ‰é—®é¢˜
            if deleted == 0:
                logger.error(f"âš ï¸ æ–‡æ¡£ {doc_id} åœ¨ç´¢å¼•ä¸­æœªæ‰¾åˆ°ï¼è¿™å¯èƒ½è¡¨ç¤ºç´¢å¼•åˆ›å»ºæ—¶æœªæ­£ç¡®è®¾ç½® document_idã€‚")
                logger.error(f"è¯·æ£€æŸ¥ä¸Šä¼ æ–‡æ¡£æ—¶çš„æ—¥å¿—ï¼Œç¡®è®¤ metadata ä¸­çš„ document_id æ˜¯å¦æ­£ç¡®è®¾ç½®ã€‚")
                # ä½†ä»ç„¶å°è¯•æ¸…ç†å…¶ä»–æ•°æ®
                logger.warning(f"å°†å°è¯•æ¸…ç† JSONã€æ–‡ä»¶ç´¢å¼•å’Œç‰©ç†æ–‡ä»¶...")
        
        # 3) è·å–å®Œæ•´çš„æ–‡æ¡£ä¿¡æ¯ï¼ˆç”¨äºåç»­æ¸…ç†ï¼‰
        if not original_filename or not file_path:
            documents = load_global_documents()
            for doc in documents:
                if doc.get('id') == doc_id:
                    original_filename = doc.get('original_filename') or original_filename
                    file_path = doc.get('file_path') or file_path
                    break
        
        # 4) åˆ é™¤ç‰©ç†æ–‡ä»¶
        if file_path and os.path.exists(file_path):
            try:
                os.remove(file_path)
                logger.info(f"æˆåŠŸåˆ é™¤ç‰©ç†æ–‡ä»¶: {file_path}")
            except Exception as e:
                logger.warning(f"åˆ é™¤ç‰©ç†æ–‡ä»¶å¤±è´¥: {str(e)}")
        
        # 5) ä»æ–‡ä»¶ç´¢å¼•ç§»é™¤
        file_index_removed = False
        try:
            documents_for_index = load_global_documents()
            for doc in documents_for_index:
                if doc.get('id') == doc_id:
                    file_index_manager.remove_file(doc_id)
                    file_index_removed = True
                    logger.info(f"ä»æ–‡ä»¶ç´¢å¼•åˆ é™¤: {doc_id}")
                    break
                # å¦‚æœé€šè¿‡æ–‡ä»¶ååŒ¹é…
                elif original_filename and doc.get('original_filename') == original_filename:
                    file_index_manager.remove_file(doc.get('id'))
                    file_index_removed = True
                    logger.info(f"ä»æ–‡ä»¶ç´¢å¼•åˆ é™¤ï¼ˆæŒ‰æ–‡ä»¶åï¼‰: {original_filename}")
                    break
        except Exception as e:
            logger.warning(f"ä»æ–‡ä»¶ç´¢å¼•åˆ é™¤å¤±è´¥: {str(e)}")
        
        # 6) ä» JSON ä¸­åˆ é™¤è®°å½•
        documents = load_global_documents()
        before = len(documents)
        if original_filename:
            documents = [d for d in documents if d.get('original_filename') != original_filename]
        else:
            documents = [d for d in documents if d.get('id') != doc_id]
        save_global_documents(documents)
        after = len(documents)
        json_deleted_count = before - after
        if json_deleted_count > 0:
            logger.info(f"ä»JSONä¸­åˆ é™¤äº† {json_deleted_count} æ¡è®°å½•")
        
        # 7) å¦‚æœç´¢å¼•ä¸­æœªæ‰¾åˆ°ï¼Œè¿™æ˜¯ä¸»è¦é—®é¢˜
        if deleted == 0:
            logger.error(f"âŒ åˆ é™¤å¤±è´¥ï¼šæ–‡æ¡£ {doc_id} åœ¨å‘é‡ç´¢å¼•ä¸­æœªæ‰¾åˆ°ï¼")
            logger.error(f"è¿™å¯èƒ½æ˜¯å› ä¸ºï¼š")
            logger.error(f"  1. ä¸Šä¼ æ—¶ metadata ä¸­çš„ document_id æœªæ­£ç¡®è®¾ç½®")
            logger.error(f"  2. ç´¢å¼•åˆ›å»ºæˆ–æŒä¹…åŒ–æ—¶å‡ºç°é—®é¢˜")
            logger.error(f"  3. ç´¢å¼•åŠ è½½æ—¶èŠ‚ç‚¹ metadata ä¸¢å¤±")
            logger.error(f"å·²æ¸…ç† JSON å’Œæ–‡ä»¶ç³»ç»Ÿæ•°æ®ï¼ˆ{json_deleted_count} æ¡JSONè®°å½•ï¼Œæ–‡ä»¶ç´¢å¼•: {file_index_removed}ï¼‰")
            raise HTTPException(
                status_code=404, 
                detail=f"æ–‡æ¡£åœ¨ç´¢å¼•ä¸­æœªæ‰¾åˆ°ï¼ˆdocument_id={doc_id}ï¼‰ã€‚å·²æ¸…ç†å…¶ä»–æ•°æ®ï¼Œä½†ç´¢å¼•å¯èƒ½éœ€è¦é‡å»ºã€‚"
            )
        
        return {
            "status": "deleted",
            "id": doc_id,
            "filename": original_filename,
            "deleted_nodes": deleted,
            "message": "æ–‡æ¡£å·²ä»ç´¢å¼•ã€JSONã€æ–‡ä»¶ç´¢å¼•å’Œç‰©ç†æ–‡ä»¶ä¸­åˆ é™¤"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ é™¤å…¨å±€æ–‡æ¡£å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ é™¤å¤±è´¥: {str(e)}")


@router.delete("/documents/by-filename")
async def delete_global_document_by_filename(filename: str):
    """æŒ‰åŸå§‹æ–‡ä»¶ååˆ é™¤å…¨å±€æ–‡æ¡£å¹¶æŒä¹…åŒ–ç´¢å¼•ã€‚"""
    import asyncio as _asyncio
    
    try:
        if not filename:
            raise HTTPException(status_code=400, detail="ç¼ºå°‘æ–‡ä»¶å")

        logger.info(f"æ”¶åˆ°æŒ‰æ–‡ä»¶ååˆ é™¤å…¨å±€æ–‡æ¡£è¯·æ±‚: {filename}")

        # 1) å¼‚æ­¥è·å–æ£€ç´¢å™¨ï¼ˆå¸¦è¶…æ—¶ï¼‰
        RETRIEVER_INIT_TIMEOUT = 120  # æ£€ç´¢å™¨åˆå§‹åŒ–è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        DELETE_OPERATION_TIMEOUT = 300  # åˆ é™¤æ“ä½œè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        
        def import_and_get_retriever():
            """åœ¨çº¿ç¨‹ä¸­æ‰§è¡Œå¯¼å…¥å’Œåˆå§‹åŒ–ï¼Œé¿å…é˜»å¡ä¸»äº‹ä»¶å¾ªç¯"""
            try:
                from app.services.llamaindex_retriever import get_retriever
                logger.info(f"âœ… LlamaIndexRetrieveræ¨¡å—å¯¼å…¥å®Œæˆ")
                retriever = get_retriever("global")
                logger.info(f"âœ… LlamaIndexRetrieveråˆå§‹åŒ–å®Œæˆ")
                return retriever
            except Exception as e:
                logger.error(f"âŒ LlamaIndexRetrieverå¯¼å…¥æˆ–åˆå§‹åŒ–å¤±è´¥: {e}", exc_info=True)
                raise
        
        retriever = None
        try:
            loop = _asyncio.get_event_loop()
            logger.info(f"â³ å¼€å§‹è·å–æ£€ç´¢å™¨å®ä¾‹ï¼ˆæœ€å¤šç­‰å¾…{RETRIEVER_INIT_TIMEOUT}ç§’ï¼‰...")
            retriever = await _asyncio.wait_for(
                loop.run_in_executor(None, import_and_get_retriever),
                timeout=RETRIEVER_INIT_TIMEOUT
            )
            logger.info(f"âœ… æ£€ç´¢å™¨å®ä¾‹è·å–æˆåŠŸ")
        except _asyncio.TimeoutError:
            logger.error(f"â° è·å–æ£€ç´¢å™¨è¶…æ—¶({RETRIEVER_INIT_TIMEOUT}ç§’)")
            raise HTTPException(
                status_code=504,
                detail=f"è·å–æ£€ç´¢å™¨è¶…æ—¶({RETRIEVER_INIT_TIMEOUT}ç§’)ï¼Œè¯·ç¨åé‡è¯•"
            )
        except Exception as e:
            logger.error(f"âŒ è·å–æ£€ç´¢å™¨å¤±è´¥: {e}", exc_info=True)
            raise HTTPException(
                status_code=500,
                detail=f"è·å–æ£€ç´¢å™¨å¤±è´¥: {str(e)}"
            )

        # 2) å¼‚æ­¥æ‰§è¡Œåˆ é™¤æ“ä½œï¼ˆå¸¦è¶…æ—¶ï¼‰
        def delete_by_filename_operation():
            """åœ¨çº¿ç¨‹ä¸­æ‰§è¡ŒæŒ‰æ–‡ä»¶ååˆ é™¤æ“ä½œ"""
            try:
                logger.info(f"å¼€å§‹æ‰§è¡ŒæŒ‰æ–‡ä»¶ååˆ é™¤æ“ä½œ: filename={filename}")
                res = retriever.delete_by_original_filename(filename)
                logger.info(f"æŒ‰æ–‡ä»¶ååˆ é™¤æ“ä½œå®Œæˆ: deleted={res.get('deleted', 0)}")
                return res
            except Exception as e:
                logger.error(f"æŒ‰æ–‡ä»¶ååˆ é™¤æ“ä½œå¤±è´¥: {e}", exc_info=True)
                raise
        
        res = None
        try:
            logger.info(f"â³ å¼€å§‹æŒ‰æ–‡ä»¶ååˆ é™¤æ“ä½œï¼ˆæœ€å¤šç­‰å¾…{DELETE_OPERATION_TIMEOUT}ç§’ï¼‰...")
            res = await _asyncio.wait_for(
                loop.run_in_executor(None, delete_by_filename_operation),
                timeout=DELETE_OPERATION_TIMEOUT
            )
            logger.info(f"âœ… æŒ‰æ–‡ä»¶ååˆ é™¤æ“ä½œæˆåŠŸå®Œæˆ")
        except _asyncio.TimeoutError:
            logger.error(f"â° æŒ‰æ–‡ä»¶ååˆ é™¤æ“ä½œè¶…æ—¶({DELETE_OPERATION_TIMEOUT}ç§’)")
            raise HTTPException(
                status_code=504,
                detail=f"åˆ é™¤æ“ä½œè¶…æ—¶({DELETE_OPERATION_TIMEOUT}ç§’)ï¼Œç´¢å¼•å¯èƒ½è¾ƒå¤§ï¼Œè¯·ç¨åé‡è¯•æˆ–è”ç³»ç®¡ç†å‘˜"
            )
        except Exception as e:
            logger.error(f"âŒ æŒ‰æ–‡ä»¶ååˆ é™¤æ“ä½œå¤±è´¥: {e}", exc_info=True)
            raise HTTPException(
                status_code=500,
                detail=f"åˆ é™¤æ“ä½œå¤±è´¥: {str(e)}"
            )
        
        deleted = res.get("deleted", 0) if res else 0

        if deleted == 0:
            logger.warning(f"æŒ‰æ–‡ä»¶åæœªå‘½ä¸­: {filename}")
            raise HTTPException(status_code=404, detail="æ–‡æ¡£æœªåœ¨ç´¢å¼•ä¸­æ‰¾åˆ°")

        # 2) åˆ é™¤ç‰©ç†æ–‡ä»¶ä¸ file_index_manager
        documents = load_global_documents()
        removed_files = []
        try:
            for doc in documents:
                if doc.get('original_filename') == filename:
                    fid = doc.get('id')
                    fpath = doc.get('file_path')
                    if fpath and os.path.exists(fpath):
                        try:
                            os.remove(fpath)
                            removed_files.append(fpath)
                        except Exception as e:
                            logger.warning(f"åˆ é™¤ç‰©ç†æ–‡ä»¶å¤±è´¥: {fpath}, {e}")
                    try:
                        file_index_manager.remove_file(fid)
                    except Exception as e:
                        logger.warning(f"ä»ç´¢å¼•åˆ é™¤å¤±è´¥: {e}")
        except Exception as e:
            logger.warning(f"åˆ é™¤ç›¸å…³ç‰©ç†æ–‡ä»¶æˆ–ç´¢å¼•å¤±è´¥: {e}")

        # 3) è¿‡æ»¤å¹¶ä¿å­˜ JSON è®°å½•
        before = len(documents)
        documents = [d for d in documents if d.get('original_filename') != filename]
        save_global_documents(documents)
        after = len(documents)
        logger.info(f"ä»JSONä¸­åˆ é™¤äº† {before - after} æ¡è®°å½•ï¼Œæ–‡ä»¶: {filename}")

        return {
            "status": "deleted",
            "filename": filename,
            "deleted_nodes": deleted,
            "removed_files": removed_files
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æŒ‰æ–‡ä»¶ååˆ é™¤å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"åˆ é™¤å¤±è´¥: {str(e)}")


@router.post("/rebuild-index")
async def rebuild_file_index():
    """é‡å»ºæ–‡ä»¶ç´¢å¼•ï¼ˆç”¨äºæ•°æ®æ¢å¤ï¼‰"""
    try:
        logger.info("å¼€å§‹é‡å»ºæ–‡ä»¶ç´¢å¼•...")
        
        rag_service = get_global_rag_service()
        vector_store = rag_service._load_vector_store("global")
        
        # é‡å»ºç´¢å¼•
        file_index_manager.rebuild_index_from_vector_store(vector_store)
        
        # è·å–é‡å»ºåçš„ç»Ÿè®¡ä¿¡æ¯
        file_count = file_index_manager.get_file_count()
        files = file_index_manager.list_files()
        
        total_chunks = sum(file_info.get("chunk_count", 0) for file_info in files)
        
        logger.info(f"ç´¢å¼•é‡å»ºå®Œæˆ: {file_count} ä¸ªæ–‡ä»¶, {total_chunks} ä¸ªå—")
        
        return {
            "status": "success",
            "message": "æ–‡ä»¶ç´¢å¼•é‡å»ºå®Œæˆ",
            "file_count": file_count,
            "total_chunks": total_chunks,
            "files": files
        }
        
    except Exception as e:
        logger.error(f"é‡å»ºæ–‡ä»¶ç´¢å¼•å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"é‡å»ºç´¢å¼•å¤±è´¥: {str(e)}")


@router.get("/index-status")
async def get_index_status():
    """è·å–ç´¢å¼•çŠ¶æ€"""
    try:
        file_count = file_index_manager.get_file_count()
        files = file_index_manager.list_files()
        
        total_chunks = sum(file_info.get("chunk_count", 0) for file_info in files)
        total_size = sum(file_info.get("file_size", 0) for file_info in files)
        
        return {
            "file_count": file_count,
            "total_chunks": total_chunks,
            "total_size": total_size,
            "index_file": str(file_index_manager.index_file),
            "files": files
        }
        
    except Exception as e:
        logger.error(f"è·å–ç´¢å¼•çŠ¶æ€å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–çŠ¶æ€å¤±è´¥: {str(e)}")
