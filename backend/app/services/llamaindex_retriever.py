"""
LlamaIndex é«˜çº§æ£€ç´¢å¼•æ“
æä¾›æ··åˆæ£€ç´¢ã€é‡æ’åºã€è¯­ä¹‰åˆ†å—ã€ä¸Šä¸‹æ–‡å‹ç¼©ç­‰åŠŸèƒ½
"""

from typing import List, Dict, Optional
from pathlib import Path
import logging
import os

logger = logging.getLogger(__name__)

try:
    # ä½¿ç”¨æœ€å°ä¾èµ–é›†åˆï¼Œé¿å…è§¦å‘ LLM æ¨¡å—
    from llama_index.core.indices.vector_store import VectorStoreIndex
    from llama_index.core.storage.storage_context import StorageContext
    from llama_index.core.indices.loading import load_index_from_storage
    from llama_index.core.node_parser import SemanticSplitterNodeParser
    from llama_index.embeddings.huggingface import HuggingFaceEmbedding
    from llama_index.core import SimpleDirectoryReader
except ImportError as e:
    # 0.10.x å¤‡ç”¨å¯¼å…¥è·¯å¾„ï¼ˆè¿›ä¸€æ­¥æœ€å°åŒ–ï¼‰
    logger.error(f"LlamaIndex å¯¼å…¥å¤±è´¥: {e}ï¼Œå°è¯•å¤‡ç”¨è·¯å¾„")
    try:
        from llama_index import VectorStoreIndex, StorageContext, load_index_from_storage, SimpleDirectoryReader
        from llama_index.node_parser import SemanticSplitterNodeParser
        from llama_index.embeddings import HuggingFaceEmbedding
    except ImportError:
        raise ImportError("æ— æ³•å¯¼å…¥ LlamaIndex æ¨¡å—ï¼Œè¯·æ£€æŸ¥å®‰è£…")

class LlamaIndexRetriever:
    """LlamaIndex é«˜çº§æ£€ç´¢å¼•æ“ - ä¸º LangGraph æä¾›æ£€ç´¢æœåŠ¡"""
    
    def __init__(self, workspace_id: str = "global"):
        self.workspace_id = workspace_id
        self.storage_dir = Path(f"llamaindex_storage/{workspace_id}")
        
        # åµŒå…¥æ¨¡å‹ï¼ˆå¼ºåˆ¶æœ¬åœ°åŠ è½½ï¼Œé¿å…è¿æ¥HuggingFaceï¼‰
        local_model_dir = os.getenv("LOCAL_BGE_MODEL_DIR", "")
        logger.info(f"æœ¬åœ°BGEæ¨¡å‹: {local_model_dir}")
        if not local_model_dir or not Path(local_model_dir).exists():
            raise RuntimeError(
                f"LOCAL_BGE_MODEL_DIR æœªè®¾ç½®æˆ–ç›®å½•ä¸å­˜åœ¨: {local_model_dir}\n"
                f"è¯·è®¾ç½®ç¯å¢ƒå˜é‡ LOCAL_BGE_MODEL_DIR æŒ‡å‘æœ¬åœ°BGEæ¨¡å‹ç›®å½•ï¼Œæˆ–ç¡®ä¿è¯¥ç›®å½•å­˜åœ¨ã€‚"
            )
        # å¼ºåˆ¶ç¦»çº¿æ¨¡å¼
        os.environ['HF_HUB_OFFLINE'] = '1'
        os.environ['HF_DATASETS_OFFLINE'] = '1'
        self.embed_model = HuggingFaceEmbedding(
            model_name=local_model_dir,
            cache_folder=local_model_dir  # ä½¿ç”¨æ¨¡å‹ç›®å½•ä½œä¸ºç¼“å­˜
        )
        logger.info(f"âœ… ä½¿ç”¨æœ¬åœ°BGEæ¨¡å‹ï¼ˆç¦»çº¿æ¨¡å¼ï¼‰: {local_model_dir}")
        
        # è¯­ä¹‰åˆ†å—å™¨
        self.node_parser = SemanticSplitterNodeParser(
            buffer_size=1,
            breakpoint_percentile_threshold=95,
            embed_model=self.embed_model
        )
        
        # # åå¤„ç†å™¨æ ˆ
        # self._init_postprocessors()
        
        # ç´¢å¼•
        self.index = self._load_or_create_index()
        logger.info(f"âœ… initå®Œæˆ")
    
    def _init_postprocessors(self):
        """åˆå§‹åŒ–åå¤„ç†å™¨æ ˆ"""
        # é‡æ’åº
        self.reranker = SentenceTransformerRerank(
            model="BAAI/bge-reranker-v2-m3",
            top_n=10
        )
        
        # ç›¸ä¼¼åº¦è¿‡æ»¤
        self.similarity_filter = SimilarityPostprocessor(
            similarity_cutoff=0.7
        )
        
        # å¥å­çº§å‹ç¼©ï¼ˆLlamaIndex åŸç”Ÿï¼‰- å¦‚æœå¯ç”¨åˆ™å¯ç”¨
        if SentenceEmbeddingOptimizer is not None:
            self.sentence_optimizer = SentenceEmbeddingOptimizer(
                embed_model=self.embed_model,
                percentile_cutoff=0.5,
                threshold_cutoff=0.7
            )
        else:
            logger.warning("SentenceEmbeddingOptimizer ä¸å¯ç”¨ï¼Œè·³è¿‡å¥å­çº§å‹ç¼©")
            self.sentence_optimizer = None
        
        # é•¿ä¸Šä¸‹æ–‡é‡æ’åº
        self.context_reorder = LongContextReorder()
    
    def _load_or_create_index(self):
        """åŠ è½½æˆ–åˆ›å»ºç´¢å¼•"""
        try:
            if self.storage_dir.exists():
                # æ£€æŸ¥æ˜¯å¦æœ‰å¿…è¦çš„ç´¢å¼•æ–‡ä»¶
                docstore_file = self.storage_dir / "docstore.json"
                if docstore_file.exists():
                    storage_context = StorageContext.from_defaults(
                        persist_dir=str(self.storage_dir)
                    )
                    index = load_index_from_storage(
                        storage_context,
                        embed_model=self.embed_model
                    )
                    logger.info(f"âœ… åŠ è½½ç´¢å¼•: {self.workspace_id}")
                    return index
                else:
                    logger.warning(f"âš ï¸ ç´¢å¼•æ–‡ä»¶ä¸å®Œæ•´ï¼Œé‡æ–°åˆ›å»º: {self.workspace_id}")
                    # åˆ›å»ºç©ºç´¢å¼•
                    return VectorStoreIndex([], embed_model=self.embed_model)
            else:
                # wikiç›®å½•ä¸å­˜åœ¨ï¼Œåˆ›å»ºç©ºç´¢å¼•
                self.storage_dir.mkdir(parents=True, exist_ok=True)
                logger.info(f"åˆ›å»ºç´¢å¼•: {self.workspace_id}")
                return VectorStoreIndex([], embed_model=self.embed_model)
        except Exception as e:
            logger.error(f"åŠ è½½ç´¢å¼•å¤±è´¥: {e}")
            # åˆ›å»ºç©ºç´¢å¼•ä½œä¸ºåå¤‡
            self.storage_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"åå¤‡æ–¹æ¡ˆï¼šåˆ›å»ºç©ºç´¢å¼•: {self.workspace_id}")
            return VectorStoreIndex([], embed_model=self.embed_model)
    
    async def retrieve(
        self,
        query: str,
        top_k: int = 5,
        use_hybrid: bool = True,
        use_compression: bool = True
    ) -> List[Dict]:
        """
        é«˜çº§æ£€ç´¢ - è¿”å› LangGraph å¯ç”¨çš„æ ¼å¼
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            use_hybrid: æ˜¯å¦ä½¿ç”¨æ··åˆæ£€ç´¢
            use_compression: æ˜¯å¦ä½¿ç”¨å‹ç¼©
        
        Returns:
            List[Dict]: åŒ…å« content, metadata, score, node_id
        """
        try:
            # å¦‚æœç´¢å¼•ä¸ºç©ºï¼Œè¿”å›ç©ºç»“æœ
            if self.index is None or len(self.index.storage_context.docstore.docs) == 0:
                logger.info(f"ç´¢å¼•ä¸ºç©ºï¼Œè¿”å›ç©ºç»“æœ: {self.workspace_id}")
                return []
            
            # 1. æ£€ç´¢ï¼ˆä½¿ç”¨å†…ç½®ç®€åŒ–æ£€ç´¢å™¨ï¼Œé¿å…è§¦å‘ LLM ç›¸å…³æ¨¡å—ï¼‰
            retriever = self.index.as_retriever(similarity_top_k=top_k * (4 if use_hybrid else 2))
            nodes = await retriever.aretrieve(query)
            
            # 4. è½¬æ¢æ ¼å¼
            results = []
            for node in nodes[:top_k]:
                results.append({
                    "content": node.get_content(),
                    "metadata": node.metadata,
                    "score": node.score if hasattr(node, 'score') else 0.0,
                    "node_id": node.node_id
                })
            
            return results
            
        except Exception as e:
            logger.error(f"æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    async def add_document(self, file_path: str, metadata: Dict = None):
        """æ·»åŠ æ–‡æ¡£ï¼ˆä½¿ç”¨è¯­ä¹‰åˆ†å—å¹¶ç”Ÿæˆå‘é‡ï¼‰"""
        try:
            from llama_index.core import SimpleDirectoryReader
            import traceback
            
            logger.info(f"ğŸ“„ å¼€å§‹åŠ è½½æ–‡æ¡£: {file_path}")
            documents = SimpleDirectoryReader(input_files=[file_path]).load_data()
            logger.info(f"âœ… æ–‡æ¡£åŠ è½½å®Œæˆï¼Œé¡µæ•°: {len(documents)}")
            
            if metadata:
                for doc in documents:
                    doc.metadata.update(metadata)
            
            logger.info(f"ğŸ“ å¼€å§‹æ’å…¥æ–‡æ¡£åˆ°ç´¢å¼•...")
            # ä½¿ç”¨ insert é€ä¸ªæ’å…¥æ–‡æ¡£ï¼Œç´¢å¼•ä¼šè‡ªåŠ¨å¤„ç†åˆ†å—å’Œå‘é‡åŒ–
            for doc in documents:
                self.index.insert(doc)
            
            logger.info(f"ğŸ’¾ å¼€å§‹æŒä¹…åŒ–ç´¢å¼•...")
            self.index.storage_context.persist(persist_dir=str(self.storage_dir))
            logger.info(f"âœ… ç´¢å¼•æŒä¹…åŒ–å®Œæˆ")
            
            # ä»ç´¢å¼•ä¸­è·å–èŠ‚ç‚¹æ•°é‡
            if hasattr(self.index, '_docstore') and self.index._docstore:
                node_count = len(self.index._docstore.docs)
                logger.info(f"âœ… æ–‡æ¡£æ’å…¥å®Œæˆï¼Œæ€»èŠ‚ç‚¹æ•°: {node_count}")
                return node_count
            else:
                logger.warning("âš ï¸ æ— æ³•è·å–èŠ‚ç‚¹æ•°é‡")
                return len(documents)  # å›é€€åˆ°æ–‡æ¡£æ•°é‡
            
        except Exception as e:
            logger.error(f"æ·»åŠ æ–‡æ¡£å¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            return 0

