"""
LlamaIndex é«˜çº§æ£€ç´¢å¼•æ“
æä¾›æ··åˆæ£€ç´¢ã€é‡æ’åºã€è¯­ä¹‰åˆ†å—ã€ä¸Šä¸‹æ–‡å‹ç¼©ç­‰åŠŸèƒ½
"""

# ============================================
# å…³é”®ï¼šå¿…é¡»åœ¨æ‰€æœ‰ HuggingFace ç›¸å…³å¯¼å…¥ä¹‹å‰è®¾ç½®ç¦»çº¿æ¨¡å¼
# HuggingFace åœ¨ import æ—¶å°±ä¼šå°è¯•è”ç½‘æ£€æŸ¥æ›´æ–°ã€ä¸‹è½½é…ç½®ç­‰
# ============================================
import os

# å¼ºåˆ¶ç¦»çº¿æ¨¡å¼ - å¿…é¡»åœ¨ import ä¹‹å‰è®¾ç½®
os.environ['HF_HUB_OFFLINE'] = '1'  # ç¦ç”¨ HuggingFace Hub è¿æ¥
os.environ['HF_DATASETS_OFFLINE'] = '1'  # ç¦ç”¨æ•°æ®é›†ä¸‹è½½
os.environ['TRANSFORMERS_OFFLINE'] = '1'  # ç¦ç”¨ Transformers åœ¨çº¿åŠŸèƒ½
os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'  # ç¦ç”¨é¥æµ‹ï¼ˆé¿å…è”ç½‘ï¼‰
os.environ['HF_HUB_DISABLE_PROGRESS_BARS'] = '1'  # ç¦ç”¨è¿›åº¦æ¡ï¼ˆå¯èƒ½è§¦å‘ç½‘ç»œæ£€æŸ¥ï¼‰
os.environ['HF_HUB_DISABLE_EXPERIMENTAL_WARNING'] = '1'  # ç¦ç”¨å®éªŒæ€§è­¦å‘Š

# ç¦ç”¨è‡ªåŠ¨ä¸‹è½½å’Œæ›´æ–°æ£€æŸ¥
os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'
os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # é¿å… tokenizer åˆå§‹åŒ–æ—¶çš„ç½‘ç»œæ£€æŸ¥

# æ›´ä¸¥æ ¼çš„ç¦»çº¿æ§åˆ¶ï¼ˆé˜²æ­¢å¯¼å…¥æ—¶è”ç½‘ï¼‰
os.environ['HF_HUB_DISABLE_VERSION_CHECK'] = '1'  # ç¦ç”¨ç‰ˆæœ¬æ£€æŸ¥
os.environ['NO_PROXY'] = '*'  # ç¦ç”¨æ‰€æœ‰ä»£ç†
os.environ['HTTP_PROXY'] = ''
os.environ['HTTPS_PROXY'] = ''
os.environ['http_proxy'] = ''
os.environ['https_proxy'] = ''

# è®¾ç½®ç½‘ç»œè¶…æ—¶ä¸º0ï¼ˆç«‹å³å¤±è´¥ï¼Œä¸ç­‰å¾…ï¼‰
os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = '0.1'
os.environ['REQUESTS_TIMEOUT'] = '0.1'

# ç¦ç”¨æ‰€æœ‰å¯èƒ½è§¦å‘ç½‘ç»œè¯·æ±‚çš„åŠŸèƒ½
os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '0'  # ç¦ç”¨HFä¼ è¾“ï¼ˆå¯èƒ½è§¦å‘ç½‘ç»œæ£€æŸ¥ï¼‰
os.environ['HF_HUB_DISABLE_EXPLICIT_TIMEOUT'] = '1'  # ç¦ç”¨æ˜¾å¼è¶…æ—¶æ£€æŸ¥

# è®¾ç½®æœ¬åœ°ç¼“å­˜ç›®å½•ï¼ˆé¿å…å°è¯•ä»è¿œç¨‹ä¸‹è½½ï¼‰
# å¦‚æœå·²ç»è®¾ç½®äº† LOCAL_BGE_MODEL_DIRï¼Œä½¿ç”¨å®ƒä½œä¸ºç¼“å­˜
if 'LOCAL_BGE_MODEL_DIR' in os.environ:
    local_model_dir = os.environ['LOCAL_BGE_MODEL_DIR']
    os.environ['HF_HOME'] = local_model_dir
    os.environ['HUGGINGFACE_HUB_CACHE'] = local_model_dir
    os.environ['TRANSFORMERS_CACHE'] = local_model_dir

from typing import List, Dict, Optional
from pathlib import Path
import logging
import threading
import time

logger = logging.getLogger(__name__)

# å…¨å±€å®ä¾‹ç¼“å­˜ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
_retriever_cache: Dict[str, 'LlamaIndexRetriever'] = {}
_embed_model_cache: Optional[object] = None
# ä½¿ç”¨ä¸¤æŠŠç‹¬ç«‹çš„é”ï¼Œé¿å…å®ä¾‹åŒ–æœŸé—´å‘ç”Ÿè‡ªé”
_retriever_lock = threading.Lock()
_embed_lock = threading.Lock()

# æ¨¡å—å¯¼å…¥æ—¶é—´ç»Ÿè®¡ï¼ˆç”¨äºè¯Šæ–­å¯¼å…¥æ€§èƒ½é—®é¢˜ï¼‰
_import_start_time = time.time()
logger.info(f"ğŸ”„ å¼€å§‹åŠ è½½ LlamaIndex æ¨¡å—ï¼ˆç¦»çº¿æ¨¡å¼å·²å¯ç”¨ï¼‰")

try:
    # ä½¿ç”¨æœ€å°ä¾èµ–é›†åˆï¼Œé¿å…è§¦å‘ LLM æ¨¡å—
    _module_times = {}
    
    # VectorStoreIndex
    _t0 = time.time()
    from llama_index.core.indices.vector_store import VectorStoreIndex
    _module_times['VectorStoreIndex'] = time.time() - _t0
    logger.info(f"âœ… VectorStoreIndex åŠ è½½æˆåŠŸ ({_module_times['VectorStoreIndex']:.3f}s)")
    
    # StorageContext
    _t0 = time.time()
    from llama_index.core.storage.storage_context import StorageContext
    _module_times['StorageContext'] = time.time() - _t0
    logger.info(f"âœ… StorageContext åŠ è½½æˆåŠŸ ({_module_times['StorageContext']:.3f}s)")
    
    # load_index_from_storage
    _t0 = time.time()
    from llama_index.core.indices.loading import load_index_from_storage
    _module_times['load_index_from_storage'] = time.time() - _t0
    logger.info(f"âœ… load_index_from_storage åŠ è½½æˆåŠŸ ({_module_times['load_index_from_storage']:.3f}s)")
    
    # SemanticSplitterNodeParser
    _t0 = time.time()
    from llama_index.core.node_parser import SemanticSplitterNodeParser
    _module_times['SemanticSplitterNodeParser'] = time.time() - _t0
    logger.info(f"âœ… SemanticSplitterNodeParser åŠ è½½æˆåŠŸ ({_module_times['SemanticSplitterNodeParser']:.3f}s)")
    
    # HuggingFaceEmbeddingï¼ˆå¯èƒ½æœ€è€—æ—¶ï¼Œå› ä¸ºå®ƒä¼šè§¦å‘ transformers å¯¼å…¥ï¼‰
    _t0 = time.time()
    from llama_index.embeddings.huggingface import HuggingFaceEmbedding
    _module_times['HuggingFaceEmbedding'] = time.time() - _t0
    logger.info(f"âœ… HuggingFaceEmbedding åŠ è½½æˆåŠŸ ({_module_times['HuggingFaceEmbedding']:.3f}s)")
    
    # SimpleDirectoryReader
    _t0 = time.time()
    from llama_index.core import SimpleDirectoryReader
    _module_times['SimpleDirectoryReader'] = time.time() - _t0
    logger.info(f"âœ… SimpleDirectoryReader åŠ è½½æˆåŠŸ ({_module_times['SimpleDirectoryReader']:.3f}s)")
    
    _total_import_time = time.time() - _import_start_time
    logger.info(f"âœ… æ‰€æœ‰ LlamaIndex æ¨¡å—åŠ è½½å®Œæˆï¼Œæ€»è€—æ—¶: {_total_import_time:.3f}s")
    logger.debug(f"æ¨¡å—åŠ è½½æ—¶é—´æ˜ç»†: {_module_times}")
    
    # å¦‚æœæ€»è€—æ—¶è¶…è¿‡5ç§’ï¼Œå‘å‡ºè­¦å‘Š
    if _total_import_time > 5.0:
        logger.warning(f"âš ï¸ æ¨¡å—åŠ è½½è€—æ—¶è¾ƒé•¿ ({_total_import_time:.3f}s)ï¼Œå¯èƒ½è§¦å‘äº†ç½‘ç»œè¯·æ±‚æˆ–æ…¢é€Ÿæ£€æŸ¥")
        if _module_times.get('HuggingFaceEmbedding', 0) > 3.0:
            logger.warning(f"âš ï¸ HuggingFaceEmbedding å¯¼å…¥è€—æ—¶ {_module_times['HuggingFaceEmbedding']:.3f}sï¼Œå¯èƒ½æ˜¯ transformers åº“è§¦å‘äº†ç½‘ç»œæ£€æŸ¥")
            
except ImportError as e:
    # 0.10.x å¤‡ç”¨å¯¼å…¥è·¯å¾„ï¼ˆè¿›ä¸€æ­¥æœ€å°åŒ–ï¼‰
    logger.error(f"âŒ LlamaIndex å¯¼å…¥å¤±è´¥: {e}ï¼Œå°è¯•å¤‡ç”¨è·¯å¾„")
    try:
        from llama_index import VectorStoreIndex, StorageContext, load_index_from_storage, SimpleDirectoryReader
        from llama_index.node_parser import SemanticSplitterNodeParser
        from llama_index.embeddings import HuggingFaceEmbedding
        logger.info(f"âœ… ä½¿ç”¨å¤‡ç”¨å¯¼å…¥è·¯å¾„åŠ è½½æˆåŠŸ")
    except ImportError:
        raise ImportError("æ— æ³•å¯¼å…¥ LlamaIndex æ¨¡å—ï¼Œè¯·æ£€æŸ¥å®‰è£…")

def _get_shared_embed_model():
    """è·å–å…±äº«çš„åµŒå…¥æ¨¡å‹ï¼ˆå•ä¾‹ï¼Œé¿å…é‡å¤åŠ è½½ï¼‰"""
    global _embed_model_cache
    
    if _embed_model_cache is None:
        with _embed_lock:
            # åŒé‡æ£€æŸ¥
            if _embed_model_cache is None:
                local_model_dir = os.getenv("LOCAL_BGE_MODEL_DIR", "")
                if not local_model_dir or not Path(local_model_dir).exists():
                    raise RuntimeError(
                        f"LOCAL_BGE_MODEL_DIR æœªè®¾ç½®æˆ–ç›®å½•ä¸å­˜åœ¨: {local_model_dir}\n"
                        f"è¯·è®¾ç½®ç¯å¢ƒå˜é‡ LOCAL_BGE_MODEL_DIR æŒ‡å‘æœ¬åœ°BGEæ¨¡å‹ç›®å½•ã€‚"
                    )
                
                # å†æ¬¡ç¡®ä¿ç¦»çº¿æ¨¡å¼ï¼ˆè™½ç„¶å·²åœ¨é¡¶éƒ¨è®¾ç½®ï¼Œä½†å†æ¬¡ç¡®è®¤ï¼‰
                # æ³¨æ„ï¼šæ­¤æ—¶ç¯å¢ƒå˜é‡å·²ç»åœ¨æ¨¡å—å¯¼å…¥å‰è®¾ç½®ï¼Œè¿™é‡Œåªæ˜¯åŒé‡ä¿é™©
                os.environ['HF_HUB_OFFLINE'] = '1'
                os.environ['HF_DATASETS_OFFLINE'] = '1'
                os.environ['TRANSFORMERS_OFFLINE'] = '1'
                
                logger.info(f"ğŸ”„ åˆå§‹åŒ–å…±äº«åµŒå…¥æ¨¡å‹ï¼ˆç¦»çº¿æ¨¡å¼ï¼‰: {local_model_dir}")
                
                try:
                    _embed_model_cache = HuggingFaceEmbedding(
                        model_name=local_model_dir,
                        cache_folder=local_model_dir,
                        trust_remote_code=True,  # ä¿¡ä»»æœ¬åœ°æ¨¡å‹
                    )
                    logger.info(f"âœ… å…±äº«åµŒå…¥æ¨¡å‹å·²ç¼“å­˜")
                except Exception as e:
                    logger.error(f"åŠ è½½åµŒå…¥æ¨¡å‹å¤±è´¥: {e}")
                    raise RuntimeError(
                        f"æ— æ³•ä»æœ¬åœ°ç›®å½•åŠ è½½æ¨¡å‹: {local_model_dir}\n"
                        f"è¯·ç¡®ä¿è¯¥ç›®å½•åŒ…å«å®Œæ•´çš„ BGE æ¨¡å‹æ–‡ä»¶ã€‚\n"
                        f"é”™è¯¯è¯¦æƒ…: {str(e)}"
                    )
    
    return _embed_model_cache


class LlamaIndexRetriever:
    """LlamaIndex é«˜çº§æ£€ç´¢å¼•æ“ - ä¸º LangGraph æä¾›æ£€ç´¢æœåŠ¡"""
    
    @classmethod
    def get_instance(cls, workspace_id: str = "global") -> 'LlamaIndexRetriever':
        """è·å–å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼ŒæŒ‰ workspace_id ç¼“å­˜ï¼‰"""
        cache_key = workspace_id

        # å¿«è·¯å¾„ï¼šæ— é”è¯»å–
        instance = _retriever_cache.get(cache_key)
        if instance is not None:
            return instance

        # æ„é€ å®ä¾‹æ”¾åˆ°é”å¤–ï¼Œé¿å…æŒé”æœŸé—´æ‰§è¡Œè€—æ—¶åˆå§‹åŒ–
        new_instance = cls(workspace_id, use_cache=True)
        
        # æ”¾å…¥ç¼“å­˜ï¼ˆåŒæ£€ï¼‰
        with _retriever_lock:
            instance = _retriever_cache.get(cache_key)
            if instance is None:
                logger.info(f"ğŸ”„ åˆ›å»ºæ–° LlamaIndexRetriever å®ä¾‹: {workspace_id}")
                _retriever_cache[cache_key] = new_instance
                logger.info(f"âœ… LlamaIndexRetriever å®ä¾‹å·²ç¼“å­˜: {workspace_id}")
                return new_instance
            else:
                return instance
    
    def __init__(self, workspace_id: str = "global", use_cache: bool = False):
        """
        åˆå§‹åŒ–æ£€ç´¢å™¨
        
        Args:
            workspace_id: å·¥ä½œåŒºID
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜çš„åµŒå…¥æ¨¡å‹ï¼ˆå†…éƒ¨ä½¿ç”¨ï¼‰
        """
        self.workspace_id = workspace_id
        
        # ä»é…ç½®ä¸­è·å–å­˜å‚¨è·¯å¾„
        from app.core.config import settings
        storage_base = Path(settings.LLAMAINDEX_STORAGE_PATH)
        self.storage_dir = storage_base / workspace_id
        
        # ä½¿ç”¨å…±äº«çš„åµŒå…¥æ¨¡å‹ï¼ˆé¿å…é‡å¤åŠ è½½ï¼‰
        if use_cache:
            self.embed_model = _get_shared_embed_model()
        else:
            # å…¼å®¹æ—§ä»£ç ï¼šç›´æ¥åˆ›å»º
            self.embed_model = _get_shared_embed_model()
            logger.warning(f"âš ï¸ ç›´æ¥åˆ›å»º LlamaIndexRetriever å®ä¾‹ï¼Œå»ºè®®ä½¿ç”¨ get_instance() æ–¹æ³•")
        
        # è¯­ä¹‰åˆ†å—å™¨ï¼ˆè½»é‡ï¼Œä¸éœ€è¦ç¼“å­˜ï¼‰
        self.node_parser = SemanticSplitterNodeParser(
            buffer_size=1,
            breakpoint_percentile_threshold=95,
            embed_model=self.embed_model
        )
        
        # ç´¢å¼•ï¼ˆå»¶è¿ŸåŠ è½½ï¼Œåœ¨ç¬¬ä¸€æ¬¡ä½¿ç”¨æ—¶åŠ è½½ï¼‰
        self._index = None
        self._index_loaded = False
    
    @property
    def index(self):
        """å»¶è¿ŸåŠ è½½ç´¢å¼•"""
        if not self._index_loaded:
            self._index = self._load_or_create_index()
            self._index_loaded = True
        return self._index
    
    def _load_or_create_index(self):
        """åŠ è½½æˆ–åˆ›å»ºç´¢å¼•"""
        try:
            if self.storage_dir.exists():
                # æ£€æŸ¥æ˜¯å¦æœ‰å¿…è¦çš„ç´¢å¼•æ–‡ä»¶
                docstore_file = self.storage_dir / "docstore.json"
                if docstore_file.exists():
                    storage_context = StorageContext.from_defaults(
                        persist_dir=str(self.storage_dir)
                    )
                    index = load_index_from_storage(
                        storage_context,
                        embed_model=self.embed_model
                    )
                    logger.info(f"âœ… åŠ è½½ç´¢å¼•: {self.workspace_id}")
                    return index
                else:
                    logger.warning(f"âš ï¸ ç´¢å¼•æ–‡ä»¶ä¸å®Œæ•´ï¼Œé‡æ–°åˆ›å»º: {self.workspace_id}")
                    # åˆ›å»ºç©ºç´¢å¼•
                    return VectorStoreIndex([], embed_model=self.embed_model)
            else:
                # wikiç›®å½•ä¸å­˜åœ¨ï¼Œåˆ›å»ºç©ºç´¢å¼•
                self.storage_dir.mkdir(parents=True, exist_ok=True)
                logger.info(f"åˆ›å»ºç´¢å¼•: {self.workspace_id}")
                return VectorStoreIndex([], embed_model=self.embed_model)
        except Exception as e:
            logger.error(f"åŠ è½½ç´¢å¼•å¤±è´¥: {e}")
            # åˆ›å»ºç©ºç´¢å¼•ä½œä¸ºåå¤‡
            self.storage_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"åå¤‡æ–¹æ¡ˆï¼šåˆ›å»ºç©ºç´¢å¼•: {self.workspace_id}")
            return VectorStoreIndex([], embed_model=self.embed_model)
    
    def _init_postprocessors(self):
        """åˆå§‹åŒ–åå¤„ç†å™¨æ ˆ"""
        # é‡æ’åº
        self.reranker = SentenceTransformerRerank(
            model="BAAI/bge-reranker-v2-m3",
            top_n=10
        )
        
        # ç›¸ä¼¼åº¦è¿‡æ»¤
        self.similarity_filter = SimilarityPostprocessor(
            similarity_cutoff=0.7
        )
        
        # å¥å­çº§å‹ç¼©ï¼ˆLlamaIndex åŸç”Ÿï¼‰- å¦‚æœå¯ç”¨åˆ™å¯ç”¨
        if SentenceEmbeddingOptimizer is not None:
            self.sentence_optimizer = SentenceEmbeddingOptimizer(
                embed_model=self.embed_model,
                percentile_cutoff=0.5,
                threshold_cutoff=0.7
            )
        else:
            logger.warning("SentenceEmbeddingOptimizer ä¸å¯ç”¨ï¼Œè·³è¿‡å¥å­çº§å‹ç¼©")
            self.sentence_optimizer = None
        
        # é•¿ä¸Šä¸‹æ–‡é‡æ’åº
        self.context_reorder = LongContextReorder()
    
    def _load_or_create_index(self):
        """åŠ è½½æˆ–åˆ›å»ºç´¢å¼•"""
        try:
            if self.storage_dir.exists():
                # æ£€æŸ¥æ˜¯å¦æœ‰å¿…è¦çš„ç´¢å¼•æ–‡ä»¶
                docstore_file = self.storage_dir / "docstore.json"
                if docstore_file.exists():
                    storage_context = StorageContext.from_defaults(
                        persist_dir=str(self.storage_dir)
                    )
                    index = load_index_from_storage(
                        storage_context,
                        embed_model=self.embed_model
                    )
                    logger.info(f"âœ… åŠ è½½ç´¢å¼•: {self.workspace_id}")
                    return index
                else:
                    logger.warning(f"âš ï¸ ç´¢å¼•æ–‡ä»¶ä¸å®Œæ•´ï¼Œé‡æ–°åˆ›å»º: {self.workspace_id}")
                    # åˆ›å»ºç©ºç´¢å¼•
                    return VectorStoreIndex([], embed_model=self.embed_model)
            else:
                # wikiç›®å½•ä¸å­˜åœ¨ï¼Œåˆ›å»ºç©ºç´¢å¼•
                self.storage_dir.mkdir(parents=True, exist_ok=True)
                logger.info(f"åˆ›å»ºç´¢å¼•: {self.workspace_id}")
                return VectorStoreIndex([], embed_model=self.embed_model)
        except Exception as e:
            logger.error(f"åŠ è½½ç´¢å¼•å¤±è´¥: {e}")
            # åˆ›å»ºç©ºç´¢å¼•ä½œä¸ºåå¤‡
            self.storage_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"åå¤‡æ–¹æ¡ˆï¼šåˆ›å»ºç©ºç´¢å¼•: {self.workspace_id}")
            return VectorStoreIndex([], embed_model=self.embed_model)
    
    async def retrieve(
        self,
        query: str,
        top_k: int = 5,
        use_hybrid: bool = True,
        use_compression: bool = True
    ) -> List[Dict]:
        """
        é«˜çº§æ£€ç´¢ - è¿”å› LangGraph å¯ç”¨çš„æ ¼å¼
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            use_hybrid: æ˜¯å¦ä½¿ç”¨æ··åˆæ£€ç´¢
            use_compression: æ˜¯å¦ä½¿ç”¨å‹ç¼©
        
        Returns:
            List[Dict]: åŒ…å« content, metadata, score, node_id
        """
        try:
            # å¦‚æœç´¢å¼•ä¸ºç©ºï¼Œè¿”å›ç©ºç»“æœ
            if self.index is None or len(self.index.storage_context.docstore.docs) == 0:
                logger.info(f"ç´¢å¼•ä¸ºç©ºï¼Œè¿”å›ç©ºç»“æœ: {self.workspace_id}")
                return []
            
            # 1. æ£€ç´¢ï¼ˆä½¿ç”¨å†…ç½®ç®€åŒ–æ£€ç´¢å™¨ï¼Œé¿å…è§¦å‘ LLM ç›¸å…³æ¨¡å—ï¼‰
            retriever = self.index.as_retriever(similarity_top_k=top_k * (4 if use_hybrid else 2))
            nodes = await retriever.aretrieve(query)
            
            # 4. è½¬æ¢æ ¼å¼ï¼ˆç¡®ä¿æ‰€æœ‰æ•°æ®ç±»å‹éƒ½æ˜¯ Python åŸç”Ÿç±»å‹ï¼Œé¿å… msgpack åºåˆ—åŒ–é”™è¯¯ï¼‰
            results = []
            for node in nodes[:top_k]:
                # ç¡®ä¿ score æ˜¯ Python floatï¼Œä¸æ˜¯ numpy.float64
                score = node.score if hasattr(node, 'score') else 0.0
                if hasattr(score, 'item'):  # numpy ç±»å‹æœ‰ item() æ–¹æ³•
                    score = float(score.item())
                else:
                    score = float(score)
                
                # ç¡®ä¿ metadata ä¸­çš„å€¼ä¹Ÿéƒ½æ˜¯ Python åŸç”Ÿç±»å‹
                metadata = {}
                if hasattr(node, 'metadata') and node.metadata:
                    for key, value in node.metadata.items():
                        # è½¬æ¢ numpy ç±»å‹ä¸º Python åŸç”Ÿç±»å‹
                        if hasattr(value, 'item'):  # numpy ç±»å‹
                            metadata[key] = value.item()
                        elif hasattr(value, 'tolist'):  # numpy array
                            metadata[key] = value.tolist()
                        else:
                            metadata[key] = value
                
                results.append({
                    "content": node.get_content(),
                    "metadata": metadata,
                    "score": score,
                    "node_id": node.node_id
                })
            
            return results
            
        except Exception as e:
            logger.error(f"æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    async def add_document(self, file_path: str, metadata: Dict = None):
        """æ·»åŠ æ–‡æ¡£ï¼šå¯¹å¸¸è§æ ¼å¼å…ˆè½¬æ¢ä¸º Markdownï¼Œå†å†™å…¥ç´¢å¼•ï¼›å…¶ä»–æ ¼å¼å›é€€ SimpleDirectoryReaderã€‚"""
        try:
            import traceback
            from pathlib import Path as _Path
            from llama_index.core.schema import Document as LIDocument

            suffix = _Path(file_path).suffix.lower()
            text: Optional[str] = None
            md_meta: Dict = {}

            try:
                if suffix in [".xlsx", ".xls"]:
                    from app.services.excel_parser import parse_excel_to_markdown
                    res = parse_excel_to_markdown(file_path)
                    text = res.get("content", "")
                    md_meta.update(res.get("metadata", {}))
                    md_meta["file_type"] = "excel"
                elif suffix in [".pptx", ".ppt"]:
                    from app.services.ppt_processor import process_ppt_to_markdown
                    res = process_ppt_to_markdown(file_path)
                    text = res.get("content", "")
                    md_meta.update(res.get("metadata", {}))
                    md_meta["file_type"] = "powerpoint"
                elif suffix in [".jpg", ".jpeg", ".png", ".gif", ".bmp", ".tiff"]:
                    from app.services.file_processor import FileProcessor
                    fp = FileProcessor()
                    res = fp._process_image(file_path)
                    text = res.get("content", "")
                    md_meta.update(res.get("metadata", {}))
                    md_meta["file_type"] = "image"
                else:
                    # PDF/WORD ç­‰ä¼˜å…ˆå°è¯•ç°æœ‰å¤„ç†å™¨ï¼ˆè‹¥å·²å¢å¼ºåˆ™è¾“å‡º Markdownï¼‰
                    from app.services.file_processor import FileProcessor
                    fp = FileProcessor()
                    try:
                        res = fp.process_file(file_path)
                        text = res.get("content", "")
                        md_meta.update(res.get("metadata", {}))
                        md_meta["file_type"] = res.get("file_type", suffix.lstrip('.'))
                    except Exception:
                        text = None
            except Exception as proc_err:
                logger.warning(f"è‡ªå®šä¹‰å¤„ç†å™¨å¤±è´¥ï¼Œå›é€€é»˜è®¤è¯»å–: {proc_err}")
                text = None

            documents = []
            if text and text.strip():
                # æ„å»ºå±‚çº§ä¿¡æ¯å‰ç¼€ï¼ˆå¦‚æœæœ‰å±‚çº§ä¿¡æ¯ï¼Œæ·»åŠ åˆ°æ–‡æ¡£å†…å®¹å‰ç«¯ä»¥ç¡®ä¿å‘é‡åŒ–ï¼‰
                hierarchy_prefix = ""
                if metadata and metadata.get('hierarchy_path'):
                    hierarchy_info = []
                    archive_name = metadata.get('archive_name')
                    archive_hierarchy = metadata.get('archive_hierarchy')
                    folder_path = metadata.get('folder_path')
                    
                    if archive_name:
                        hierarchy_info.append(f"å‹ç¼©åŒ…: {archive_name}")
                    if archive_hierarchy:
                        hierarchy_info.append(f"åµŒå¥—è·¯å¾„: {archive_hierarchy}")
                    if folder_path:
                        hierarchy_info.append(f"æ–‡ä»¶å¤¹: {folder_path}")
                    
                    if hierarchy_info:
                        hierarchy_prefix = f"## æ–‡ä»¶å±‚çº§ä¿¡æ¯\n\n" + "\n".join(hierarchy_info) + "\n\n---\n\n"
                    
                    # å®Œæ•´å±‚çº§è·¯å¾„ä¹Ÿæ·»åŠ åˆ°å…ƒæ•°æ®
                    doc_meta = metadata.copy() if metadata else {}
                    doc_meta.update(md_meta)
                    doc_meta.setdefault("original_path", str(file_path))
                    doc_meta.setdefault("hierarchy_path", metadata.get('hierarchy_path'))
                else:
                    doc_meta = metadata.copy() if metadata else {}
                    doc_meta.update(md_meta)
                    doc_meta.setdefault("original_path", str(file_path))
                
                # å°†å±‚çº§ä¿¡æ¯æ·»åŠ åˆ°æ–‡æ¡£å†…å®¹å‰ç«¯
                enhanced_text = hierarchy_prefix + text if hierarchy_prefix else text
                documents = [LIDocument(text=enhanced_text, metadata=doc_meta)]
            else:
                # å›é€€ SimpleDirectoryReader
                from llama_index.core import SimpleDirectoryReader
                documents = SimpleDirectoryReader(input_files=[file_path]).load_data()
                if metadata:
                    # å¦‚æœæœ‰å±‚çº§ä¿¡æ¯ï¼Œä¹Ÿæ·»åŠ åˆ°å›é€€æ–¹æ¡ˆçš„æ–‡æ¡£ä¸­
                    if metadata.get('hierarchy_path'):
                        hierarchy_info = []
                        archive_name = metadata.get('archive_name')
                        archive_hierarchy = metadata.get('archive_hierarchy')
                        folder_path = metadata.get('folder_path')
                        
                        if archive_name:
                            hierarchy_info.append(f"å‹ç¼©åŒ…: {archive_name}")
                        if archive_hierarchy:
                            hierarchy_info.append(f"åµŒå¥—è·¯å¾„: {archive_hierarchy}")
                        if folder_path:
                            hierarchy_info.append(f"æ–‡ä»¶å¤¹: {folder_path}")
                        
                        hierarchy_prefix = ""
                        if hierarchy_info:
                            hierarchy_prefix = f"## æ–‡ä»¶å±‚çº§ä¿¡æ¯\n\n" + "\n".join(hierarchy_info) + "\n\n---\n\n"
                    
                    for d in documents:
                        d.metadata.update(metadata)
                        if metadata.get('hierarchy_path'):
                            d.metadata['hierarchy_path'] = metadata.get('hierarchy_path')
                        # å¦‚æœæœ‰å±‚çº§å‰ç¼€ï¼Œæ·»åŠ åˆ°æ–‡æ¡£å†…å®¹
                        if hierarchy_prefix:
                            d.text = hierarchy_prefix + d.text

            # æ’å…¥ä¸æŒä¹…åŒ–
            for doc in documents:
                self.index.insert(doc)
            self.index.storage_context.persist(persist_dir=str(self.storage_dir))

            if hasattr(self.index, '_docstore') and self.index._docstore:
                return len(self.index._docstore.docs)
            return len(documents)

        except Exception as e:
            logger.error(f"æ·»åŠ æ–‡æ¡£å¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            return 0

