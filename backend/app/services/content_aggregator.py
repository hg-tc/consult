"""
å†…å®¹èšåˆæœåŠ¡
ä»RAGæ£€ç´¢ç»“æœå’Œå¯¹è¯å†å²ä¸­æå–å¹¶ç»“æ„åŒ–å†…å®¹
"""

import logging
from typing import List, Dict, Any, Optional
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class ContentSection:
    """å†…å®¹ç« èŠ‚"""
    title: str
    content: str
    table_data: Optional[List[List[str]]] = None
    subsections: Optional[List['ContentSection']] = None


@dataclass
class AggregatedContent:
    """èšåˆåçš„å†…å®¹"""
    title: str
    outline: List[str]
    sections: List[ContentSection]
    references: List[Dict]
    
    def to_document_content(self):
        """è½¬æ¢ä¸ºDocumentContentæ ¼å¼ï¼ˆå…¼å®¹ç°æœ‰ç”Ÿæˆå™¨ï¼‰"""
        from app.services.document_generator_service import DocumentContent
        
        # æ„å»ºæ–‡æ¡£å†…å®¹
        content_text = f"# {self.title}\n\n"
        
        for section in self.sections:
            content_text += f"## {section.title}\n\n"
            content_text += f"{section.content}\n\n"
            
            if section.table_data:
                content_text += "| " + " | ".join(["åˆ—1", "åˆ—2", "åˆ—3"]) + " |\n"
                content_text += "| " + " | ".join(["---", "---", "---"]) + " |\n"
                for row in section.table_data:
                    content_text += "| " + " | ".join(row) + " |\n"
                content_text += "\n"
        
        return DocumentContent(
            title=self.title,
            sections=[
                {
                    'title': section.title,
                    'content': section.content,
                    'table_data': section.table_data
                }
                for section in self.sections
            ],
            metadata={
                "outline": self.outline,
                "references": self.references
            }
        )


class ContentAggregator:
    """å†…å®¹èšåˆå™¨"""
    
    def __init__(self, rag_service, llm):
        self.rag_service = rag_service
        self.llm = llm
        self.outline_prompt = """åŸºäºä»¥ä¸‹ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä¸ªç»“æ„åŒ–çš„æ–‡æ¡£å¤§çº²ï¼š

æ–‡æ¡£æ ‡é¢˜: {title}
æ–‡æ¡£ç±»å‹: {doc_type}
å†…å®¹æ¥æº: {content_source}

æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹:
{documents_content}

å¯¹è¯å†å²å…³é”®ä¿¡æ¯:
{conversation_content}

è¯·ç”Ÿæˆä¸€ä¸ªæ¸…æ™°çš„æ–‡æ¡£å¤§çº²ï¼ŒåŒ…å«ï¼š
1. ä¸»è¦ç« èŠ‚æ ‡é¢˜
2. æ¯ä¸ªç« èŠ‚çš„ç®€è¦æè¿°
3. å»ºè®®çš„å†…å®¹ç»„ç»‡æ–¹å¼

ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
    "outline": ["ç« èŠ‚1", "ç« èŠ‚2", "ç« èŠ‚3"],
    "sections": [
        {{
            "title": "ç« èŠ‚1",
            "description": "ç« èŠ‚æè¿°",
            "content_type": "text/table/mixed"
        }}
    ]
}}"""

        self.content_prompt = """åŸºäºä»¥ä¸‹å¤§çº²å’Œå†…å®¹ï¼Œç”Ÿæˆè¯¦ç»†çš„æ–‡æ¡£å†…å®¹ï¼š

æ–‡æ¡£æ ‡é¢˜: {title}
æ–‡æ¡£ç±»å‹: {doc_type}
ç« èŠ‚: {section_title}

æ£€ç´¢åˆ°çš„ç›¸å…³å†…å®¹:
{relevant_content}

å¯¹è¯å†å²ä¿¡æ¯:
{conversation_content}

è¯·ç”Ÿæˆè¯¥ç« èŠ‚çš„è¯¦ç»†å†…å®¹ï¼Œè¦æ±‚ï¼š
1. å†…å®¹å‡†ç¡®ã€å®Œæ•´
2. é€»è¾‘æ¸…æ™°
3. é€‚åˆ{doc_type}æ ¼å¼
4. å¦‚æœæœ‰æ•°æ®ï¼Œå¯ä»¥ç»„ç»‡æˆè¡¨æ ¼å½¢å¼

è¿”å›æ ¼å¼ï¼š
{{
    "content": "è¯¦ç»†å†…å®¹æ–‡æœ¬",
    "table_data": [["è¡Œ1åˆ—1", "è¡Œ1åˆ—2"], ["è¡Œ2åˆ—1", "è¡Œ2åˆ—2"]],
    "subsections": ["å­ç« èŠ‚1", "å­ç« èŠ‚2"]
}}"""

    async def aggregate_content(
        self, 
        intent,
        workspace_id: str,
        conversation_history: List[Dict],
        search_query: Optional[str] = None
    ) -> AggregatedContent:
        """
        èšåˆå†…å®¹ï¼ˆä¼˜åŒ–ç‰ˆï¼šå‡å°‘LLMè°ƒç”¨æ¬¡æ•°ï¼‰
        
        Args:
            intent: æ„å›¾è¯†åˆ«ç»“æœ
            workspace_id: å·¥ä½œåŒºID
            conversation_history: å¯¹è¯å†å²
            search_query: æœç´¢æŸ¥è¯¢
            
        Returns:
            AggregatedContent: èšåˆåçš„å†…å®¹
        """
        try:
            logger.info(f"å¼€å§‹å†…å®¹èšåˆ: {intent.title}")
            start_time = time.time()
            
            # 1. åŸºäºæ„å›¾æ£€ç´¢ç›¸å…³æ–‡æ¡£
            documents_content = ""
            references = []
            
            if intent.content_source in ['documents', 'mixed']:
                docs = await self._retrieve_documents(
                    workspace_id, 
                    search_query or intent.inferred_query or intent.title
                )
                
                if docs:
                    # å¢å¼ºæ–‡æ¡£å†…å®¹æå–
                    for i, doc in enumerate(docs):
                        doc_name = doc.get('document_name', f'æ–‡æ¡£{i+1}')
                        doc_content = doc.get('content', '') or doc.get('content_preview', '')
                        similarity = doc.get('similarity', 0)
                        
                        documents_content += f"ğŸ“„ æ–‡æ¡£{i+1}: {doc_name} (ç›¸ä¼¼åº¦: {similarity:.2f})\n"
                        documents_content += f"ğŸ“ å†…å®¹: {doc_content}\n"
                        
                        # æ·»åŠ å…ƒæ•°æ®ä¿¡æ¯
                        if doc.get('metadata'):
                            metadata = doc['metadata']
                            if metadata.get('page_number'):
                                documents_content += f"ğŸ“– é¡µç : {metadata['page_number']}\n"
                            if metadata.get('file_type'):
                                documents_content += f"ğŸ“ ç±»å‹: {metadata['file_type']}\n"
                        
                        documents_content += "\n" + "="*50 + "\n\n"
                    
                    references = docs
                    logger.info(f"æˆåŠŸæ£€ç´¢åˆ° {len(docs)} ä¸ªç›¸å…³æ–‡æ¡£ç”¨äºå†…å®¹ç”Ÿæˆ")
            
            # 2. æå–å¯¹è¯å†å²å…³é”®ä¿¡æ¯
            conversation_content = ""
            if intent.content_source in ['conversation', 'mixed']:
                conversation_content = await self._extract_key_points(conversation_history)
            
            # 3. ä¸¤é˜¶æ®µç”Ÿæˆï¼šå…ˆç”Ÿæˆè¯¦ç»†å¤§çº²ï¼Œå†ç”Ÿæˆå…·ä½“å†…å®¹
            logger.info("å¼€å§‹ä¸¤é˜¶æ®µæ–‡æ¡£ç”Ÿæˆ...")
            
            # é˜¶æ®µ1ï¼šç”Ÿæˆè¯¦ç»†å¤§çº²
            detailed_outline = await self._generate_detailed_outline(
                intent, documents_content, conversation_content
            )
            logger.info(f"å¤§çº²ç”Ÿæˆå®Œæˆï¼ŒåŒ…å« {len(detailed_outline.get('sections', []))} ä¸ªç« èŠ‚")
            
            # é˜¶æ®µ2ï¼šæ ¹æ®å¤§çº²ç”Ÿæˆå…·ä½“å†…å®¹
            sections = await self._generate_content_from_outline(
                intent, detailed_outline, documents_content, conversation_content
            )
            logger.info(f"å†…å®¹ç”Ÿæˆå®Œæˆï¼Œå…± {len(sections)} ä¸ªç« èŠ‚")
            
            return AggregatedContent(
                title=intent.title,
                outline=detailed_outline.get('outline', []),
                sections=sections,
                references=references
            )
            
        except Exception as e:
            logger.error(f"å†…å®¹èšåˆå¤±è´¥: {e}")
            # è¿”å›å¢å¼ºçš„åŸºç¡€å†…å®¹
            fallback_content = self._generate_fallback_content(intent, documents_content, conversation_content)
            return AggregatedContent(
                title=intent.title,
                outline=fallback_content.get('outline', [intent.title]),
                sections=fallback_content.get('sections', [ContentSection(
                    title=intent.title,
                    content=f"åŸºäºç”¨æˆ·è¯·æ±‚ç”Ÿæˆçš„å†…å®¹ï¼š{intent.inferred_query or intent.title}"
                )]),
                references=[]
            )
    
    async def _generate_detailed_outline(self, intent, documents_content: str, conversation_content: str) -> Dict:
        """é˜¶æ®µ1ï¼šç”Ÿæˆè¯¦ç»†å¤§çº²"""
        try:
            doc_type_guidance = self._get_document_type_guidance(intent.doc_type.value)
            structure_guidance = self._get_content_structure_guidance(intent.doc_type.value)
            
            outline_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ–‡æ¡£ç»“æ„è®¾è®¡å¸ˆï¼Œè¯·ä¸ºä»¥ä¸‹éœ€æ±‚è®¾è®¡ä¸€ä¸ªè¯¦ç»†çš„å¤§çº²ï¼š

ğŸ“‹ **æ–‡æ¡£åŸºæœ¬ä¿¡æ¯**
- æ ‡é¢˜: {intent.title}
- ç±»å‹: {intent.doc_type.value.upper()}
- å†…å®¹æ¥æº: {intent.content_source}

{doc_type_guidance}

ğŸ“š **å‚è€ƒå†…å®¹** (è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯è®¾è®¡å¤§çº²):
{documents_content[:3000] if documents_content else 'æ— ç›¸å…³å‚è€ƒå†…å®¹'}

ğŸ’¬ **ç”¨æˆ·éœ€æ±‚ä¿¡æ¯**:
{conversation_content[:1000] if conversation_content else 'æ— å¯¹è¯å†å²'}

{structure_guidance}

ğŸ¯ **å¤§çº²è®¾è®¡è¦æ±‚**:
1. æ ¹æ®ç”¨æˆ·å…·ä½“éœ€æ±‚å®šåˆ¶ç« èŠ‚ç»“æ„
2. æ¯ä¸ªç« èŠ‚éƒ½è¦æœ‰æ˜ç¡®çš„ç›®çš„å’Œä»·å€¼
3. ç« èŠ‚ä¹‹é—´è¦æœ‰é€»è¾‘é€’è¿›å…³ç³»
4. åŒ…å«å…·ä½“çš„å­ç« èŠ‚å’Œè¦ç‚¹
5. é€‚åˆ{intent.doc_type.value}æ–‡æ¡£çš„ç‰¹ç‚¹

ğŸ“ **è¿”å›æ ¼å¼** (ä¸¥æ ¼JSON):
{{
    "outline": ["ç« èŠ‚1", "ç« èŠ‚2", "ç« èŠ‚3", "ç« èŠ‚4"],
    "sections": [
        {{
            "title": "ç« èŠ‚1",
            "description": "ç« èŠ‚çš„è¯¦ç»†æè¿°å’Œç›®çš„",
            "subsections": ["å­ç« èŠ‚1.1", "å­ç« èŠ‚1.2", "å­ç« èŠ‚1.3"],
            "key_points": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"],
            "content_type": "text/table/mixed",
            "estimated_length": "300-500å­—"
        }},
        {{
            "title": "ç« èŠ‚2",
            "description": "ç« èŠ‚çš„è¯¦ç»†æè¿°å’Œç›®çš„",
            "subsections": ["å­ç« èŠ‚2.1", "å­ç« èŠ‚2.2"],
            "key_points": ["è¦ç‚¹1", "è¦ç‚¹2"],
            "content_type": "text",
            "estimated_length": "400-600å­—"
        }}
    ]
}}

è¯·ç¡®ä¿å¤§çº²ç»“æ„åˆç†ã€å†…å®¹å……å®ã€ç¬¦åˆç”¨æˆ·éœ€æ±‚ã€‚"""
            
            response = await self.llm.ainvoke(outline_prompt)
            
            # å¤„ç†å“åº”
            if hasattr(response, 'content'):
                response_text = response.content
            else:
                response_text = str(response)
            
            # è§£æJSONå“åº”
            import json
            # æ¸…ç†å“åº”å†…å®¹
            if response_text.startswith('```json'):
                response_text = response_text[7:]
            if response_text.endswith('```'):
                response_text = response_text[:-3]
            response_text = response_text.strip()
            
            json_start = response_text.find('{')
            json_end = response_text.rfind('}') + 1
            if json_start != -1 and json_end != -1:
                json_str = response_text[json_start:json_end]
                return json.loads(json_str)
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆè¯¦ç»†å¤§çº²å¤±è´¥: {e}")
        
        # å›é€€åˆ°åŸºç¡€å¤§çº²
        return {
            "outline": ["æ¦‚è¿°", "è¯¦ç»†åˆ†æ", "ç»“è®ºä¸å»ºè®®"],
            "sections": [
                {
                    "title": "æ¦‚è¿°",
                    "description": "æ–‡æ¡£æ¦‚è¿°å’ŒèƒŒæ™¯ä»‹ç»",
                    "subsections": ["èƒŒæ™¯", "ç›®æ ‡", "èŒƒå›´"],
                    "key_points": ["é—®é¢˜èƒŒæ™¯", "è§£å†³ç›®æ ‡"],
                    "content_type": "text",
                    "estimated_length": "300-400å­—"
                },
                {
                    "title": "è¯¦ç»†åˆ†æ",
                    "description": "æ·±å…¥åˆ†æå’Œå…·ä½“å†…å®¹",
                    "subsections": ["ç°çŠ¶åˆ†æ", "é—®é¢˜è¯†åˆ«", "è§£å†³æ–¹æ¡ˆ"],
                    "key_points": ["æ•°æ®åˆ†æ", "é—®é¢˜åˆ†æ", "æ–¹æ¡ˆè®¾è®¡"],
                    "content_type": "mixed",
                    "estimated_length": "500-700å­—"
                },
                {
                    "title": "ç»“è®ºä¸å»ºè®®",
                    "description": "æ€»ç»“å’Œå»ºè®®æªæ–½",
                    "subsections": ["ä¸»è¦å‘ç°", "å»ºè®®æªæ–½", "åç»­è¡ŒåŠ¨"],
                    "key_points": ["å…³é”®ç»“è®º", "å®æ–½å»ºè®®"],
                    "content_type": "text",
                    "estimated_length": "300-500å­—"
                }
            ]
        }
    
    async def _generate_content_from_outline(self, intent, outline: Dict, documents_content: str, conversation_content: str) -> List[ContentSection]:
        """é˜¶æ®µ2ï¼šæ ¹æ®å¤§çº²ç”Ÿæˆå…·ä½“å†…å®¹"""
        sections = []
        
        for section_info in outline.get('sections', []):
            try:
                logger.info(f"æ­£åœ¨ç”Ÿæˆç« èŠ‚: {section_info.get('title', 'æœªçŸ¥')}")
                
                # ä¸ºæ¯ä¸ªç« èŠ‚ç”Ÿæˆè¯¦ç»†å†…å®¹
                content = await self._generate_section_content(
                    intent, section_info, documents_content, conversation_content
                )
                sections.append(content)
                
            except Exception as e:
                logger.error(f"ç”Ÿæˆç« èŠ‚å†…å®¹å¤±è´¥: {e}")
                # æ·»åŠ åŸºç¡€ç« èŠ‚å†…å®¹
                sections.append(ContentSection(
                    title=section_info.get('title', 'æœªçŸ¥ç« èŠ‚'),
                    content=f"# {section_info.get('title', 'æœªçŸ¥ç« èŠ‚')}\n\n{section_info.get('description', 'ç« èŠ‚å†…å®¹')}"
                ))
        
        return sections
    
    async def _generate_section_content(self, intent, section_info: Dict, documents_content: str, conversation_content: str) -> ContentSection:
        """ä¸ºå•ä¸ªç« èŠ‚ç”Ÿæˆè¯¦ç»†å†…å®¹"""
        try:
            section_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å†…å®¹æ’°å†™ä¸“å®¶ï¼Œè¯·æ ¹æ®ä»¥ä¸‹å¤§çº²ä¿¡æ¯ç”Ÿæˆè¯¦ç»†çš„ç« èŠ‚å†…å®¹ï¼š

ğŸ“‹ **ç« èŠ‚ä¿¡æ¯**
- æ ‡é¢˜: {section_info.get('title', '')}
- æè¿°: {section_info.get('description', '')}
- å­ç« èŠ‚: {', '.join(section_info.get('subsections', []))}
- å…³é”®è¦ç‚¹: {', '.join(section_info.get('key_points', []))}
- å†…å®¹ç±»å‹: {section_info.get('content_type', 'text')}
- é¢„è®¡é•¿åº¦: {section_info.get('estimated_length', '300-500å­—')}

ğŸ“š **å‚è€ƒå†…å®¹** (è¯·å……åˆ†åˆ©ç”¨):
{documents_content[:2000] if documents_content else 'æ— ç›¸å…³å‚è€ƒå†…å®¹'}

ğŸ’¬ **ç”¨æˆ·éœ€æ±‚ä¿¡æ¯**:
{conversation_content[:800] if conversation_content else 'æ— å¯¹è¯å†å²'}

ğŸ¯ **å†…å®¹è¦æ±‚**:
1. ä¸¥æ ¼æŒ‰ç…§ç« èŠ‚å¤§çº²ç»“æ„ç»„ç»‡å†…å®¹
2. åŒ…å«æ‰€æœ‰å…³é”®è¦ç‚¹å’Œå­ç« èŠ‚
3. å†…å®¹ä¸“ä¸šã€å‡†ç¡®ã€æœ‰æ·±åº¦
4. è¯­è¨€è¡¨è¾¾æ¸…æ™°ã€é€»è¾‘æ€§å¼º
5. å……åˆ†åˆ©ç”¨å‚è€ƒå†…å®¹å’Œç”¨æˆ·éœ€æ±‚
6. è¾¾åˆ°é¢„è®¡é•¿åº¦è¦æ±‚
7. é€‚åˆ{intent.doc_type.value}æ–‡æ¡£æ ¼å¼

ğŸ“ **è¿”å›æ ¼å¼** (ä¸¥æ ¼JSON):
{{
    "content": "è¯¦ç»†çš„ç« èŠ‚å†…å®¹ï¼ŒåŒ…å«æ‰€æœ‰å­ç« èŠ‚å’Œè¦ç‚¹...",
    "table_data": [["è¡¨å¤´1", "è¡¨å¤´2"], ["æ•°æ®1", "æ•°æ®2"]],
    "subsections": ["å­ç« èŠ‚1", "å­ç« èŠ‚2"]
}}

è¯·ç¡®ä¿å†…å®¹å……å®ã€ä¸“ä¸šã€æœ‰ä»·å€¼ã€‚"""
            
            response = await self.llm.ainvoke(section_prompt)
            
            # å¤„ç†å“åº”
            if hasattr(response, 'content'):
                response_text = response.content
            else:
                response_text = str(response)
            
            logger.info(f"LLMå“åº” (ç« èŠ‚: {section_info.get('title', '')}): {response_text[:200]}...")
            
            # è§£æJSONå“åº”
            import json
            # æ¸…ç†å“åº”å†…å®¹
            if response_text.startswith('```json'):
                response_text = response_text[7:]
            if response_text.endswith('```'):
                response_text = response_text[:-3]
            response_text = response_text.strip()
            
            json_start = response_text.find('{')
            json_end = response_text.rfind('}') + 1
            if json_start != -1 and json_end != -1:
                json_str = response_text[json_start:json_end]
                try:
                    content_data = json.loads(json_str)
                    logger.info(f"æˆåŠŸè§£æJSONï¼Œå†…å®¹é•¿åº¦: {len(content_data.get('content', ''))}")
                    
                    return ContentSection(
                        title=section_info.get('title', ''),
                        content=content_data.get('content', ''),
                        table_data=content_data.get('table_data'),
                        subsections=content_data.get('subsections')
                    )
                except json.JSONDecodeError as e:
                    logger.error(f"JSONè§£æå¤±è´¥: {e}, JSONå†…å®¹: {json_str[:100]}...")
            else:
                logger.error(f"æœªæ‰¾åˆ°JSONæ ¼å¼ï¼Œå“åº”å†…å®¹: {response_text[:200]}...")
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆç« èŠ‚å†…å®¹å¤±è´¥: {e}")
        
        # å›é€€åˆ°åŸºç¡€å†…å®¹
        fallback_content = f"""# {section_info.get('title', 'æœªçŸ¥ç« èŠ‚')}

## æ¦‚è¿°
{section_info.get('description', 'ç« èŠ‚æè¿°')}

## ä¸»è¦å†…å®¹
{documents_content[:300] if documents_content else 'æš‚æ— ç›¸å…³å‚è€ƒå†…å®¹'}

## å…³é”®è¦ç‚¹
{chr(10).join([f"- {point}" for point in section_info.get('key_points', [])])}

## æ€»ç»“
åŸºäºä»¥ä¸Šåˆ†æï¼Œæˆ‘ä»¬å¯ä»¥å¾—å‡ºç›¸å…³ç»“è®ºå’Œå»ºè®®ã€‚"""
        
        return ContentSection(
            title=section_info.get('title', 'æœªçŸ¥ç« èŠ‚'),
            content=fallback_content,
            table_data=None,
            subsections=section_info.get('subsections', [])
        )

    async def _generate_complete_document(self, intent, documents_content: str, conversation_content: str) -> Dict:
        """ä½¿ç”¨å¢å¼ºçš„LLMè°ƒç”¨ç”Ÿæˆé«˜è´¨é‡å®Œæ•´æ–‡æ¡£"""
        try:
            # æ„å»ºå¢å¼ºçš„æç¤ºè¯
            doc_type_guidance = self._get_document_type_guidance(intent.doc_type.value)
            content_structure = self._get_content_structure_guidance(intent.doc_type.value)
            
            prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ–‡æ¡£æ’°å†™ä¸“å®¶ï¼Œè¯·ä¸ºä»¥ä¸‹è¯·æ±‚ç”Ÿæˆé«˜è´¨é‡çš„æ–‡æ¡£å†…å®¹ï¼š

ğŸ“‹ **æ–‡æ¡£åŸºæœ¬ä¿¡æ¯**
- æ ‡é¢˜: {intent.title}
- ç±»å‹: {intent.doc_type.value.upper()}
- å†…å®¹æ¥æº: {intent.content_source}

{doc_type_guidance}

ğŸ“š **å‚è€ƒå†…å®¹** (è¯·å……åˆ†åˆ©ç”¨ä»¥ä¸‹ä¿¡æ¯):
{documents_content[:4000] if documents_content else 'æ— ç›¸å…³å‚è€ƒå†…å®¹'}

ğŸ’¬ **å¯¹è¯å†å²å…³é”®ä¿¡æ¯**:
{conversation_content[:1500] if conversation_content else 'æ— å¯¹è¯å†å²'}

{content_structure}

ğŸ¯ **è´¨é‡è¦æ±‚**:
1. å†…å®¹å¿…é¡»å‡†ç¡®ã€ä¸“ä¸šã€æœ‰æ·±åº¦
2. é€»è¾‘æ¸…æ™°ï¼Œç»“æ„åˆç†
3. æ¯ä¸ªç« èŠ‚è‡³å°‘300-500å­—
4. åŒ…å«å…·ä½“çš„æ•°æ®ã€æ¡ˆä¾‹æˆ–åˆ†æ
5. è¯­è¨€è¡¨è¾¾ä¸“ä¸šä¸”æ˜“æ‡‚
6. ç¬¦åˆ{intent.doc_type.value}æ–‡æ¡£çš„æ ‡å‡†æ ¼å¼

ğŸ“ **è¿”å›æ ¼å¼** (ä¸¥æ ¼JSON):
{{
    "outline": ["ç« èŠ‚1", "ç« èŠ‚2", "ç« èŠ‚3", "ç« èŠ‚4"],
    "sections": [
        {{
            "title": "ç« èŠ‚1",
            "content": "è¯¦ç»†çš„ç« èŠ‚å†…å®¹ï¼ŒåŒ…å«å…·ä½“ä¿¡æ¯ã€æ•°æ®ã€åˆ†æç­‰ï¼Œè‡³å°‘300å­—..."
        }},
        {{
            "title": "ç« èŠ‚2", 
            "content": "è¯¦ç»†çš„ç« èŠ‚å†…å®¹ï¼ŒåŒ…å«å…·ä½“ä¿¡æ¯ã€æ•°æ®ã€åˆ†æç­‰ï¼Œè‡³å°‘300å­—..."
        }}
    ]
}}

è¯·ç¡®ä¿ç”Ÿæˆçš„å†…å®¹ä¸“ä¸šã€è¯¦ç»†ã€æœ‰ä»·å€¼ï¼Œé¿å…ç©ºæ´çš„è¡¨è¿°ã€‚"""
            
            response = await self.llm.ainvoke(prompt)
            
            # å¤„ç†å“åº”
            if hasattr(response, 'content'):
                response_text = response.content
            else:
                response_text = str(response)
            
            # è§£æJSONå“åº”
            import json
            # æ¸…ç†å“åº”å†…å®¹
            if response_text.startswith('```json'):
                response_text = response_text[7:]
            if response_text.endswith('```'):
                response_text = response_text[:-3]
            response_text = response_text.strip()
            
            json_start = response_text.find('{')
            json_end = response_text.rfind('}') + 1
            if json_start != -1 and json_end != -1:
                json_str = response_text[json_start:json_end]
                return json.loads(json_str)
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå®Œæ•´æ–‡æ¡£å¤±è´¥: {e}")
        
            # å›é€€åˆ°ç®€å•å†…å®¹
            return {
                "outline": [intent.title],
                "sections": [{
                    "title": intent.title,
                    "content": f"åŸºäºç”¨æˆ·è¯·æ±‚ç”Ÿæˆçš„å†…å®¹ï¼š{intent.inferred_query or intent.title}\n\n{documents_content[:500] if documents_content else ''}"
                }]
            }
    
    def _get_document_type_guidance(self, doc_type: str) -> str:
        """è·å–æ–‡æ¡£ç±»å‹æŒ‡å¯¼"""
        guidance_map = {
            "word": """
ğŸ“„ **Wordæ–‡æ¡£æŒ‡å¯¼**:
- é€‚åˆæŠ¥å‘Šã€åˆ†ææ–‡æ¡£ã€è¯´æ˜æ–‡æ¡£ç­‰
- éœ€è¦æ¸…æ™°çš„ç« èŠ‚ç»“æ„å’Œå±‚æ¬¡
- å¯ä»¥åŒ…å«è¡¨æ ¼ã€å›¾è¡¨è¯´æ˜
- è¯­è¨€æ­£å¼ã€ä¸“ä¸š
- å»ºè®®åŒ…å«ï¼šæ¦‚è¿°ã€è¯¦ç»†åˆ†æã€ç»“è®ºã€å»ºè®®ç­‰ç« èŠ‚""",
            
            "excel": """
ğŸ“Š **Excelæ–‡æ¡£æŒ‡å¯¼**:
- é€‚åˆæ•°æ®æŠ¥å‘Šã€è´¢åŠ¡åˆ†æã€ç»Ÿè®¡æŠ¥å‘Šç­‰
- é‡ç‚¹çªå‡ºæ•°æ®åˆ†æå’Œè¡¨æ ¼å±•ç¤º
- æ¯ä¸ªç« èŠ‚å¯ä»¥å¯¹åº”ä¸€ä¸ªå·¥ä½œè¡¨
- åŒ…å«æ•°æ®è§£è¯»å’Œåˆ†æ
- å»ºè®®åŒ…å«ï¼šæ•°æ®æ¦‚è§ˆã€è¯¦ç»†åˆ†æã€è¶‹åŠ¿åˆ†æã€ç»“è®ºç­‰ç« èŠ‚""",
            
            "ppt": """
ğŸ¯ **PPTæ–‡æ¡£æŒ‡å¯¼**:
- é€‚åˆæ¼”ç¤ºæ–‡ç¨¿ã€æ±‡æŠ¥ææ–™ã€åŸ¹è®­èµ„æ–™ç­‰
- å†…å®¹ç®€æ´æ˜äº†ï¼Œé‡ç‚¹çªå‡º
- æ¯é¡µå†…å®¹ä¸å®œè¿‡å¤š
- è¯­è¨€ç”ŸåŠ¨ï¼Œé€‚åˆå£å¤´è¡¨è¾¾
- å»ºè®®åŒ…å«ï¼šæ¦‚è¿°ã€æ ¸å¿ƒå†…å®¹ã€æ¡ˆä¾‹åˆ†æã€æ€»ç»“ç­‰ç« èŠ‚""",
            
            "pdf": """
ğŸ“‹ **PDFæ–‡æ¡£æŒ‡å¯¼**:
- é€‚åˆæ­£å¼æŠ¥å‘Šã€æŠ€æœ¯æ–‡æ¡£ã€ç™½çš®ä¹¦ç­‰
- å†…å®¹å®Œæ•´ã€ç»“æ„ä¸¥è°¨
- å¯ä»¥åŒ…å«å›¾è¡¨ã€é™„å½•
- è¯­è¨€ä¸“ä¸šã€å‡†ç¡®
- å»ºè®®åŒ…å«ï¼šæ‘˜è¦ã€æ­£æ–‡ã€ç»“è®ºã€å‚è€ƒæ–‡çŒ®ç­‰ç« èŠ‚"""
        }
        return guidance_map.get(doc_type, guidance_map["word"])
    
    def _get_content_structure_guidance(self, doc_type: str) -> str:
        """è·å–å†…å®¹ç»“æ„æŒ‡å¯¼"""
        structure_map = {
            "word": """
ğŸ—ï¸ **Wordæ–‡æ¡£ç»“æ„å»ºè®®**:
1. **æ¦‚è¿°/å¼•è¨€** - ä»‹ç»èƒŒæ™¯ã€ç›®çš„ã€èŒƒå›´
2. **ç°çŠ¶åˆ†æ** - å½“å‰æƒ…å†µã€é—®é¢˜è¯†åˆ«
3. **è¯¦ç»†åˆ†æ** - æ·±å…¥åˆ†æã€æ•°æ®æ”¯æ’‘
4. **è§£å†³æ–¹æ¡ˆ** - å»ºè®®æªæ–½ã€å®æ–½è®¡åˆ’
5. **ç»“è®ºä¸å»ºè®®** - æ€»ç»“è¦ç‚¹ã€åç»­è¡ŒåŠ¨""",
            
            "excel": """
ğŸ“ˆ **Excelæ–‡æ¡£ç»“æ„å»ºè®®**:
1. **æ•°æ®æ¦‚è§ˆ** - æ€»ä½“æ•°æ®ã€å…³é”®æŒ‡æ ‡
2. **åˆ†ç±»åˆ†æ** - æŒ‰ç±»åˆ«ã€æ—¶é—´ã€åœ°åŒºç­‰åˆ†æ
3. **è¶‹åŠ¿åˆ†æ** - å˜åŒ–è¶‹åŠ¿ã€é¢„æµ‹åˆ†æ
4. **å¯¹æ¯”åˆ†æ** - åŒæœŸå¯¹æ¯”ã€ç›®æ ‡å¯¹æ¯”
5. **ç»“è®ºä¸å»ºè®®** - æ•°æ®æ´å¯Ÿã€è¡ŒåŠ¨å»ºè®®""",
            
            "ppt": """
ğŸ¨ **PPTæ–‡æ¡£ç»“æ„å»ºè®®**:
1. **å¼€åœº/èƒŒæ™¯** - é—®é¢˜èƒŒæ™¯ã€é‡è¦æ€§
2. **æ ¸å¿ƒå†…å®¹** - ä¸»è¦è§‚ç‚¹ã€å…³é”®ä¿¡æ¯
3. **æ¡ˆä¾‹åˆ†æ** - å…·ä½“æ¡ˆä¾‹ã€å®ä¾‹è¯´æ˜
4. **è¡ŒåŠ¨æ–¹æ¡ˆ** - å…·ä½“æªæ–½ã€å®æ–½æ­¥éª¤
5. **æ€»ç»“/å±•æœ›** - è¦ç‚¹å›é¡¾ã€æœªæ¥è§„åˆ’""",
            
            "pdf": """
ğŸ“‘ **PDFæ–‡æ¡£ç»“æ„å»ºè®®**:
1. **æ‰§è¡Œæ‘˜è¦** - æ ¸å¿ƒè¦ç‚¹ã€ä¸»è¦ç»“è®º
2. **èƒŒæ™¯ä»‹ç»** - é—®é¢˜èƒŒæ™¯ã€ç ”ç©¶èŒƒå›´
3. **æ–¹æ³•è®º** - åˆ†ææ–¹æ³•ã€æ•°æ®æ¥æº
4. **è¯¦ç»†åˆ†æ** - æ·±å…¥åˆ†æã€è¯æ®æ”¯æ’‘
5. **ç»“è®ºä¸å»ºè®®** - ç ”ç©¶å‘ç°ã€æ”¿ç­–å»ºè®®"""
        }
        return structure_map.get(doc_type, structure_map["word"])
    
    def _generate_fallback_content(self, intent, documents_content: str, conversation_content: str) -> Dict:
        """ç”Ÿæˆå›é€€å†…å®¹"""
        try:
            # åŸºäºå¯ç”¨å†…å®¹ç”ŸæˆåŸºç¡€ç»“æ„
            outline = [
                "æ¦‚è¿°",
                "è¯¦ç»†åˆ†æ", 
                "ç»“è®ºä¸å»ºè®®"
            ]
            
            sections = []
            
            # æ¦‚è¿°ç« èŠ‚
            overview_content = f"""
# {intent.title}

## æ¦‚è¿°
æœ¬æ–‡æ¡£åŸºäºç”¨æˆ·éœ€æ±‚"{intent.inferred_query or intent.title}"ç”Ÿæˆã€‚

## èƒŒæ™¯
{document_content[:300] if (doc_content := documents_content) else 'æš‚æ— ç›¸å…³èƒŒæ™¯ä¿¡æ¯'}

## ç›®æ ‡
æ ¹æ®ç”¨æˆ·éœ€æ±‚ï¼Œæœ¬æ–‡æ¡£æ—¨åœ¨æä¾›ç›¸å…³åˆ†æå’Œå»ºè®®ã€‚
"""
            
            sections.append(ContentSection(
                title="æ¦‚è¿°",
                content=overview_content.strip()
            ))
            
            # è¯¦ç»†åˆ†æç« èŠ‚
            analysis_content = f"""
## è¯¦ç»†åˆ†æ

### ç›¸å…³æ–‡æ¡£ä¿¡æ¯
{document_content[:500] if (doc_content := documents_content) else 'æš‚æ— ç›¸å…³æ–‡æ¡£ä¿¡æ¯'}

### å¯¹è¯å†å²è¦ç‚¹
{conversation_content[:300] if conversation_content else 'æš‚æ— å¯¹è¯å†å²'}

### åˆ†æè¦ç‚¹
åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œæˆ‘ä»¬å¯ä»¥å¾—å‡ºä»¥ä¸‹åˆ†æè¦ç‚¹ï¼š
1. ç”¨æˆ·å…³æ³¨çš„æ ¸å¿ƒé—®é¢˜
2. ç›¸å…³æ•°æ®å’Œäº‹å®
3. å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ
"""
            
            sections.append(ContentSection(
                title="è¯¦ç»†åˆ†æ",
                content=analysis_content.strip()
            ))
            
            # ç»“è®ºç« èŠ‚
            conclusion_content = f"""
## ç»“è®ºä¸å»ºè®®

### ä¸»è¦å‘ç°
åŸºäºåˆ†æï¼Œæˆ‘ä»¬å‘ç°ä»¥ä¸‹å…³é”®ç‚¹ï¼š
- ç”¨æˆ·éœ€æ±‚æ˜ç¡®
- ç›¸å…³èµ„æºå¯ç”¨
- éœ€è¦è¿›ä¸€æ­¥è¡ŒåŠ¨

### å»ºè®®æªæ–½
1. ç»§ç»­æ·±å…¥åˆ†æç›¸å…³æ•°æ®
2. åˆ¶å®šå…·ä½“çš„å®æ–½è®¡åˆ’
3. å®šæœŸè¯„ä¼°å’Œè°ƒæ•´

### åç»­è¡ŒåŠ¨
å»ºè®®ç”¨æˆ·æ ¹æ®å…·ä½“éœ€æ±‚è¿›ä¸€æ­¥å®Œå–„ç›¸å…³å†…å®¹ã€‚
"""
            
            sections.append(ContentSection(
                title="ç»“è®ºä¸å»ºè®®",
                content=conclusion_content.strip()
            ))
            
            return {
                "outline": outline,
                "sections": sections
            }
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå›é€€å†…å®¹å¤±è´¥: {e}")
            return {
                "outline": [intent.title],
                "sections": [ContentSection(
                    title=intent.title,
                    content=f"åŸºäºç”¨æˆ·è¯·æ±‚ç”Ÿæˆçš„å†…å®¹ï¼š{intent.inferred_query or intent.title}"
                )]
            }
    
    async def _enhance_content_quality(
        self, 
        intent, 
        initial_content: Dict, 
        documents_content: str, 
        conversation_content: str
    ) -> Dict:
        """å¢å¼ºå†…å®¹è´¨é‡ï¼ˆç¬¬äºŒè½®ï¼‰"""
        try:
            logger.info("å¼€å§‹ç¬¬äºŒè½®å†…å®¹è´¨é‡å¢å¼º...")
            
            # åˆ†æåˆå§‹å†…å®¹çš„è´¨é‡
            quality_analysis = await self._analyze_content_quality(initial_content)
            
            # åŸºäºè´¨é‡åˆ†æè¿›è¡Œå†…å®¹æ‰©å±•
            enhanced_sections = []
            for section in initial_content.get('sections', []):
                enhanced_section = await self._expand_section_content(
                    section, quality_analysis, documents_content, conversation_content
                )
                enhanced_sections.append(enhanced_section)
            
            return {
                "outline": initial_content.get('outline', []),
                "sections": enhanced_sections,
                "quality_analysis": quality_analysis
            }
            
        except Exception as e:
            logger.error(f"å†…å®¹è´¨é‡å¢å¼ºå¤±è´¥: {e}")
            return initial_content
    
    async def _analyze_content_quality(self, content: Dict) -> Dict:
        """åˆ†æå†…å®¹è´¨é‡"""
        try:
            sections = content.get('sections', [])
            
            quality_metrics = {
                "total_sections": len(sections),
                "avg_content_length": 0,
                "needs_expansion": [],
                "missing_elements": []
            }
            
            total_length = 0
            for i, section in enumerate(sections):
                content_length = len(section.get('content', ''))
                total_length += content_length
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰©å±•
                if content_length < 800:  # å°‘äº800å­—éœ€è¦æ‰©å±•
                    quality_metrics["needs_expansion"].append(i)
                
                # æ£€æŸ¥ç¼ºå¤±å…ƒç´ 
                content_text = section.get('content', '').lower()
                if 'æ•°æ®' not in content_text and 'åˆ†æ' not in content_text:
                    quality_metrics["missing_elements"].append(f"section_{i}_data")
                if 'æ¡ˆä¾‹' not in content_text and 'ä¾‹å­' not in content_text:
                    quality_metrics["missing_elements"].append(f"section_{i}_examples")
            
            quality_metrics["avg_content_length"] = total_length / max(len(sections), 1)
            
            return quality_metrics
            
        except Exception as e:
            logger.error(f"å†…å®¹è´¨é‡åˆ†æå¤±è´¥: {e}")
            return {"needs_expansion": [], "missing_elements": []}
    
    async def _expand_section_content(
        self, 
        section: Dict, 
        quality_analysis: Dict, 
        documents_content: str, 
        conversation_content: str
    ) -> Dict:
        """æ‰©å±•ç« èŠ‚å†…å®¹"""
        try:
            section_index = quality_analysis.get("needs_expansion", [])
            if not section_index:
                return section
            
            # æ„å»ºæ‰©å±•æç¤º
            expansion_prompt = f"""è¯·æ‰©å±•ä»¥ä¸‹ç« èŠ‚å†…å®¹ï¼Œä½¿å…¶æ›´åŠ è¯¦ç»†å’Œä¸“ä¸šï¼š

ç« èŠ‚æ ‡é¢˜: {section.get('title', '')}
å½“å‰å†…å®¹: {section.get('content', '')}

å‚è€ƒä¿¡æ¯:
{documents_content[:2000]}

å¯¹è¯å†å²:
{conversation_content[:1000]}

è¯·ç”Ÿæˆæ‰©å±•åçš„å†…å®¹ï¼Œè¦æ±‚ï¼š
1. å†…å®¹é•¿åº¦è‡³å°‘800-1200å­—
2. åŒ…å«å…·ä½“çš„æ•°æ®å’Œåˆ†æ
3. æ·»åŠ ç›¸å…³æ¡ˆä¾‹å’Œä¾‹å­
4. ä¿æŒé€»è¾‘æ¸…æ™°å’Œä¸“ä¸šæ€§
5. å¦‚æœå¯èƒ½ï¼Œç”Ÿæˆè¡¨æ ¼æ•°æ®

è¿”å›JSONæ ¼å¼ï¼š
{{
    "expanded_content": "æ‰©å±•åçš„è¯¦ç»†å†…å®¹...",
    "table_data": [
        ["åˆ—1", "åˆ—2", "åˆ—3"],
        ["æ•°æ®1", "æ•°æ®2", "æ•°æ®3"]
    ],
    "subsections": [
        {{
            "title": "å­ç« èŠ‚æ ‡é¢˜",
            "content": "å­ç« èŠ‚å†…å®¹"
        }}
    ]
}}"""
            
            response = await self.llm.ainvoke(expansion_prompt)
            response_text = response.content if hasattr(response, 'content') else str(response)
            
            # è§£ææ‰©å±•ç»“æœ
            try:
                import re
                json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                if json_match:
                    expanded_data = json.loads(json_match.group())
                    
                    return {
                        "title": section.get('title', ''),
                        "content": expanded_data.get('expanded_content', section.get('content', '')),
                        "table_data": expanded_data.get('table_data'),
                        "subsections": expanded_data.get('subsections', [])
                    }
            except Exception as e:
                logger.warning(f"è§£ææ‰©å±•ç»“æœå¤±è´¥: {e}")
            
            # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›åŸå†…å®¹
            return section
            
        except Exception as e:
            logger.error(f"æ‰©å±•ç« èŠ‚å†…å®¹å¤±è´¥: {e}")
            return section
    
    async def _extract_structured_data_and_optimize(
        self, 
        intent, 
        enhanced_content: Dict, 
        documents_content: str, 
        conversation_content: str
    ) -> Dict:
        """æå–ç»“æ„åŒ–æ•°æ®å¹¶æœ€ç»ˆä¼˜åŒ–ï¼ˆç¬¬ä¸‰è½®ï¼‰"""
        try:
            logger.info("å¼€å§‹ç¬¬ä¸‰è½®ç»“æ„åŒ–æ•°æ®æå–å’Œä¼˜åŒ–...")
            
            # æå–ç»“æ„åŒ–æ•°æ®
            structured_data = await self._extract_tables_and_charts(enhanced_content, documents_content)
            
            # ä¼˜åŒ–å†…å®¹ç»“æ„
            optimized_sections = []
            for section in enhanced_content.get('sections', []):
                optimized_section = await self._optimize_section_structure(section, structured_data)
                optimized_sections.append(optimized_section)
            
            return {
                "outline": enhanced_content.get('outline', []),
                "sections": optimized_sections,
                "structured_data": structured_data
            }
            
        except Exception as e:
            logger.error(f"ç»“æ„åŒ–æ•°æ®æå–å’Œä¼˜åŒ–å¤±è´¥: {e}")
            return enhanced_content
    
    async def _extract_tables_and_charts(self, content: Dict, documents_content: str) -> Dict:
        """æå–è¡¨æ ¼å’Œå›¾è¡¨æ•°æ®"""
        try:
            extraction_prompt = f"""åŸºäºä»¥ä¸‹å†…å®¹ï¼Œæå–å’Œç”Ÿæˆç»“æ„åŒ–æ•°æ®ï¼š

æ–‡æ¡£å†…å®¹: {content}

å‚è€ƒä¿¡æ¯: {documents_content[:1500]}

è¯·åˆ†æå†…å®¹å¹¶ç”Ÿæˆï¼š
1. æ•°æ®è¡¨æ ¼ï¼ˆå¦‚æœæœ‰æ•°å€¼æ•°æ®ï¼‰
2. å¯¹æ¯”è¡¨æ ¼ï¼ˆå¦‚æœæœ‰æ¯”è¾ƒå†…å®¹ï¼‰
3. æµç¨‹å›¾æˆ–æ­¥éª¤å›¾ï¼ˆå¦‚æœæœ‰æµç¨‹æè¿°ï¼‰
4. æ—¶é—´çº¿ï¼ˆå¦‚æœæœ‰æ—¶é—´ç›¸å…³å†…å®¹ï¼‰

è¿”å›JSONæ ¼å¼ï¼š
{{
    "tables": [
        {{
            "title": "è¡¨æ ¼æ ‡é¢˜",
            "headers": ["åˆ—1", "åˆ—2", "åˆ—3"],
            "data": [
                ["è¡Œ1åˆ—1", "è¡Œ1åˆ—2", "è¡Œ1åˆ—3"],
                ["è¡Œ2åˆ—1", "è¡Œ2åˆ—2", "è¡Œ2åˆ—3"]
            ]
        }}
    ],
    "charts": [
        {{
            "type": "bar|line|pie",
            "title": "å›¾è¡¨æ ‡é¢˜",
            "data": {{"labels": ["æ ‡ç­¾1", "æ ‡ç­¾2"], "values": [10, 20]}}
        }}
    ],
    "timelines": [
        {{
            "title": "æ—¶é—´çº¿æ ‡é¢˜",
            "events": [
                {{"date": "2024-01", "event": "äº‹ä»¶æè¿°"}},
                {{"date": "2024-02", "event": "äº‹ä»¶æè¿°"}}
            ]
        }}
    ]
}}"""
            
            response = await self.llm.ainvoke(extraction_prompt)
            response_text = response.content if hasattr(response, 'content') else str(response)
            
            # è§£æç»“æ„åŒ–æ•°æ®
            try:
                import re
                json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                if json_match:
                    return json.loads(json_match.group())
            except Exception as e:
                logger.warning(f"è§£æç»“æ„åŒ–æ•°æ®å¤±è´¥: {e}")
            
            return {"tables": [], "charts": [], "timelines": []}
            
        except Exception as e:
            logger.error(f"æå–ç»“æ„åŒ–æ•°æ®å¤±è´¥: {e}")
            return {"tables": [], "charts": [], "timelines": []}
    
    async def _optimize_section_structure(self, section: Dict, structured_data: Dict) -> Dict:
        """ä¼˜åŒ–ç« èŠ‚ç»“æ„"""
        try:
            # å°†ç»“æ„åŒ–æ•°æ®æ•´åˆåˆ°ç« èŠ‚ä¸­
            optimized_section = section.copy()
            
            # æ·»åŠ ç›¸å…³è¡¨æ ¼
            if structured_data.get('tables'):
                for table in structured_data['tables']:
                    if table.get('title') and table.get('title').lower() in section.get('title', '').lower():
                        optimized_section['table_data'] = table.get('data', [])
                        break
            
            # ä¼˜åŒ–å†…å®¹æ ¼å¼
            content = optimized_section.get('content', '')
            
            # æ·»åŠ æ•°æ®å¼•ç”¨
            if structured_data.get('tables'):
                content += "\n\n### ç›¸å…³æ•°æ®è¡¨æ ¼\n"
                for table in structured_data['tables'][:2]:  # æœ€å¤šæ·»åŠ 2ä¸ªè¡¨æ ¼
                    content += f"\n**{table.get('title', 'æ•°æ®è¡¨æ ¼')}**\n"
                    content += "| " + " | ".join(table.get('headers', [])) + " |\n"
                    content += "| " + " | ".join(["---"] * len(table.get('headers', []))) + " |\n"
                    for row in table.get('data', []):
                        content += "| " + " | ".join(row) + " |\n"
                    content += "\n"
            
            optimized_section['content'] = content
            
            return optimized_section
            
        except Exception as e:
            logger.error(f"ä¼˜åŒ–ç« èŠ‚ç»“æ„å¤±è´¥: {e}")
            return section

    async def _retrieve_documents(self, workspace_id: str, query: str) -> List[Dict]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        try:
            # ä½¿ç”¨RAGæœåŠ¡æ£€ç´¢æ–‡æ¡£ï¼Œå¢åŠ æ£€ç´¢æ•°é‡
            response = await self.rag_service.ask_question(
                workspace_id=workspace_id,
                question=query,
                top_k=15  # å¢åŠ æ£€ç´¢æ•°é‡ä»¥è·å¾—æ›´å¤šå†…å®¹
            )
            
            references = response.get('references', [])
            
            # å¢å¼ºæ–‡æ¡£å†…å®¹ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿçš„ä¿¡æ¯
            enhanced_refs = []
            for ref in references:
                enhanced_ref = ref.copy()
                # ç¡®ä¿æœ‰è¶³å¤Ÿçš„å†…å®¹
                if len(enhanced_ref.get('content', '')) < 100:
                    enhanced_ref['content'] = enhanced_ref.get('content_preview', '') or enhanced_ref.get('content', '')
                enhanced_refs.append(enhanced_ref)
            
            return enhanced_refs
            
        except Exception as e:
            logger.error(f"æ–‡æ¡£æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    async def _extract_key_points(self, conversation_history: List[Dict]) -> str:
        """æå–å¯¹è¯å†å²å…³é”®ä¿¡æ¯"""
        if not conversation_history:
            return ""
        
        # æ„å»ºå¯¹è¯å†å²å­—ç¬¦ä¸²
        history_text = ""
        for msg in conversation_history[-10:]:  # åªå–æœ€è¿‘10æ¡
            role = "ç”¨æˆ·" if msg.get('role') == 'user' else "åŠ©æ‰‹"
            content = msg.get('content', '')
            history_text += f"{role}: {content}\n"
        
        # ä½¿ç”¨LLMæå–å…³é”®ä¿¡æ¯
        try:
            extract_prompt = f"""è¯·ä»ä»¥ä¸‹å¯¹è¯å†å²ä¸­æå–å…³é”®ä¿¡æ¯ï¼Œç”¨äºé«˜è´¨é‡æ–‡æ¡£ç”Ÿæˆï¼š

å¯¹è¯å†å²:
{history_text}

è¯·è¯¦ç»†æå–ä»¥ä¸‹ä¿¡æ¯ï¼š
1. **ç”¨æˆ·çš„å…·ä½“éœ€æ±‚å’Œç›®æ ‡** - ç”¨æˆ·æƒ³è¦ä»€ä¹ˆï¼Ÿ
2. **è®¨è®ºçš„æ ¸å¿ƒé—®é¢˜å’Œè¯é¢˜** - ä¸»è¦å…³æ³¨ä»€ä¹ˆï¼Ÿ
3. **é‡è¦çš„æ•°æ®ã€æ¡ˆä¾‹ã€äº‹å®** - æœ‰ä»€ä¹ˆå…·ä½“ä¿¡æ¯ï¼Ÿ
4. **ç”¨æˆ·çš„åå¥½å’Œè¦æ±‚** - æœ‰ä»€ä¹ˆç‰¹æ®Šè¦æ±‚ï¼Ÿ
5. **å…³é”®è§‚ç‚¹å’Œç»“è®º** - è¾¾æˆäº†ä»€ä¹ˆå…±è¯†ï¼Ÿ
6. **éœ€è¦å¼ºè°ƒçš„é‡ç‚¹** - ä»€ä¹ˆæœ€é‡è¦ï¼Ÿ

è¯·ä»¥ç»“æ„åŒ–çš„æ–¹å¼æ•´ç†è¿™äº›ä¿¡æ¯ï¼Œç¡®ä¿å¯¹ç”Ÿæˆä¸“ä¸šæ–‡æ¡£æœ‰ä»·å€¼ã€‚"""
            
            response = await self.llm.ainvoke(extract_prompt)
            return response if isinstance(response, str) else str(response)
            
        except Exception as e:
            logger.error(f"æå–å¯¹è¯å…³é”®ä¿¡æ¯å¤±è´¥: {e}")
            return history_text
    
    async def _generate_outline(self, intent, documents_content: str, conversation_content: str) -> Dict:
        """ç”Ÿæˆæ–‡æ¡£å¤§çº²"""
        try:
            prompt = self.outline_prompt.format(
                title=intent.title,
                doc_type=intent.doc_type.value,
                content_source=intent.content_source,
                documents_content=documents_content[:2000],  # é™åˆ¶é•¿åº¦
                conversation_content=conversation_content[:1000]
            )
            
            response = await self.llm.ainvoke(prompt)
            
            # å¤„ç†ä¸åŒç±»å‹çš„å“åº”
            if hasattr(response, 'content'):
                response_text = response.content
            else:
                response_text = str(response)
            
            # è§£æJSONå“åº”
            import json
            # æ¸…ç†å“åº”å†…å®¹ï¼Œç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
            if response_text.startswith('```json'):
                response_text = response_text[7:]
            if response_text.endswith('```'):
                response_text = response_text[:-3]
            response_text = response_text.strip()
            
            json_start = response_text.find('{')
            json_end = response_text.rfind('}') + 1
            if json_start != -1 and json_end != -1:
                json_str = response_text[json_start:json_end]
                return json.loads(json_str)
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå¤§çº²å¤±è´¥: {e}")
        
        # å›é€€åˆ°ç®€å•å¤§çº²
        return {
            "outline": [intent.title],
            "sections": [{
                "title": intent.title,
                "description": "ä¸»è¦å†…å®¹",
                "content_type": "text"
            }]
        }
    
    async def _fill_section_content(self, intent, section_info: Dict, documents_content: str, conversation_content: str) -> ContentSection:
        """å¡«å……ç« èŠ‚è¯¦ç»†å†…å®¹"""
        try:
            prompt = self.content_prompt.format(
                title=intent.title,
                doc_type=intent.doc_type.value,
                section_title=section_info.get('title', ''),
                relevant_content=documents_content[:1500],
                conversation_content=conversation_content[:800]
            )
            
            response = await self.llm.ainvoke(prompt)
            
            # å¤„ç†ä¸åŒç±»å‹çš„å“åº”
            if hasattr(response, 'content'):
                response_text = response.content
            else:
                response_text = str(response)
            
            # è§£æJSONå“åº”
            import json
            # æ¸…ç†å“åº”å†…å®¹ï¼Œç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
            if response_text.startswith('```json'):
                response_text = response_text[7:]
            if response_text.endswith('```'):
                response_text = response_text[:-3]
            response_text = response_text.strip()
            
            json_start = response_text.find('{')
            json_end = response_text.rfind('}') + 1
            if json_start != -1 and json_end != -1:
                json_str = response_text[json_start:json_end]
                content_data = json.loads(json_str)
                
                return ContentSection(
                    title=section_info.get('title', ''),
                    content=content_data.get('content', ''),
                    table_data=content_data.get('table_data'),
                    subsections=content_data.get('subsections')
                )
            
        except Exception as e:
            logger.error(f"å¡«å……ç« èŠ‚å†…å®¹å¤±è´¥: {e}")
            # ç›´æ¥ä½¿ç”¨æ–‡æœ¬å†…å®¹ï¼Œä¸è§£æJSON
            if hasattr(response, 'content'):
                content_text = response.content
            elif 'response_text' in locals():
                content_text = response_text
            else:
                content_text = f"è¿™æ˜¯{section_info.get('title', '')}çš„å†…å®¹ã€‚"
            
            return ContentSection(
                title=section_info.get('title', ''),
                content=content_text,
                table_data=None
            )
        
        # å›é€€åˆ°ç®€å•å†…å®¹
        return ContentSection(
            title=section_info.get('title', ''),
            content=f"è¿™æ˜¯{section_info.get('title', '')}çš„å†…å®¹ã€‚",
            table_data=None
        )
